{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_HACIxMBlMv",
    "outputId": "c7e1ba5c-6496-42c3-b38a-6c4789cad2a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 2000\n",
      "Number of columns in the dataset: 50\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
      "0        2       0         3          1   4        1       4         1   \n",
      "1        2       0         1          2   2        1       4         3   \n",
      "2        3       0         2          1   3        1       4         3   \n",
      "3        2       0         1          1   1        0       4         1   \n",
      "4        2       0         1          1   3        0       4         3   \n",
      "\n",
      "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
      "0          2   2  ...         3        3          3           2    3   \n",
      "1          3   1  ...         2        3          2           2    3   \n",
      "2          3   0  ...         4        4          2           1    4   \n",
      "3          3   1  ...         1        2          2           1    3   \n",
      "4          2   0  ...         3        4          1           3    4   \n",
      "\n",
      "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
      "0         1        4          4           2    4  \n",
      "1         1        3          2           2    3  \n",
      "2         2        4          2           0    4  \n",
      "3         1        3          2           1    2  \n",
      "4         1        3          1           3    4  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Data Types and Non-Null Counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 50 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   blue_q0     2000 non-null   int64\n",
      " 1   red_q1      2000 non-null   int64\n",
      " 2   green_q2    2000 non-null   int64\n",
      " 3   purple_q3   2000 non-null   int64\n",
      " 4   q4          2000 non-null   int64\n",
      " 5   blue_q5     2000 non-null   int64\n",
      " 6   red_q6      2000 non-null   int64\n",
      " 7   green_q7    2000 non-null   int64\n",
      " 8   purple_q8   2000 non-null   int64\n",
      " 9   q9          2000 non-null   int64\n",
      " 10  blue_q10    2000 non-null   int64\n",
      " 11  red_q11     2000 non-null   int64\n",
      " 12  green_q12   2000 non-null   int64\n",
      " 13  purple_q13  2000 non-null   int64\n",
      " 14  q14         2000 non-null   int64\n",
      " 15  blue_q15    2000 non-null   int64\n",
      " 16  red_q16     2000 non-null   int64\n",
      " 17  green_q17   2000 non-null   int64\n",
      " 18  purple_q18  2000 non-null   int64\n",
      " 19  q19         2000 non-null   int64\n",
      " 20  blue_q20    2000 non-null   int64\n",
      " 21  red_q21     2000 non-null   int64\n",
      " 22  green_q22   2000 non-null   int64\n",
      " 23  purple_q23  2000 non-null   int64\n",
      " 24  q24         2000 non-null   int64\n",
      " 25  blue_q25    2000 non-null   int64\n",
      " 26  red_q26     2000 non-null   int64\n",
      " 27  green_q27   2000 non-null   int64\n",
      " 28  purple_q28  2000 non-null   int64\n",
      " 29  q29         2000 non-null   int64\n",
      " 30  blue_q30    2000 non-null   int64\n",
      " 31  red_q31     2000 non-null   int64\n",
      " 32  green_q32   2000 non-null   int64\n",
      " 33  purple_q33  2000 non-null   int64\n",
      " 34  q34         2000 non-null   int64\n",
      " 35  blue_q35    2000 non-null   int64\n",
      " 36  red_q36     2000 non-null   int64\n",
      " 37  green_q37   2000 non-null   int64\n",
      " 38  purple_q38  2000 non-null   int64\n",
      " 39  q39         2000 non-null   int64\n",
      " 40  blue_q40    2000 non-null   int64\n",
      " 41  red_q41     2000 non-null   int64\n",
      " 42  green_q42   2000 non-null   int64\n",
      " 43  purple_q43  2000 non-null   int64\n",
      " 44  q44         2000 non-null   int64\n",
      " 45  blue_q45    2000 non-null   int64\n",
      " 46  red_q46     2000 non-null   int64\n",
      " 47  green_q47   2000 non-null   int64\n",
      " 48  purple_q48  2000 non-null   int64\n",
      " 49  q49         2000 non-null   int64\n",
      "dtypes: int64(50)\n",
      "memory usage: 781.4 KB\n",
      "\n",
      "Training set shape: (1600, 50)\n",
      "Test set shape: (400, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset from the GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of instances in the dataset:\", df.shape[0])\n",
    "print(\"Number of columns in the dataset:\", df.shape[1])\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display additional information\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "df.info()\n",
    "\n",
    "# Normalize the data by dividing by 4.0\n",
    "df = df / 4.0\n",
    "\n",
    "# Split the data into features (X)\n",
    "X = df.values  # Convert the DataFrame into a NumPy array for model input\n",
    "\n",
    "# Split the dataset into training (80%) and testing sets (20%)\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shapes to verify\n",
    "print(\"\\nTraining set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgTRpCHI8ia2",
    "outputId": "9b6cd8df-69dc-4d8f-c514-c6ecee50410f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FA MSE on training set: 0.048127\n",
      "FA MSE on test set: 0.046869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the FA model with the number of components matching your latent dimension\n",
    "n_components = 5  # Use your desired number of factors\n",
    "fa = FactorAnalysis(n_components=n_components, random_state=42)\n",
    "\n",
    "# Fit FA on the training data\n",
    "fa.fit(X_train)\n",
    "\n",
    "# Transform the training data to get the latent factors\n",
    "X_train_reduced_fa = fa.transform(X_train)\n",
    "\n",
    "# Manually reconstruct the training data using the latent factors and the factor loadings\n",
    "X_train_reconstructed_fa = np.dot(X_train_reduced_fa, fa.components_) + np.mean(X_train, axis=0)\n",
    "\n",
    "# Calculate the MSE on the training set\n",
    "mse_train_fa = mean_squared_error(X_train, X_train_reconstructed_fa)\n",
    "print(f\"\\nFA MSE on training set: {mse_train_fa:.6f}\")\n",
    "\n",
    "# Transform the test data using the trained FA model\n",
    "X_test_reduced_fa = fa.transform(X_test)\n",
    "\n",
    "# Reconstruct the test data using the latent factors and the factor loadings\n",
    "X_test_reconstructed_fa = np.dot(X_test_reduced_fa, fa.components_) + np.mean(X_train, axis=0)  # Use train mean for consistent reconstruction\n",
    "\n",
    "# Calculate the MSE on the test set\n",
    "mse_test_fa = mean_squared_error(X_test, X_test_reconstructed_fa)\n",
    "print(f\"FA MSE on test set: {mse_test_fa:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruuZYBO38ia3",
    "outputId": "b58bd924-8fb3-41f5-a6d1-628d24dcc03f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FA Adjusted Rand Index (ARI) on variables: 0.5265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Get the factor loadings (components)\n",
    "# Shape of components_: (n_components, n_features)\n",
    "factor_loadings = fa.components_.T  # Transpose to shape (n_features, n_components)\n",
    "\n",
    "# For each variable (feature), assign it to the factor with the highest absolute loading\n",
    "predicted_variable_labels = np.argmax(np.abs(factor_loadings), axis=1)  # Shape: (n_features,)\n",
    "\n",
    "# Create true labels for variables based on known groupings\n",
    "# Assuming 'df' is your original DataFrame with column names matching the features\n",
    "\n",
    "# Identify column prefixes for each true factor\n",
    "factor_columns = {\n",
    "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
    "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
    "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
    "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
    "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
    "}\n",
    "\n",
    "# Map factor names to column indices\n",
    "factor_indices = {}\n",
    "for factor_name, columns in factor_columns.items():\n",
    "    indices = [df.columns.get_loc(col) for col in columns]\n",
    "    factor_indices[factor_name] = indices\n",
    "\n",
    "# Create true labels for variables\n",
    "n_features = X_train.shape[1]\n",
    "true_variable_labels = np.full(n_features, -1)  # Initialize with -1\n",
    "\n",
    "factor_names = ['Factor1', 'Factor2', 'Factor3', 'Factor4', 'Factor5']\n",
    "factor_name_to_index = {name: idx for idx, name in enumerate(factor_names)}\n",
    "\n",
    "for factor_name, indices in factor_indices.items():\n",
    "    factor_idx = factor_name_to_index[factor_name]\n",
    "    true_variable_labels[indices] = factor_idx\n",
    "\n",
    "# Ensure all variables have been assigned\n",
    "assert np.all(true_variable_labels >= 0), \"Some variables have not been assigned a true label\"\n",
    "\n",
    "# Compute the Adjusted Rand Index\n",
    "ari_fa = adjusted_rand_score(true_variable_labels, predicted_variable_labels)\n",
    "print(f\"\\nFA Adjusted Rand Index (ARI) on variables: {ari_fa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1Ynqkiu8vCl"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Plotting the Attention Matrix for FA Model\n",
    "# --------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Compute Absolute Factor Loadings\n",
    "abs_factor_loadings = np.abs(factor_loadings)  # Shape: (n_features, n_components)\n",
    "\n",
    "# Step 2: Apply Softmax Along Each Variable (Rows)\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "softmax_loadings = softmax(abs_factor_loadings)  # Shape: (n_features, n_components)\n",
    "\n",
    "# Step 3: Rearrange Observed Variables According to Ground Truth Groupings\n",
    "\n",
    "# Create a new ordering of indices\n",
    "new_order = []\n",
    "for factor_name in factor_names:\n",
    "    new_order.extend(factor_indices[factor_name])\n",
    "\n",
    "# Ensure all indices are included\n",
    "assert len(new_order) == n_features, \"Not all indices are included in the new order.\"\n",
    "\n",
    "# Rearrange the columns (observed variables) using new_order\n",
    "softmax_loadings_reordered = softmax_loadings[new_order, :]  # Shape: (n_features, n_components)\n",
    "\n",
    "# Transpose to get shape (n_components, n_features)\n",
    "attention_matrix = softmax_loadings_reordered.T  # Shape: (n_components, n_features)\n",
    "\n",
    "# Step 4: Plot the Attention Matrix\n",
    "\n",
    "def plot_attention_heatmap(attn_matrix, title, factor_names, variable_labels):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(\n",
    "        attn_matrix,\n",
    "        cmap='viridis',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={'label': 'Attention Weight (Softmax of Absolute Loadings)'}\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Observed Variables')\n",
    "    plt.ylabel('Latent Factors')\n",
    "    plt.xticks(ticks=np.arange(len(variable_labels))+0.5, labels=variable_labels, rotation=90, fontsize=8)\n",
    "    plt.yticks(ticks=np.arange(len(factor_names))+0.5, labels=factor_names, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate labels for the variables after reordering\n",
    "variable_labels = df.columns[new_order]\n",
    "\n",
    "# Plot the attention matrix\n",
    "plot_attention_heatmap(\n",
    "    attn_matrix=attention_matrix,\n",
    "    title='Attention Matrix (Factor Analysis Model with Softmax)',\n",
    "    factor_names=factor_names,\n",
    "    variable_labels=variable_labels\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
