{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_HACIxMBlMv",
    "outputId": "a266a5c4-b111-46e8-b71d-77b930d36ab0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 2000\n",
      "Number of columns in the dataset: 50\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
      "0        2       0         3          1   4        1       4         1   \n",
      "1        2       0         1          2   2        1       4         3   \n",
      "2        3       0         2          1   3        1       4         3   \n",
      "3        2       0         1          1   1        0       4         1   \n",
      "4        2       0         1          1   3        0       4         3   \n",
      "\n",
      "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
      "0          2   2  ...         3        3          3           2    3   \n",
      "1          3   1  ...         2        3          2           2    3   \n",
      "2          3   0  ...         4        4          2           1    4   \n",
      "3          3   1  ...         1        2          2           1    3   \n",
      "4          2   0  ...         3        4          1           3    4   \n",
      "\n",
      "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
      "0         1        4          4           2    4  \n",
      "1         1        3          2           2    3  \n",
      "2         2        4          2           0    4  \n",
      "3         1        3          2           1    2  \n",
      "4         1        3          1           3    4  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Data Types and Non-Null Counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 50 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   blue_q0     2000 non-null   int64\n",
      " 1   red_q1      2000 non-null   int64\n",
      " 2   green_q2    2000 non-null   int64\n",
      " 3   purple_q3   2000 non-null   int64\n",
      " 4   q4          2000 non-null   int64\n",
      " 5   blue_q5     2000 non-null   int64\n",
      " 6   red_q6      2000 non-null   int64\n",
      " 7   green_q7    2000 non-null   int64\n",
      " 8   purple_q8   2000 non-null   int64\n",
      " 9   q9          2000 non-null   int64\n",
      " 10  blue_q10    2000 non-null   int64\n",
      " 11  red_q11     2000 non-null   int64\n",
      " 12  green_q12   2000 non-null   int64\n",
      " 13  purple_q13  2000 non-null   int64\n",
      " 14  q14         2000 non-null   int64\n",
      " 15  blue_q15    2000 non-null   int64\n",
      " 16  red_q16     2000 non-null   int64\n",
      " 17  green_q17   2000 non-null   int64\n",
      " 18  purple_q18  2000 non-null   int64\n",
      " 19  q19         2000 non-null   int64\n",
      " 20  blue_q20    2000 non-null   int64\n",
      " 21  red_q21     2000 non-null   int64\n",
      " 22  green_q22   2000 non-null   int64\n",
      " 23  purple_q23  2000 non-null   int64\n",
      " 24  q24         2000 non-null   int64\n",
      " 25  blue_q25    2000 non-null   int64\n",
      " 26  red_q26     2000 non-null   int64\n",
      " 27  green_q27   2000 non-null   int64\n",
      " 28  purple_q28  2000 non-null   int64\n",
      " 29  q29         2000 non-null   int64\n",
      " 30  blue_q30    2000 non-null   int64\n",
      " 31  red_q31     2000 non-null   int64\n",
      " 32  green_q32   2000 non-null   int64\n",
      " 33  purple_q33  2000 non-null   int64\n",
      " 34  q34         2000 non-null   int64\n",
      " 35  blue_q35    2000 non-null   int64\n",
      " 36  red_q36     2000 non-null   int64\n",
      " 37  green_q37   2000 non-null   int64\n",
      " 38  purple_q38  2000 non-null   int64\n",
      " 39  q39         2000 non-null   int64\n",
      " 40  blue_q40    2000 non-null   int64\n",
      " 41  red_q41     2000 non-null   int64\n",
      " 42  green_q42   2000 non-null   int64\n",
      " 43  purple_q43  2000 non-null   int64\n",
      " 44  q44         2000 non-null   int64\n",
      " 45  blue_q45    2000 non-null   int64\n",
      " 46  red_q46     2000 non-null   int64\n",
      " 47  green_q47   2000 non-null   int64\n",
      " 48  purple_q48  2000 non-null   int64\n",
      " 49  q49         2000 non-null   int64\n",
      "dtypes: int64(50)\n",
      "memory usage: 781.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of instances in the dataset:\", df.shape[0])\n",
    "print(\"Number of columns in the dataset:\", df.shape[1])\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display additional information\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yaO1N84CB7_U"
   },
   "outputs": [],
   "source": [
    "df = df / 4.0\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jA29S2CJQV",
    "outputId": "97c14234-90fb-43e0-944e-38113a3f8b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([2000, 50])\n",
      "Number of training batches: 50\n",
      "Number of validation batches: 13\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "data_array = df.to_numpy()\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Create a generator for deterministic splitting and sampling\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "# Split the dataset into training and validation sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "# Create a RandomSampler with the same generator for deterministic shuffling\n",
    "train_sampler = RandomSampler(train_dataset, generator=generator)\n",
    "\n",
    "# Create DataLoaders for the training and validation sets\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Display the shape of the tensor to verify\n",
    "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rywEvDazCYkL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Encoder class with explicit layer definitions and assert statements\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=50, hidden_dims=[128, 64], output_dim=5, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Assertions for input dimensions\n",
    "        assert input_dim == 50, f\"Expected input_dim to be 50, but got {input_dim}\"\n",
    "        assert output_dim == 5, f\"Expected output_dim to be 5, but got {output_dim}\"\n",
    "        assert embedding_dim == 64, f\"Expected embedding_dim to be 64, but got {embedding_dim}\"\n",
    "\n",
    "        # Define the layers explicitly\n",
    "        self.layers = nn.ModuleList()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "\n",
    "        # Create Linear layers\n",
    "        for i in range(len(dims) - 1):\n",
    "            in_features = dims[i]\n",
    "            out_features = dims[i + 1]\n",
    "            layer = nn.Linear(in_features, out_features)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # Learnable embedding vectors e_i for each z_i\n",
    "        self.e = nn.Parameter(torch.randn(output_dim, embedding_dim))\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Assert the number of Linear layers\n",
    "        expected_num_linear_layers = len(hidden_dims) + 1  # Number of hidden layers + output layer\n",
    "        actual_num_linear_layers = len(self.layers)\n",
    "        assert actual_num_linear_layers == expected_num_linear_layers, \\\n",
    "            f\"Expected {expected_num_linear_layers} Linear layers, but got {actual_num_linear_layers}\"\n",
    "\n",
    "        # Assert the input and output dimensions of the Linear layers\n",
    "        # First Linear layer\n",
    "        first_linear = self.layers[0]\n",
    "        assert first_linear.in_features == 50, \\\n",
    "            f\"Expected first Linear layer to have input features 50, but got {first_linear.in_features}\"\n",
    "\n",
    "        # Last Linear layer\n",
    "        last_linear = self.layers[-1]\n",
    "        assert last_linear.out_features == 5, \\\n",
    "            f\"Expected last Linear layer to have output features 5, but got {last_linear.out_features}\"\n",
    "\n",
    "        # Assert the shape of e\n",
    "        assert self.e.shape == (5, 64), \\\n",
    "            f\"Expected embedding matrix e to have shape (5, 64), but got {self.e.shape}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assert the input shape\n",
    "        assert x.dim() == 2, f\"Expected input x to be a 2D tensor, but got {x.dim()}D tensor\"\n",
    "        assert x.shape[1] == 50, f\"Expected input x to have 50 features, but got {x.shape[1]}\"\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Pass the input through the Linear layers with ReLU activations\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # Apply ReLU after each hidden layer except the last layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "\n",
    "        Z = x  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Assert the shape of Z\n",
    "        assert Z.shape == (batch_size, 5), \\\n",
    "            f\"Expected Z to have shape ({batch_size}, 5), but got {Z.shape}\"\n",
    "\n",
    "        # Convert Z to \\hat Z by multiplying each scalar z_i with its own embedding vector e_i\n",
    "        Z_expanded = Z.unsqueeze(2)  # Shape: (batch_size, output_dim, 1)\n",
    "        assert Z_expanded.shape == (batch_size, 5, 1), \\\n",
    "            f\"Expected Z_expanded to have shape ({batch_size}, 5, 1), but got {Z_expanded.shape}\"\n",
    "\n",
    "        e_expanded = self.e.unsqueeze(0)  # Shape: (1, output_dim, embedding_dim)\n",
    "        assert e_expanded.shape == (1, 5, 64), \\\n",
    "            f\"Expected e_expanded to have shape (1, 5, 64), but got {e_expanded.shape}\"\n",
    "\n",
    "        # Multiply Z_expanded and e_expanded to get hat_Z\n",
    "        hat_Z = Z_expanded * e_expanded  # Shape: (batch_size, output_dim, embedding_dim)\n",
    "        assert hat_Z.shape == (batch_size, 5, 64), \\\n",
    "            f\"Expected hat_Z to have shape ({batch_size}, 5, 64), but got {hat_Z.shape}\"\n",
    "\n",
    "        return hat_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tUXzn0AHHq1T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim=50, embedding_dim=64, hidden_dims=[]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim      # Number of observed variables (n)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Assert input dimensions\n",
    "        assert input_dim == 50, f\"Expected input_dim to be 50, but got {input_dim}\"\n",
    "        assert embedding_dim == 64, f\"Expected embedding_dim to be 64, but got {embedding_dim}\"\n",
    "\n",
    "        # Learnable query embeddings (e1, e2, ..., en)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
    "\n",
    "        # Assert query_embeddings shape\n",
    "        assert self.query_embeddings.shape == (50, 64), \\\n",
    "            f\"Expected query_embeddings to have shape (50, 64), but got {self.query_embeddings.shape}\"\n",
    "\n",
    "        # MultiheadAttention module with 1 head\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Define individual MLPs for each observed variable\n",
    "        dims = [embedding_dim] + hidden_dims + [1]\n",
    "\n",
    "        # Create MLPs for each observed variable\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            nn.Sequential(*[\n",
    "                nn.Linear(dims[i], dims[i + 1]) if i == len(dims) - 2 else nn.Sequential(\n",
    "                    nn.Linear(dims[i], dims[i + 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                for i in range(len(dims) - 1)\n",
    "            ])\n",
    "            for _ in range(input_dim)\n",
    "        ])\n",
    "\n",
    "        # Assert that we have one MLP per observed variable\n",
    "        assert len(self.mlp_layers) == 50, \\\n",
    "            f\"Expected 50 MLPs in mlp_layers, but got {len(self.mlp_layers)}\"\n",
    "\n",
    "        # Verify that MLPs do not share parameters\n",
    "        mlp_params = [set(mlp.parameters()) for mlp in self.mlp_layers]\n",
    "        for i in range(len(mlp_params)):\n",
    "            for j in range(i + 1, len(mlp_params)):\n",
    "                assert mlp_params[i].isdisjoint(mlp_params[j]), \\\n",
    "                    f\"MLP {i} and MLP {j} share parameters\"\n",
    "\n",
    "    def forward(self, hat_Z):\n",
    "        \"\"\"\n",
    "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Assert the shape of hat_Z\n",
    "        assert hat_Z.dim() == 3, f\"Expected hat_Z to be a 3D tensor, but got {hat_Z.dim()}D tensor\"\n",
    "        batch_size, output_dim, embedding_dim = hat_Z.shape\n",
    "        assert embedding_dim == 64, \\\n",
    "            f\"Expected hat_Z embedding_dim to be 64, but got {embedding_dim}\"\n",
    "        assert output_dim == 5, \\\n",
    "            f\"Expected hat_Z output_dim to be 5, but got {output_dim}\"\n",
    "\n",
    "        # Prepare query embeddings and expand to batch size\n",
    "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "        assert query_embeddings.shape == (batch_size, 50, 64), \\\n",
    "            f\"Expected query_embeddings to have shape ({batch_size}, 50, 64), but got {query_embeddings.shape}\"\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)  # Output shape: (batch_size, input_dim, embedding_dim)\n",
    "        assert attn_output.shape == (batch_size, 50, 64), \\\n",
    "            f\"Expected attn_output to have shape ({batch_size}, 50, 64), but got {attn_output.shape}\"\n",
    "        assert attn_weights.shape == (batch_size, 50, 5), \\\n",
    "            f\"Expected attn_weights to have shape ({batch_size}, 50, 5), but got {attn_weights.shape}\"\n",
    "\n",
    "        # Add residual connection and apply layer normalization\n",
    "        out = self.layer_norm(attn_output + query_embeddings)  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "        assert out.shape == (batch_size, 50, 64), \\\n",
    "            f\"Expected out to have shape ({batch_size}, 50, 64), but got {out.shape}\"\n",
    "\n",
    "        # Pass each context vector through its corresponding MLP\n",
    "        x_hat = []\n",
    "        for i in range(self.input_dim):\n",
    "            x_i = out[:, i, :]  # Shape: (batch_size, embedding_dim)\n",
    "            assert x_i.shape == (batch_size, 64), \\\n",
    "                f\"Expected x_i to have shape ({batch_size}, 64), but got {x_i.shape}\"\n",
    "\n",
    "            x_i_hat = self.mlp_layers[i](x_i)  # Shape: (batch_size, 1)\n",
    "            assert x_i_hat.shape == (batch_size, 1), \\\n",
    "                f\"Expected x_i_hat to have shape ({batch_size}, 1), but got {x_i_hat.shape}\"\n",
    "\n",
    "            x_hat.append(x_i_hat)\n",
    "        x_hat = torch.cat(x_hat, dim=1)  # Shape: (batch_size, input_dim)\n",
    "        assert x_hat.shape == (batch_size, 50), \\\n",
    "            f\"Expected x_hat to have shape ({batch_size}, 50), but got {x_hat.shape}\"\n",
    "\n",
    "        return x_hat, attn_weights  # Return attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xnBqmgVjIat0"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=encoder_hidden_dims\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            input_dim=input_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=decoder_hidden_dims\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hat_Z = self.encoder(x)     # Obtain \\hat{Z} from the encoder\n",
    "        x_hat, attn_weights = self.decoder(hat_Z)  # Reconstruct x from \\hat{Z} using the decoder\n",
    "        return x_hat, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ki1licN-PTun",
    "outputId": "fb864687-79d0-4763-ec7f-18a71da1ed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "No trained model found. Starting training from scratch.\n",
      "Epoch [1/25], Lambda Entropy: 0.000000, Train Total Loss: 0.1244, Train Recon Loss: 0.1244, Train Entropy Loss: 0.9415, Train ARI: -0.0046, Val Total Loss: 0.0846, Val Recon Loss: 0.0846, Val Entropy Loss: 0.8934, Val ARI: 0.0135\n",
      "Epoch [2/25], Lambda Entropy: 0.000006, Train Total Loss: 0.0783, Train Recon Loss: 0.0783, Train Entropy Loss: 0.7880, Train ARI: 0.0280, Val Total Loss: 0.0719, Val Recon Loss: 0.0719, Val Entropy Loss: 0.7658, Val ARI: 0.0542\n",
      "Epoch [3/25], Lambda Entropy: 0.000013, Train Total Loss: 0.0688, Train Recon Loss: 0.0688, Train Entropy Loss: 0.7718, Train ARI: 0.0941, Val Total Loss: 0.0635, Val Recon Loss: 0.0635, Val Entropy Loss: 0.7481, Val ARI: 0.1454\n",
      "Epoch [4/25], Lambda Entropy: 0.000022, Train Total Loss: 0.0616, Train Recon Loss: 0.0616, Train Entropy Loss: 0.7645, Train ARI: 0.1740, Val Total Loss: 0.0589, Val Recon Loss: 0.0589, Val Entropy Loss: 0.7876, Val ARI: 0.2105\n",
      "Epoch [5/25], Lambda Entropy: 0.000033, Train Total Loss: 0.0583, Train Recon Loss: 0.0583, Train Entropy Loss: 0.8105, Train ARI: 0.2217, Val Total Loss: 0.0566, Val Recon Loss: 0.0566, Val Entropy Loss: 0.8275, Val ARI: 0.2512\n",
      "Epoch [6/25], Lambda Entropy: 0.000047, Train Total Loss: 0.0558, Train Recon Loss: 0.0558, Train Entropy Loss: 0.8350, Train ARI: 0.2840, Val Total Loss: 0.0551, Val Recon Loss: 0.0550, Val Entropy Loss: 0.8437, Val ARI: 0.3042\n",
      "Epoch [7/25], Lambda Entropy: 0.000063, Train Total Loss: 0.0546, Train Recon Loss: 0.0545, Train Entropy Loss: 0.8556, Train ARI: 0.3171, Val Total Loss: 0.0538, Val Recon Loss: 0.0538, Val Entropy Loss: 0.8674, Val ARI: 0.3247\n",
      "Epoch [8/25], Lambda Entropy: 0.000083, Train Total Loss: 0.0531, Train Recon Loss: 0.0530, Train Entropy Loss: 0.8757, Train ARI: 0.3616, Val Total Loss: 0.0531, Val Recon Loss: 0.0530, Val Entropy Loss: 0.8857, Val ARI: 0.3945\n",
      "Epoch [9/25], Lambda Entropy: 0.000107, Train Total Loss: 0.0515, Train Recon Loss: 0.0514, Train Entropy Loss: 0.8866, Train ARI: 0.4400, Val Total Loss: 0.0510, Val Recon Loss: 0.0509, Val Entropy Loss: 0.8891, Val ARI: 0.4932\n",
      "Epoch [10/25], Lambda Entropy: 0.000137, Train Total Loss: 0.0502, Train Recon Loss: 0.0500, Train Entropy Loss: 0.8912, Train ARI: 0.5222, Val Total Loss: 0.0505, Val Recon Loss: 0.0503, Val Entropy Loss: 0.8971, Val ARI: 0.5619\n",
      "Epoch [11/25], Lambda Entropy: 0.000173, Train Total Loss: 0.0495, Train Recon Loss: 0.0494, Train Entropy Loss: 0.8982, Train ARI: 0.5574, Val Total Loss: 0.0502, Val Recon Loss: 0.0500, Val Entropy Loss: 0.9053, Val ARI: 0.5614\n",
      "Epoch [12/25], Lambda Entropy: 0.000218, Train Total Loss: 0.0490, Train Recon Loss: 0.0488, Train Entropy Loss: 0.9062, Train ARI: 0.5671, Val Total Loss: 0.0495, Val Recon Loss: 0.0493, Val Entropy Loss: 0.9131, Val ARI: 0.5667\n",
      "Epoch [13/25], Lambda Entropy: 0.000272, Train Total Loss: 0.0487, Train Recon Loss: 0.0485, Train Entropy Loss: 0.9133, Train ARI: 0.5684, Val Total Loss: 0.0494, Val Recon Loss: 0.0492, Val Entropy Loss: 0.9206, Val ARI: 0.5844\n",
      "Epoch [14/25], Lambda Entropy: 0.000338, Train Total Loss: 0.0488, Train Recon Loss: 0.0484, Train Entropy Loss: 0.9196, Train ARI: 0.5718, Val Total Loss: 0.0494, Val Recon Loss: 0.0491, Val Entropy Loss: 0.9214, Val ARI: 0.5676\n",
      "Epoch [15/25], Lambda Entropy: 0.000419, Train Total Loss: 0.0484, Train Recon Loss: 0.0480, Train Entropy Loss: 0.9234, Train ARI: 0.5717, Val Total Loss: 0.0488, Val Recon Loss: 0.0484, Val Entropy Loss: 0.9281, Val ARI: 0.5842\n",
      "Epoch [16/25], Lambda Entropy: 0.000518, Train Total Loss: 0.0482, Train Recon Loss: 0.0477, Train Entropy Loss: 0.9252, Train ARI: 0.5714, Val Total Loss: 0.0490, Val Recon Loss: 0.0485, Val Entropy Loss: 0.9284, Val ARI: 0.5636\n",
      "Epoch [17/25], Lambda Entropy: 0.000639, Train Total Loss: 0.0482, Train Recon Loss: 0.0476, Train Entropy Loss: 0.9275, Train ARI: 0.5627, Val Total Loss: 0.0491, Val Recon Loss: 0.0485, Val Entropy Loss: 0.9282, Val ARI: 0.5764\n",
      "Epoch [18/25], Lambda Entropy: 0.000786, Train Total Loss: 0.0482, Train Recon Loss: 0.0475, Train Entropy Loss: 0.9295, Train ARI: 0.5598, Val Total Loss: 0.0492, Val Recon Loss: 0.0485, Val Entropy Loss: 0.9316, Val ARI: 0.5770\n",
      "Epoch [19/25], Lambda Entropy: 0.000966, Train Total Loss: 0.0482, Train Recon Loss: 0.0473, Train Entropy Loss: 0.9306, Train ARI: 0.5607, Val Total Loss: 0.0490, Val Recon Loss: 0.0481, Val Entropy Loss: 0.9300, Val ARI: 0.5556\n",
      "Epoch [20/25], Lambda Entropy: 0.001186, Train Total Loss: 0.0486, Train Recon Loss: 0.0475, Train Entropy Loss: 0.9315, Train ARI: 0.5374, Val Total Loss: 0.0492, Val Recon Loss: 0.0481, Val Entropy Loss: 0.9323, Val ARI: 0.5178\n",
      "Epoch [21/25], Lambda Entropy: 0.001454, Train Total Loss: 0.0485, Train Recon Loss: 0.0471, Train Entropy Loss: 0.9309, Train ARI: 0.5151, Val Total Loss: 0.0493, Val Recon Loss: 0.0480, Val Entropy Loss: 0.9350, Val ARI: 0.5445\n",
      "Epoch [22/25], Lambda Entropy: 0.001782, Train Total Loss: 0.0487, Train Recon Loss: 0.0471, Train Entropy Loss: 0.9304, Train ARI: 0.5183, Val Total Loss: 0.0500, Val Recon Loss: 0.0483, Val Entropy Loss: 0.9337, Val ARI: 0.5148\n",
      "Epoch [23/25], Lambda Entropy: 0.002183, Train Total Loss: 0.0490, Train Recon Loss: 0.0470, Train Entropy Loss: 0.9286, Train ARI: 0.5044, Val Total Loss: 0.0502, Val Recon Loss: 0.0482, Val Entropy Loss: 0.9306, Val ARI: 0.5275\n",
      "Epoch [24/25], Lambda Entropy: 0.002672, Train Total Loss: 0.0494, Train Recon Loss: 0.0469, Train Entropy Loss: 0.9258, Train ARI: 0.4882, Val Total Loss: 0.0504, Val Recon Loss: 0.0479, Val Entropy Loss: 0.9289, Val ARI: 0.5058\n",
      "Epoch [25/25], Lambda Entropy: 0.003270, Train Total Loss: 0.0498, Train Recon Loss: 0.0468, Train Entropy Loss: 0.9205, Train ARI: 0.4886, Val Total Loss: 0.0512, Val Recon Loss: 0.0482, Val Entropy Loss: 0.9209, Val ARI: 0.4991\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Assuming 'df' is your original dataframe\n",
    "\n",
    "# Identify column prefixes for each true factor\n",
    "factor_columns = {\n",
    "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
    "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
    "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
    "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
    "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
    "}\n",
    "\n",
    "# Map factor names to column indices\n",
    "factor_indices = {}\n",
    "for factor_name, columns in factor_columns.items():\n",
    "    indices = [df.columns.get_loc(col) for col in columns]\n",
    "    factor_indices[factor_name] = indices\n",
    "\n",
    "# Create true_labels based on the factor_indices\n",
    "input_dim = len(df.columns)\n",
    "true_labels = np.full(input_dim, -1, dtype=int)  # Initialize with -1\n",
    "\n",
    "# Map factor names to indices\n",
    "factor_names = ['Factor1', 'Factor2', 'Factor3', 'Factor4', 'Factor5']\n",
    "factor_name_to_index = {name: idx for idx, name in enumerate(factor_names)}\n",
    "\n",
    "# Assign labels to observed variables based on factors\n",
    "for factor_name, indices in factor_indices.items():\n",
    "    factor_idx = factor_name_to_index[factor_name]\n",
    "    true_labels[indices] = factor_idx\n",
    "\n",
    "# Ensure all observed variables have been assigned\n",
    "assert np.all(true_labels >= 0), \"Some observed variables have not been assigned a true label\"\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = len(df.columns)  # Number of observed variables\n",
    "output_dim = 5               # Output dimension of the encoder (number of factors)\n",
    "embedding_dim = 64           # Embedding dimension for the embeddings e and e_i's\n",
    "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
    "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 25          # Number of epochs\n",
    "batch_size = 32          # Batch size (already set in the DataLoader)\n",
    "print_every = 1          # How often to print loss (in epochs)\n",
    "\n",
    "# Define the maximum value for the entropy regularization coefficient\n",
    "max_lambda_entropy = 0.4*1e-2  # Adjust this value as needed\n",
    "\n",
    "# Flag to enable or disable entropy regularizer\n",
    "use_entropy_regularizer = True # Set to True to enable, False to disable\n",
    "\n",
    "def get_lambda_entropy(epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=True):\n",
    "    if not use_entropy_regularizer:\n",
    "        return 0.0\n",
    "    if schedule_type == 'constant':\n",
    "        # Always return max_lambda_entropy\n",
    "        return max_lambda_entropy\n",
    "    elif schedule_type == 'linear':\n",
    "        # Linear increase from 0 to max_lambda_entropy\n",
    "        return max_lambda_entropy * (epoch / num_epochs)\n",
    "    elif schedule_type == 'exponential':\n",
    "        # Exponential increase from 0 to max_lambda_entropy\n",
    "        k = 5  # Adjust this value to control the rate of increase\n",
    "        numerator = math.exp(k * epoch / num_epochs) - 1\n",
    "        denominator = math.exp(k) - 1\n",
    "        return max_lambda_entropy * (numerator / denominator)\n",
    "    elif schedule_type == 'logarithmic':\n",
    "        # Logarithmic increase from 0 to max_lambda_entropy\n",
    "        if epoch == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return max_lambda_entropy * math.log(epoch + 1) / math.log(num_epochs + 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
    "\n",
    "# Define a function to compute ARI between true and predicted groupings\n",
    "def compute_ari_per_sample(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Computes the ARI between true_labels and predicted_labels.\n",
    "    Both true_labels and predicted_labels should be 1D arrays of the same length.\n",
    "    \"\"\"\n",
    "    assert true_labels.shape == predicted_labels.shape, f\"Shapes of true_labels {true_labels.shape} and predicted_labels {predicted_labels.shape} do not match\"\n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    return ari\n",
    "\n",
    "# Load the trained model if it exists\n",
    "model_path = \"trained_model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Trained model found. Loading the model.\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    # Since we saved the state_dict directly, we can load it directly\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"No trained model found. Starting training from scratch.\")\n",
    "\n",
    "# Initialize a list to store the average attention matrices per epoch\n",
    "attention_matrices = []\n",
    "\n",
    "# More interpretable entropy normalization\n",
    "ent_norm = 1.0 / (input_dim * math.log(output_dim))\n",
    "\n",
    "# Initialize lists to store ARIs\n",
    "train_ari_list = []\n",
    "val_ari_list = []\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute lambda_entropy for the current epoch\n",
    "    lambda_entropy = get_lambda_entropy(\n",
    "        epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=use_entropy_regularizer)\n",
    "\n",
    "    # Assert that lambda_entropy is within expected bounds\n",
    "    assert lambda_entropy >= 0.0, f\"Lambda entropy should be non-negative, got {lambda_entropy}\"\n",
    "    assert lambda_entropy <= max_lambda_entropy, f\"Lambda entropy should not exceed max_lambda_entropy ({max_lambda_entropy}), got {lambda_entropy}\"\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0          # Accumulates total loss (reconstruction + regularizer)\n",
    "    running_recon_loss = 0.0    # Accumulates reconstruction loss\n",
    "    running_entropy_loss = 0.0  # Accumulates entropy loss\n",
    "    epoch_attn_weights = []     # List to store attention weights for all batches in the epoch\n",
    "    epoch_ari = []              # List to store average ARI per batch\n",
    "\n",
    "    for batch_idx, (batch,) in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Assert that batch has expected shape\n",
    "        batch_size = batch.size(0)\n",
    "        assert batch.dim() == 2, f\"Expected batch to be a 2D tensor, got {batch.dim()}D tensor\"\n",
    "        assert batch.shape[1] == input_dim, f\"Expected batch to have {input_dim} features, got {batch.shape[1]}\"\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute predicted x_hat and attention weights\n",
    "        x_hat, attn_weights = model(batch)\n",
    "\n",
    "        # Assert shapes of x_hat and attn_weights\n",
    "        assert x_hat.shape == (batch_size, input_dim), f\"Expected x_hat shape ({batch_size}, {input_dim}), got {x_hat.shape}\"\n",
    "        assert attn_weights.dim() == 3, f\"Expected attn_weights to be 3D tensor, got {attn_weights.dim()}D tensor\"\n",
    "        assert attn_weights.shape == (batch_size, input_dim, output_dim), f\"Expected attn_weights shape ({batch_size}, {input_dim}, {output_dim}), got {attn_weights.shape}\"\n",
    "\n",
    "        # No need to squeeze attn_weights since it's already 3D\n",
    "        # Collect attention weights for the epoch\n",
    "        epoch_attn_weights.append(attn_weights.detach().cpu())\n",
    "\n",
    "        # Compute the reconstruction loss\n",
    "        recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "        # Assert that recon_loss is a scalar\n",
    "        assert recon_loss.dim() == 0, f\"Expected recon_loss to be a scalar, got tensor with shape {recon_loss.shape}\"\n",
    "\n",
    "        # Initialize entropy_regularizer to zero\n",
    "        entropy_regularizer = 0.0\n",
    "\n",
    "        # Add a small epsilon to prevent log(0)\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        # Compute entropy for each query (input_dim)\n",
    "        entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Assert entropy shape\n",
    "        assert entropy.shape == (batch_size, input_dim), f\"Expected entropy shape ({batch_size}, {input_dim}), got {entropy.shape}\"\n",
    "\n",
    "        # Sum entropies over queries and average over batch\n",
    "        entropy_regularizer = ent_norm * torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
    "\n",
    "        # Assert that entropy_regularizer is scalar\n",
    "        assert entropy_regularizer.dim() == 0, f\"Expected entropy_regularizer to be a scalar, got tensor with shape {entropy_regularizer.shape}\"\n",
    "\n",
    "        # Total loss\n",
    "        loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "        # Assert that loss is scalar\n",
    "        assert loss.dim() == 0, f\"Expected loss to be a scalar, got tensor with shape {loss.shape}\"\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training losses\n",
    "        running_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_entropy_loss += entropy_regularizer.item()  # Accumulate entropy regularizer loss\n",
    "\n",
    "        # Compute ARI per sample and average over batch\n",
    "        attn_weights_np = attn_weights.detach().cpu().numpy()  # Shape: (batch_size, input_dim, output_dim)\n",
    "        batch_ari = []\n",
    "        for i in range(batch_size):\n",
    "            # For each sample in the batch\n",
    "            predicted_labels = np.argmax(attn_weights_np[i], axis=1)  # Shape: (input_dim,)\n",
    "            # Compute ARI between true_labels and predicted_labels\n",
    "            ari = compute_ari_per_sample(true_labels, predicted_labels)\n",
    "            batch_ari.append(ari)\n",
    "        # Average ARI over batch\n",
    "        avg_ari_batch = np.mean(batch_ari)\n",
    "        epoch_ari.append(avg_ari_batch)\n",
    "\n",
    "    # Compute average losses and ARI for training\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_train_recon_loss = running_recon_loss / len(train_loader)\n",
    "    avg_train_entropy_loss = running_entropy_loss / len(train_loader)\n",
    "    avg_train_ari = np.mean(epoch_ari)\n",
    "    train_ari_list.append(avg_train_ari)\n",
    "\n",
    "    # Compute the average attention matrix for the epoch\n",
    "    epoch_attn_weights_tensor = torch.cat(epoch_attn_weights, dim=0)  # Shape: (num_samples_in_epoch, input_dim, output_dim)\n",
    "\n",
    "    # Assert shape of epoch_attn_weights_tensor\n",
    "    num_samples_in_epoch = epoch_attn_weights_tensor.size(0)\n",
    "    assert epoch_attn_weights_tensor.shape == (num_samples_in_epoch, input_dim, output_dim), f\"Expected epoch_attn_weights_tensor shape ({num_samples_in_epoch}, {input_dim}, {output_dim}), got {epoch_attn_weights_tensor.shape}\"\n",
    "\n",
    "    avg_attn_weights_epoch = epoch_attn_weights_tensor.mean(dim=0)    # Shape: (input_dim, output_dim)\n",
    "\n",
    "    # Assert shape of avg_attn_weights_epoch\n",
    "    assert avg_attn_weights_epoch.shape == (input_dim, output_dim), f\"Expected avg_attn_weights_epoch shape ({input_dim}, {output_dim}), got {avg_attn_weights_epoch.shape}\"\n",
    "\n",
    "    avg_attn_weights_epoch_np = avg_attn_weights_epoch.numpy()\n",
    "\n",
    "    # Transpose to have shape (output_dim, input_dim) so that queries are on x-axis and keys on y-axis\n",
    "    attention_matrices.append(avg_attn_weights_epoch_np.T)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0          # Accumulates total loss (reconstruction + regularizer)\n",
    "    val_recon_loss = 0.0    # Accumulates reconstruction loss\n",
    "    val_entropy_loss = 0.0  # Accumulates entropy regularizer loss\n",
    "    epoch_attn_weights_val = []\n",
    "    epoch_ari_val = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch,) in enumerate(val_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Assert that batch has expected shape\n",
    "            batch_size = batch.size(0)\n",
    "            assert batch.dim() == 2, f\"Expected batch to be a 2D tensor, got {batch.dim()}D tensor\"\n",
    "            assert batch.shape[1] == input_dim, f\"Expected batch to have {input_dim} features, got {batch.shape[1]}\"\n",
    "\n",
    "            # Forward pass for validation\n",
    "            x_hat, attn_weights = model(batch)\n",
    "\n",
    "            # Assert shapes of x_hat and attn_weights\n",
    "            assert x_hat.shape == (batch_size, input_dim), f\"Expected x_hat shape ({batch_size}, {input_dim}), got {x_hat.shape}\"\n",
    "            assert attn_weights.dim() == 3, f\"Expected attn_weights to be 3D tensor, got {attn_weights.dim()}D tensor\"\n",
    "            assert attn_weights.shape == (batch_size, input_dim, output_dim), f\"Expected attn_weights shape ({batch_size}, {input_dim}, {output_dim}), got {attn_weights.shape}\"\n",
    "\n",
    "            # Collect attention weights for validation\n",
    "            epoch_attn_weights_val.append(attn_weights.detach().cpu())\n",
    "\n",
    "            # Compute the reconstruction loss\n",
    "            recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "            # Assert that recon_loss is a scalar\n",
    "            assert recon_loss.dim() == 0, f\"Expected recon_loss to be a scalar, got tensor with shape {recon_loss.shape}\"\n",
    "\n",
    "            # Initialize entropy_regularizer to zero\n",
    "            entropy_regularizer = 0.0\n",
    "\n",
    "            # Add a small epsilon to prevent log(0)\n",
    "            epsilon = 1e-8\n",
    "\n",
    "            # Compute entropy for each query (input_dim)\n",
    "            entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "            # Assert entropy shape\n",
    "            assert entropy.shape == (batch_size, input_dim), f\"Expected entropy shape ({batch_size}, {input_dim}), got {entropy.shape}\"\n",
    "\n",
    "            # Sum entropies over queries and average over batch\n",
    "            entropy_regularizer = ent_norm * torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
    "\n",
    "            # Assert that entropy_regularizer is scalar\n",
    "            assert entropy_regularizer.dim() == 0, f\"Expected entropy_regularizer to be a scalar, got tensor with shape {entropy_regularizer.shape}\"\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "            # Assert that loss is scalar\n",
    "            assert loss.dim() == 0, f\"Expected loss to be a scalar, got tensor with shape {loss.shape}\"\n",
    "\n",
    "            # Accumulate validation losses\n",
    "            val_loss += loss.item()\n",
    "            val_recon_loss += recon_loss.item()\n",
    "            val_entropy_loss += entropy_regularizer.item()  # Accumulate entropy regularizer loss\n",
    "\n",
    "            # Compute ARI per sample and average over batch\n",
    "            attn_weights_np = attn_weights.detach().cpu().numpy()  # Shape: (batch_size, input_dim, output_dim)\n",
    "            batch_ari = []\n",
    "            for i in range(batch_size):\n",
    "                # For each sample in the batch\n",
    "                predicted_labels = np.argmax(attn_weights_np[i], axis=1)  # Shape: (input_dim,)\n",
    "                # Compute ARI between true_labels and predicted_labels\n",
    "                ari = compute_ari_per_sample(true_labels, predicted_labels)\n",
    "                batch_ari.append(ari)\n",
    "            # Average ARI over batch\n",
    "            avg_ari_batch = np.mean(batch_ari)\n",
    "            epoch_ari_val.append(avg_ari_batch)\n",
    "\n",
    "        # Compute average losses and ARI for validation\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_recon_loss = val_recon_loss / len(val_loader)\n",
    "        avg_val_entropy_loss = val_entropy_loss / len(val_loader)\n",
    "        avg_val_ari = np.mean(epoch_ari_val)\n",
    "        val_ari_list.append(avg_val_ari)\n",
    "\n",
    "    # Print average losses and ARI for the epoch\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Lambda Entropy: {lambda_entropy:.6f}, '\n",
    "              f'Train Total Loss: {avg_train_loss:.4f}, Train Recon Loss: {avg_train_recon_loss:.4f}, Train Entropy Loss: {avg_train_entropy_loss:.4f}, Train ARI: {avg_train_ari:.4f}, '\n",
    "              f'Val Total Loss: {avg_val_loss:.4f}, Val Recon Loss: {avg_val_recon_loss:.4f}, Val Entropy Loss: {avg_val_entropy_loss:.4f}, Val ARI: {avg_val_ari:.4f}')\n",
    "\n",
    "# Save the trained model after training\n",
    "# torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "# print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.animation import FuncAnimation\n",
    "# from IPython.display import HTML, display\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# # Assuming 'df' is your original dataframe\n",
    "\n",
    "# # Identify column prefixes for each true factor\n",
    "# factor_columns = {\n",
    "#     'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
    "#     'Factor2': [col for col in df.columns if col.startswith('green')],\n",
    "#     'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
    "#     'Factor4': [col for col in df.columns if col.startswith('red')],\n",
    "#     'Factor5': [col for col in df.columns if col.startswith('q')]\n",
    "# }\n",
    "\n",
    "# # Map factor names to column indices\n",
    "# factor_indices = {}\n",
    "# for factor_name, columns in factor_columns.items():\n",
    "#     indices = [df.columns.get_loc(col) for col in columns]\n",
    "#     factor_indices[factor_name] = indices\n",
    "\n",
    "# Create a new ordering of indices\n",
    "new_order = []\n",
    "for factor_name in factor_columns.keys():\n",
    "    new_order.extend(factor_indices[factor_name])\n",
    "\n",
    "# Ensure all indices are included\n",
    "assert len(new_order) == df.shape[1], \"Not all indices are included in the new order.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rearranging attention matrices for the animation\n",
    "# reordered_attention_matrices = []\n",
    "\n",
    "# for attn_matrix in attention_matrices:\n",
    "#     # attn_matrix has shape (output_dim, input_dim)\n",
    "#     # Rearrange the columns (observed variables) according to new_order\n",
    "#     attn_matrix_reordered = attn_matrix[:, new_order]  # Shape: (output_dim, input_dim)\n",
    "#     reordered_attention_matrices.append(attn_matrix_reordered)\n",
    "\n",
    "# # Convert the list to a NumPy array for animation\n",
    "# attention_matrices_array = np.stack(reordered_attention_matrices)  # Shape: (num_epochs, output_dim, input_dim)\n",
    "\n",
    "# # Determine the number of frames (epochs)\n",
    "# num_frames = attention_matrices_array.shape[0]\n",
    "\n",
    "# # Set up the figure and axis for the animation\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# # Initialize the heatmap with empty data\n",
    "# im = ax.imshow(np.zeros_like(attention_matrices_array[0]), aspect='auto', cmap='viridis', vmin=0, vmax=1)\n",
    "# ax.set_xlabel('Observed Variables')\n",
    "# ax.set_ylabel('Latent Factors')\n",
    "# title = ax.set_title('Attention Matrix at Epoch 1')\n",
    "\n",
    "# # Function to initialize the heatmap\n",
    "# def init():\n",
    "#     data = attention_matrices_array[0]\n",
    "#     im.set_data(data)\n",
    "#     title.set_text('Attention Matrix at Epoch 1')\n",
    "#     return [im, title]\n",
    "\n",
    "# # Function to update the heatmap for each frame\n",
    "# def update(frame):\n",
    "#     data = attention_matrices_array[frame]\n",
    "#     im.set_data(data)\n",
    "#     title.set_text(f'Attention Matrix at Epoch {frame + 1}')\n",
    "#     return [im, title]\n",
    "\n",
    "# # Create the animation\n",
    "# ani = FuncAnimation(fig, update, frames=num_frames, init_func=init, blit=True)\n",
    "\n",
    "# # Close the figure to prevent the static plot from displaying\n",
    "# plt.close(fig)\n",
    "\n",
    "# # Display the animation in the notebook\n",
    "# display(HTML(ani.to_jshtml()))\n",
    "\n",
    "# # Save the animation to an MP4 file (optional)\n",
    "# # ani.save('attention_animation.mp4', writer='ffmpeg', fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average attention matrices (assuming you have 'model', 'train_loader', and 'val_loader')\n",
    "def compute_average_attention(model, dataloader, device):\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "    total_attn = None\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs = data[0].to(device)\n",
    "\n",
    "            # Forward pass up to obtaining attention weights\n",
    "            hat_Z = model.encoder(inputs)\n",
    "            _, attn_weights = model.decoder(hat_Z)\n",
    "\n",
    "            # attn_weights shape: (batch_size, input_dim, output_dim)\n",
    "            batch_size = attn_weights.size(0)\n",
    "            if total_attn is None:\n",
    "                total_attn = attn_weights.sum(dim=0)  # Sum over batch dimension\n",
    "            else:\n",
    "                total_attn += attn_weights.sum(dim=0)\n",
    "            num_samples += batch_size\n",
    "\n",
    "    # Average the attention weights\n",
    "    avg_attn = total_attn / num_samples\n",
    "\n",
    "    return avg_attn.cpu().numpy()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Compute average attention matrices\n",
    "avg_attn_train = compute_average_attention(model, train_loader, device)\n",
    "avg_attn_val = compute_average_attention(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index: 0.5811047786058746\n"
     ]
    }
   ],
   "source": [
    "# Calculating ARI\n",
    "input_dim = avg_attn_train.shape[0]   # Number of observed variables\n",
    "output_dim = avg_attn_train.shape[1]  # Number of latent factors\n",
    "\n",
    "# Step 1: Assign each observed variable to the latent factor with the highest attention weight\n",
    "predicted_labels = np.argmax(avg_attn_train, axis=1)  # Shape: (input_dim,)\n",
    "\n",
    "# Step 2: Create true labels based on ideal groups\n",
    "true_labels = np.full(input_dim, -1)  # Initialize with -1\n",
    "\n",
    "# Map factor names to indices\n",
    "factor_names = ['Factor1', 'Factor2', 'Factor3', 'Factor4', 'Factor5']\n",
    "factor_name_to_index = {name: idx for idx, name in enumerate(factor_names)}\n",
    "\n",
    "# Assign labels to observed variables based on factors\n",
    "for factor_name, indices in factor_indices.items():\n",
    "    factor_idx = factor_name_to_index[factor_name]\n",
    "    true_labels[indices] = factor_idx\n",
    "\n",
    "# Ensure all observed variables have been assigned\n",
    "assert np.all(true_labels >= 0), \"Some observed variables have not been assigned a true label\"\n",
    "\n",
    "# Step 3: Compute Adjusted Rand Index\n",
    "ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Adjusted Rand Index:\", ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Transpose the average attention matrices\n",
    "avg_attn_train_transposed = avg_attn_train.T  # Shape: (output_dim, input_dim)\n",
    "avg_attn_val_transposed = avg_attn_val.T      # Shape: (output_dim, input_dim)\n",
    "\n",
    "# Rearrange the columns (observed variables) using new_order\n",
    "avg_attn_train_reordered = avg_attn_train_transposed[:, new_order]\n",
    "avg_attn_val_reordered = avg_attn_val_transposed[:, new_order]\n",
    "\n",
    "def plot_attention_heatmap(attn_matrix, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        attn_matrix,\n",
    "        cmap='viridis',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Observed Variables')\n",
    "    plt.ylabel('Latent Factors')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot average attention matrices with fixed color scaling\n",
    "plot_attention_heatmap(avg_attn_train_reordered, 'Average Attention Matrix - Training Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
