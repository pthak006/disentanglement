{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corex transformer on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (50000, 50)\n",
      "Test Data Shape: (12500, 50)\n",
      "Number of training batches: 1563\n",
      "Number of test batches: 391\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "# Import the SyntheticData class from factor_eval.py\n",
    "# Make sure factor_eval.py is in the same directory as your notebook\n",
    "from factor_eval import SyntheticData\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "n_samples = 50000\n",
    "n_test = 12500\n",
    "n_sources = 5\n",
    "k = 10\n",
    "snr = 5\n",
    "correlate_sources = False\n",
    "get_covariance = False\n",
    "random_scale = False\n",
    "nuisance = 0\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Generate synthetic data\n",
    "# We want:\n",
    "# - 50,000 training samples, 12,500 test samples\n",
    "# - 5 latent factors (n_sources=5), k=10 variables per factor => 1170 observed variables in total\n",
    "# - SNR = 0.5 for moderate noise\n",
    "# - No correlation between sources (correlate_sources=False)\n",
    "# - No nuisance variables (nuisance=0)\n",
    "# - Use a fixed seed for reproducibility\n",
    "synthetic_data = SyntheticData(\n",
    "    n_samples=n_samples,\n",
    "    n_test=n_test, \n",
    "    n_sources=n_sources,\n",
    "    k=k,\n",
    "    snr=snr,             # Lower SNR to increase noise\n",
    "    correlate_sources=correlate_sources,  # Introduce correlation among latent factors\n",
    "    get_covariance=get_covariance,\n",
    "    random_scale=random_scale,    # Randomly scale variables for additional complexity\n",
    "    nuisance=nuisance,           # No nuisance variables\n",
    "    seed=seed               # Fixed seed for reproducibility\n",
    ")\n",
    "\n",
    "n_observed = synthetic_data.train.shape[1]\n",
    "# Extract training and test sets\n",
    "X_train = synthetic_data.train\n",
    "X_test = synthetic_data.test\n",
    "\n",
    "# -------------------------- Normalization Block (BEGIN) --------------------------\n",
    "# Compute min and max from the training data\n",
    "train_min = np.min(X_train, axis=0, keepdims=True)\n",
    "train_max = np.max(X_train, axis=0, keepdims=True)\n",
    "\n",
    "# Apply min-max normalization: (X - min) / (max - min)\n",
    "# Handle the case where max == min to avoid division by zero (if any variable is constant)\n",
    "denominator = (train_max - train_min)\n",
    "denominator[denominator == 0] = 1e-8  # A small number to avoid division by zero\n",
    "\n",
    "X_train = (X_train - train_min) / denominator\n",
    "X_test = (X_test - train_min) / denominator\n",
    "# -------------------------- Normalization Block (END) --------------------------\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets for training and test sets\n",
    "train_dataset = TensorDataset(train_tensor)\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "\n",
    "# Create a generator for deterministic shuffling\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "# We don't need to do a random split now since SyntheticData already provides train/test sets\n",
    "# We'll just create a RandomSampler for the train dataset to ensure reproducible shuffling\n",
    "train_sampler = RandomSampler(train_dataset, generator=generator)\n",
    "\n",
    "# Create DataLoaders for training and test sets\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Display shapes and verify\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Test Data Shape: {X_test.shape}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of training data:\n",
      "[[0.50868695 0.4987838  0.61679009 0.51139569 0.57808391 0.53119005\n",
      "  0.59510766 0.49915261 0.48816219 0.56378535 0.54389231 0.46161187\n",
      "  0.44176781 0.44011464 0.5212177  0.44182656 0.50323608 0.47318434\n",
      "  0.48656391 0.43129428 0.62553241 0.63957391 0.55248758 0.50820067\n",
      "  0.5742377  0.5785061  0.5395921  0.58961463 0.62899889 0.52459604\n",
      "  0.59427365 0.68768851 0.60007923 0.67818638 0.65084412 0.63100136\n",
      "  0.65105545 0.60424543 0.67327988 0.57388115 0.45481663 0.58123842\n",
      "  0.5387345  0.37032482 0.46125067 0.51695561 0.4722074  0.52507556\n",
      "  0.42781294 0.38517918]\n",
      " [0.48820029 0.41353023 0.45875492 0.43953774 0.46819887 0.50459208\n",
      "  0.4889965  0.47278801 0.51049108 0.49991677 0.63305502 0.63168452\n",
      "  0.68632706 0.72601394 0.65869075 0.69213919 0.69485117 0.65005208\n",
      "  0.52951655 0.60731036 0.63851802 0.54294673 0.591343   0.58300996\n",
      "  0.56239008 0.5920765  0.61931209 0.52943182 0.61405599 0.63433162\n",
      "  0.38440047 0.51977148 0.40141907 0.39123537 0.49510953 0.48944575\n",
      "  0.45425396 0.43832339 0.47266266 0.45655738 0.50610283 0.60363285\n",
      "  0.56628643 0.42117238 0.58289333 0.65146169 0.48652048 0.50339252\n",
      "  0.40926248 0.49582224]\n",
      " [0.48147637 0.45589568 0.49956868 0.49070448 0.43052276 0.4151708\n",
      "  0.55507923 0.51957538 0.45185397 0.42118945 0.46856721 0.47383979\n",
      "  0.46014611 0.42999676 0.42412445 0.43945044 0.35801432 0.30829401\n",
      "  0.45092991 0.37948903 0.55296598 0.45044464 0.61399752 0.44212254\n",
      "  0.52276576 0.59201676 0.52445453 0.50389102 0.53186127 0.61597683\n",
      "  0.33380187 0.28558073 0.28977169 0.2728595  0.30993147 0.29823597\n",
      "  0.34530745 0.34815231 0.31216106 0.2720615  0.31140434 0.34655416\n",
      "  0.25315457 0.24899563 0.34501331 0.34439646 0.38616755 0.27713212\n",
      "  0.30393631 0.36241099]\n",
      " [0.48743644 0.43168662 0.47252165 0.46558521 0.43245762 0.46714296\n",
      "  0.47750878 0.44329325 0.44072348 0.45120618 0.39051557 0.36939755\n",
      "  0.40965503 0.41049236 0.41131896 0.40264042 0.35745904 0.4015953\n",
      "  0.42111845 0.36435036 0.53322874 0.63238612 0.52829614 0.49168098\n",
      "  0.48327992 0.46072422 0.54291703 0.43656055 0.49518802 0.4604278\n",
      "  0.44145454 0.45722381 0.38646667 0.48165979 0.40634871 0.36912124\n",
      "  0.47917115 0.53836064 0.49803943 0.47503083 0.35302483 0.34075754\n",
      "  0.25908798 0.32182878 0.28569193 0.37826652 0.37249523 0.32181662\n",
      "  0.50172057 0.38736368]\n",
      " [0.75022736 0.64869172 0.65626663 0.69999725 0.72872545 0.71270592\n",
      "  0.70099533 0.75061753 0.74431042 0.70943898 0.58096042 0.48555598\n",
      "  0.47318929 0.47290671 0.58755651 0.42923928 0.45525765 0.54740379\n",
      "  0.42504714 0.32098959 0.49648499 0.58129485 0.50171251 0.51892119\n",
      "  0.46769552 0.48364103 0.5441408  0.4653181  0.44218611 0.43948071\n",
      "  0.34003512 0.28291872 0.31166754 0.35040188 0.25756208 0.32681988\n",
      "  0.39966015 0.35153711 0.40331815 0.38692259 0.4224001  0.44098025\n",
      "  0.4364913  0.4108753  0.37019508 0.47893857 0.43128051 0.42748463\n",
      "  0.51297554 0.42755595]]\n",
      "\n",
      "Summary statistics for the first 5 columns of the training data:\n",
      "Column 0: mean=0.5221, std=0.1226, min=0.0000, max=1.0000\n",
      "Column 1: mean=0.5110, std=0.1258, min=0.0000, max=1.0000\n",
      "Column 2: mean=0.5005, std=0.1233, min=0.0000, max=1.0000\n",
      "Column 3: mean=0.5086, std=0.1274, min=0.0000, max=1.0000\n",
      "Column 4: mean=0.5043, std=0.1163, min=0.0000, max=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first 5 rows of the training data\n",
    "print(\"First 5 rows of training data:\")\n",
    "print(X_train[:5])\n",
    "\n",
    "# Let's also print summary statistics of the first few columns to see their distribution\n",
    "num_cols_to_inspect = 5  # you can change this number\n",
    "cols_to_inspect = X_train[:, :num_cols_to_inspect]\n",
    "\n",
    "print(\"\\nSummary statistics for the first 5 columns of the training data:\")\n",
    "means = cols_to_inspect.mean(axis=0)\n",
    "stds = cols_to_inspect.std(axis=0)\n",
    "mins = cols_to_inspect.min(axis=0)\n",
    "maxs = cols_to_inspect.max(axis=0)\n",
    "\n",
    "for i in range(num_cols_to_inspect):\n",
    "    print(f\"Column {i}: mean={means[i]:.4f}, std={stds[i]:.4f}, min={mins[i]:.4f}, max={maxs[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rywEvDazCYkL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Encoder class with explicit layer definitions and assert statements\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=n_observed, hidden_dims=[128, 64], output_dim=5, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Assertions for input dimensions\n",
    "        assert input_dim == n_observed, f\"Expected input_dim to be 50, but got {input_dim}\"\n",
    "        assert output_dim == 5, f\"Expected output_dim to be 5, but got {output_dim}\"\n",
    "        assert embedding_dim == 64, f\"Expected embedding_dim to be 64, but got {embedding_dim}\"\n",
    "\n",
    "        # Define the layers explicitly\n",
    "        self.layers = nn.ModuleList()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "\n",
    "        # Create Linear layers\n",
    "        for i in range(len(dims) - 1):\n",
    "            in_features = dims[i]\n",
    "            out_features = dims[i + 1]\n",
    "            layer = nn.Linear(in_features, out_features)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # Learnable embedding vectors e_i for each z_i\n",
    "        self.e = nn.Parameter(torch.randn(output_dim, embedding_dim))\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Assert the number of Linear layers\n",
    "        expected_num_linear_layers = len(hidden_dims) + 1  # Number of hidden layers + output layer\n",
    "        actual_num_linear_layers = len(self.layers)\n",
    "        assert actual_num_linear_layers == expected_num_linear_layers, \\\n",
    "            f\"Expected {expected_num_linear_layers} Linear layers, but got {actual_num_linear_layers}\"\n",
    "\n",
    "        # Assert the input and output dimensions of the Linear layers\n",
    "        # First Linear layer\n",
    "        first_linear = self.layers[0]\n",
    "        assert first_linear.in_features == n_observed, \\\n",
    "            f\"Expected first Linear layer to have input features 50, but got {first_linear.in_features}\"\n",
    "\n",
    "        # Last Linear layer\n",
    "        last_linear = self.layers[-1]\n",
    "        assert last_linear.out_features == 5, \\\n",
    "            f\"Expected last Linear layer to have output features 5, but got {last_linear.out_features}\"\n",
    "\n",
    "        # Assert the shape of e\n",
    "        assert self.e.shape == (5, 64), \\\n",
    "            f\"Expected embedding matrix e to have shape (5, 64), but got {self.e.shape}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assert the input shape\n",
    "        assert x.dim() == 2, f\"Expected input x to be a 2D tensor, but got {x.dim()}D tensor\"\n",
    "        assert x.shape[1] == n_observed, f\"Expected input x to have 50 features, but got {x.shape[1]}\"\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Pass the input through the Linear layers with ReLU activations\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # Apply ReLU after each hidden layer except the last layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "\n",
    "        Z = x  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Assert the shape of Z\n",
    "        assert Z.shape == (batch_size, 5), \\\n",
    "            f\"Expected Z to have shape ({batch_size}, 5), but got {Z.shape}\"\n",
    "\n",
    "        # Convert Z to \\hat Z by multiplying each scalar z_i with its own embedding vector e_i\n",
    "        Z_expanded = Z.unsqueeze(2)  # Shape: (batch_size, output_dim, 1)\n",
    "        assert Z_expanded.shape == (batch_size, 5, 1), \\\n",
    "            f\"Expected Z_expanded to have shape ({batch_size}, 5, 1), but got {Z_expanded.shape}\"\n",
    "\n",
    "        e_expanded = self.e.unsqueeze(0)  # Shape: (1, output_dim, embedding_dim)\n",
    "        assert e_expanded.shape == (1, 5, 64), \\\n",
    "            f\"Expected e_expanded to have shape (1, 5, 64), but got {e_expanded.shape}\"\n",
    "\n",
    "        # Multiply Z_expanded and e_expanded to get hat_Z\n",
    "        hat_Z = Z_expanded * e_expanded  # Shape: (batch_size, output_dim, embedding_dim)\n",
    "        assert hat_Z.shape == (batch_size, 5, 64), \\\n",
    "            f\"Expected hat_Z to have shape ({batch_size}, 5, 64), but got {hat_Z.shape}\"\n",
    "\n",
    "        return hat_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tUXzn0AHHq1T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim=n_observed, embedding_dim=64, hidden_dims=[]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim      # Number of observed variables (n)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Assert input dimensions\n",
    "        assert input_dim == n_observed, f\"Expected input_dim to be 50, but got {input_dim}\"\n",
    "        assert embedding_dim == 64, f\"Expected embedding_dim to be 64, but got {embedding_dim}\"\n",
    "\n",
    "        # Learnable query embeddings (e1, e2, ..., en)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
    "\n",
    "        # Assert query_embeddings shape\n",
    "        assert self.query_embeddings.shape == (n_observed, 64), \\\n",
    "            f\"Expected query_embeddings to have shape (50, 64), but got {self.query_embeddings.shape}\"\n",
    "\n",
    "        # MultiheadAttention module with 1 head\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Define individual MLPs for each observed variable\n",
    "        dims = [embedding_dim] + hidden_dims + [1]\n",
    "\n",
    "        # Create MLPs for each observed variable\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            nn.Sequential(*[\n",
    "                nn.Linear(dims[i], dims[i + 1]) if i == len(dims) - 2 else nn.Sequential(\n",
    "                    nn.Linear(dims[i], dims[i + 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                for i in range(len(dims) - 1)\n",
    "            ])\n",
    "            for _ in range(input_dim)\n",
    "        ])\n",
    "\n",
    "        # Assert that we have one MLP per observed variable\n",
    "        assert len(self.mlp_layers) == n_observed, \\\n",
    "            f\"Expected 50 MLPs in mlp_layers, but got {len(self.mlp_layers)}\"\n",
    "\n",
    "        # Verify that MLPs do not share parameters\n",
    "        mlp_params = [set(mlp.parameters()) for mlp in self.mlp_layers]\n",
    "        for i in range(len(mlp_params)):\n",
    "            for j in range(i + 1, len(mlp_params)):\n",
    "                assert mlp_params[i].isdisjoint(mlp_params[j]), \\\n",
    "                    f\"MLP {i} and MLP {j} share parameters\"\n",
    "\n",
    "    def forward(self, hat_Z):\n",
    "        \"\"\"\n",
    "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Assert the shape of hat_Z\n",
    "        assert hat_Z.dim() == 3, f\"Expected hat_Z to be a 3D tensor, but got {hat_Z.dim()}D tensor\"\n",
    "        batch_size, output_dim, embedding_dim = hat_Z.shape\n",
    "        assert embedding_dim == 64, \\\n",
    "            f\"Expected hat_Z embedding_dim to be 64, but got {embedding_dim}\"\n",
    "        assert output_dim == 5, \\\n",
    "            f\"Expected hat_Z output_dim to be 5, but got {output_dim}\"\n",
    "\n",
    "        # Prepare query embeddings and expand to batch size\n",
    "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "        assert query_embeddings.shape == (batch_size, n_observed, 64), \\\n",
    "            f\"Expected query_embeddings to have shape ({batch_size}, 50, 64), but got {query_embeddings.shape}\"\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)  # Output shape: (batch_size, input_dim, embedding_dim)\n",
    "        assert attn_output.shape == (batch_size, n_observed, 64), \\\n",
    "            f\"Expected attn_output to have shape ({batch_size}, 50, 64), but got {attn_output.shape}\"\n",
    "        assert attn_weights.shape == (batch_size, n_observed, 5), \\\n",
    "            f\"Expected attn_weights to have shape ({batch_size}, 50, 5), but got {attn_weights.shape}\"\n",
    "\n",
    "        # Add residual connection and apply layer normalization\n",
    "        out = self.layer_norm(attn_output + query_embeddings)  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "        assert out.shape == (batch_size, n_observed, 64), \\\n",
    "            f\"Expected out to have shape ({batch_size}, n_observed, 64), but got {out.shape}\"\n",
    "\n",
    "        # Pass each context vector through its corresponding MLP\n",
    "        x_hat = []\n",
    "        for i in range(self.input_dim):\n",
    "            x_i = out[:, i, :]  # Shape: (batch_size, embedding_dim)\n",
    "            assert x_i.shape == (batch_size, 64), \\\n",
    "                f\"Expected x_i to have shape ({batch_size}, 64), but got {x_i.shape}\"\n",
    "\n",
    "            x_i_hat = self.mlp_layers[i](x_i)  # Shape: (batch_size, 1)\n",
    "            assert x_i_hat.shape == (batch_size, 1), \\\n",
    "                f\"Expected x_i_hat to have shape ({batch_size}, 1), but got {x_i_hat.shape}\"\n",
    "\n",
    "            x_hat.append(x_i_hat)\n",
    "        x_hat = torch.cat(x_hat, dim=1)  # Shape: (batch_size, input_dim)\n",
    "        assert x_hat.shape == (batch_size, n_observed), \\\n",
    "            f\"Expected x_hat to have shape ({batch_size}, 50), but got {x_hat.shape}\"\n",
    "\n",
    "        return x_hat, attn_weights  # Return attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xnBqmgVjIat0"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=encoder_hidden_dims\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            input_dim=input_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=decoder_hidden_dims\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hat_Z = self.encoder(x)     # Obtain \\hat{Z} from the encoder\n",
    "        x_hat, attn_weights = self.decoder(hat_Z)  # Reconstruct x from \\hat{Z} using the decoder\n",
    "        return x_hat, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "No trained model found. Starting training from scratch.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume 'synthetic_data' and 'model' are already defined.\n",
    "# Also assume 'train_loader' and 'test_loader' are defined from previous code.\n",
    "\n",
    "# Extract parameters from synthetic_data\n",
    "input_dim = synthetic_data.train.shape[1]  # Number of observed variables\n",
    "output_dim = synthetic_data.n_sources       # Number of latent factors\n",
    "\n",
    "# true_labels: Use the clusters from synthetic_data directly\n",
    "true_labels = np.array(synthetic_data.clusters, dtype=int)\n",
    "assert np.all(true_labels >= -1), \"Some observed variables have invalid cluster labels.\"\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = n_observed  # Number of observed variables\n",
    "output_dim = 5               # Output dimension of the encoder (number of factors)\n",
    "embedding_dim = 64           # Embedding dimension for the embeddings e and e_i's\n",
    "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
    "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20         # Number of epochs\n",
    "print_every = 1          # How often to print loss (in epochs)\n",
    "\n",
    "# Define the maximum value for the entropy regularization coefficient\n",
    "max_lambda_entropy =  2 * 1e-3  # Adjust this value as needed\n",
    "\n",
    "# Flag to enable or disable entropy regularizer\n",
    "use_entropy_regularizer = True # Set to True to enable entropy regularization\n",
    "\n",
    "def get_lambda_entropy(epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=True):\n",
    "    if not use_entropy_regularizer:\n",
    "        return 0.0\n",
    "    if schedule_type == 'constant':\n",
    "        # Always return max_lambda_entropy\n",
    "        return max_lambda_entropy\n",
    "    elif schedule_type == 'linear':\n",
    "        # Linear increase from 0 to max_lambda_entropy\n",
    "        return max_lambda_entropy * (epoch / num_epochs)\n",
    "    elif schedule_type == 'exponential':\n",
    "        # Exponential increase from 0 to max_lambda_entropy\n",
    "        k = 5  # Adjust this value to control the rate of increase\n",
    "        numerator = math.exp(k * epoch / num_epochs) - 1\n",
    "        denominator = math.exp(k) - 1\n",
    "        return max_lambda_entropy * (numerator / denominator)\n",
    "    elif schedule_type == 'logarithmic':\n",
    "        # Logarithmic increase from 0 to max_lambda_entropy\n",
    "        if epoch == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return max_lambda_entropy * math.log(epoch + 1) / math.log(num_epochs + 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
    "\n",
    "def compute_ari_per_sample(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Computes the ARI between true_labels and predicted_labels.\n",
    "    Both true_labels and predicted_labels should be 1D arrays of the same length.\n",
    "    If there are -1 labels (nuisance), they will be filtered out.\n",
    "    \"\"\"\n",
    "    # Filter out nuisance variables (-1)\n",
    "    mask = true_labels != -1\n",
    "    filtered_true = true_labels[mask]\n",
    "    filtered_pred = predicted_labels[mask]\n",
    "\n",
    "    if len(filtered_true) == 0:\n",
    "        # No variables to compare, default ARI to 1.0 (no meaningful difference)\n",
    "        return 1.0\n",
    "    ari = adjusted_rand_score(filtered_true, filtered_pred)\n",
    "    return ari\n",
    "\n",
    "# Load the trained model if it exists\n",
    "model_path = \"trained_model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Trained model found. Loading the model.\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"No trained model found. Starting training from scratch.\")\n",
    "\n",
    "# Initialize a list to store the average attention matrices per epoch\n",
    "attention_matrices = []\n",
    "\n",
    "# More interpretable entropy normalization\n",
    "ent_norm = 1.0 / (input_dim * math.log(output_dim))\n",
    "\n",
    "# Initialize lists to store ARIs\n",
    "train_ari_list = []\n",
    "test_ari_list = []  # Using 'test_ari_list' instead of 'val_ari_list'\n",
    "\n",
    "# Training loop with testing\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute lambda_entropy for the current epoch\n",
    "    lambda_entropy = get_lambda_entropy(\n",
    "        epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=use_entropy_regularizer\n",
    "    )\n",
    "\n",
    "    # Assert that lambda_entropy is within expected bounds\n",
    "    assert lambda_entropy >= 0.0, f\"Lambda entropy should be non-negative, got {lambda_entropy}\"\n",
    "    assert lambda_entropy <= max_lambda_entropy, f\"Lambda entropy should not exceed max_lambda_entropy ({max_lambda_entropy}), got {lambda_entropy}\"\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_recon_loss = 0.0\n",
    "    running_entropy_loss = 0.0\n",
    "    epoch_attn_weights = []\n",
    "    epoch_ari = []\n",
    "\n",
    "    for batch_idx, (batch,) in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.size(0)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        x_hat, attn_weights = model(batch)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "        # Entropy regularizer\n",
    "        entropy_regularizer = 0.0\n",
    "        epsilon = 1e-8\n",
    "        entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)\n",
    "        entropy_regularizer = ent_norm * torch.mean(torch.sum(entropy, dim=1))\n",
    "\n",
    "        # Total loss\n",
    "        loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        running_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_entropy_loss += entropy_regularizer.item()\n",
    "\n",
    "        # Compute ARI per sample\n",
    "        attn_weights_np = attn_weights.detach().cpu().numpy()\n",
    "        batch_ari_values = []\n",
    "        for i in range(batch_size):\n",
    "            predicted_labels = np.argmax(attn_weights_np[i], axis=1)\n",
    "            ari = compute_ari_per_sample(true_labels, predicted_labels)\n",
    "            batch_ari_values.append(ari)\n",
    "        avg_ari_batch = np.mean(batch_ari_values)\n",
    "        epoch_ari.append(avg_ari_batch)\n",
    "\n",
    "        epoch_attn_weights.append(attn_weights.detach().cpu())\n",
    "\n",
    "    # Compute average training losses and ARI\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_train_recon_loss = running_recon_loss / len(train_loader)\n",
    "    avg_train_entropy_loss = running_entropy_loss / len(train_loader)\n",
    "    avg_train_ari = np.mean(epoch_ari)\n",
    "    train_ari_list.append(avg_train_ari)\n",
    "\n",
    "    # Compute average attention matrix for the epoch\n",
    "    epoch_attn_weights_tensor = torch.cat(epoch_attn_weights, dim=0)\n",
    "    avg_attn_weights_epoch = epoch_attn_weights_tensor.mean(dim=0)\n",
    "    avg_attn_weights_epoch_np = avg_attn_weights_epoch.numpy()\n",
    "    attention_matrices.append(avg_attn_weights_epoch_np.T)\n",
    "\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_recon_loss = 0.0\n",
    "    test_entropy_loss = 0.0\n",
    "    epoch_ari_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch,) in enumerate(test_loader):\n",
    "            batch = batch.to(device)\n",
    "            batch_size = batch.size(0)\n",
    "\n",
    "            # Forward pass\n",
    "            x_hat, attn_weights = model(batch)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "            # Entropy regularizer\n",
    "            epsilon = 1e-8\n",
    "            entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)\n",
    "            entropy_regularizer = ent_norm * torch.mean(torch.sum(entropy, dim=1))\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "            # Accumulate test losses\n",
    "            test_loss += loss.item()\n",
    "            test_recon_loss += recon_loss.item()\n",
    "            test_entropy_loss += entropy_regularizer.item()\n",
    "\n",
    "            # Compute ARI on test set\n",
    "            attn_weights_np = attn_weights.detach().cpu().numpy()\n",
    "            batch_ari_values = []\n",
    "            for i in range(batch_size):\n",
    "                predicted_labels = np.argmax(attn_weights_np[i], axis=1)\n",
    "                ari = compute_ari_per_sample(true_labels, predicted_labels)\n",
    "                batch_ari_values.append(ari)\n",
    "            avg_ari_batch = np.mean(batch_ari_values)\n",
    "            epoch_ari_test.append(avg_ari_batch)\n",
    "\n",
    "        # Compute average test losses and ARI\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        avg_test_recon_loss = test_recon_loss / len(test_loader)\n",
    "        avg_test_entropy_loss = test_entropy_loss / len(test_loader)\n",
    "        avg_test_ari = np.mean(epoch_ari_test)\n",
    "        test_ari_list.append(avg_test_ari)\n",
    "\n",
    "    # Print results for the epoch\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Lambda Entropy: {lambda_entropy:.6f}, '\n",
    "              f'Train Total Loss: {avg_train_loss:.4f}, Train Recon Loss: {avg_train_recon_loss:.4f}, Train Entropy Loss: {avg_train_entropy_loss:.4f}, Train ARI: {avg_train_ari:.4f}, '\n",
    "              f'Test Total Loss: {avg_test_loss:.4f}, Test Recon Loss: {avg_test_recon_loss:.4f}, Test Entropy Loss: {avg_test_entropy_loss:.4f}, Test ARI: {avg_test_ari:.4f}')\n",
    "\n",
    "# Optionally, save the trained model\n",
    "# torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "# print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average attention matrices (assuming you have 'model', 'train_loader', and 'val_loader')\n",
    "def compute_average_attention(model, dataloader, device):\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "    total_attn = None\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs = data[0].to(device)\n",
    "\n",
    "            # Forward pass up to obtaining attention weights\n",
    "            hat_Z = model.encoder(inputs)\n",
    "            _, attn_weights = model.decoder(hat_Z)\n",
    "\n",
    "            # attn_weights shape: (batch_size, input_dim, output_dim)\n",
    "            batch_size = attn_weights.size(0)\n",
    "            if total_attn is None:\n",
    "                total_attn = attn_weights.sum(dim=0)  # Sum over batch dimension\n",
    "            else:\n",
    "                total_attn += attn_weights.sum(dim=0)\n",
    "            num_samples += batch_size\n",
    "\n",
    "    # Average the attention weights\n",
    "    avg_attn = total_attn / num_samples\n",
    "\n",
    "    return avg_attn.cpu().numpy()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Compute average attention matrices\n",
    "avg_attn_train = compute_average_attention(model, train_loader, device)\n",
    "avg_attn_val = compute_average_attention(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we still want to compute ARI from the average attention matrix.\n",
    "# We'll use synthetic_data.clusters as true_labels.\n",
    "\n",
    "true_labels = np.array(synthetic_data.clusters, dtype=int)  # from synthetic_data\n",
    "input_dim = avg_attn_train.shape[0]\n",
    "output_dim = avg_attn_train.shape[1]\n",
    "\n",
    "# Predicted labels based on average attention\n",
    "predicted_labels = np.argmax(avg_attn_train, axis=1)\n",
    "\n",
    "# Filter out nuisance if -1 are present (optional)\n",
    "mask = true_labels != -1\n",
    "filtered_true = true_labels[mask]\n",
    "filtered_pred = predicted_labels[mask]\n",
    "\n",
    "if len(filtered_true) > 0:\n",
    "    ari = adjusted_rand_score(filtered_true, filtered_pred)\n",
    "else:\n",
    "    ari = 1.0  # If no non-nuisance variables, default ARI can be 1.0\n",
    "\n",
    "print(\"Adjusted Rand Index:\", ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_attention_heatmap(attn_matrix, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        attn_matrix,\n",
    "        cmap='viridis',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Observed Variables')\n",
    "    plt.ylabel('Latent Factors')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Directly plot the transposed average attention matrix for training set\n",
    "avg_attn_train_transposed = avg_attn_train.T  # (output_dim, input_dim)\n",
    "plot_attention_heatmap(avg_attn_train_transposed, 'Average Attention Matrix - Training Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_attn_val_transposed = avg_attn_val.T\n",
    "plot_attention_heatmap(avg_attn_val_transposed, 'Average Attention Matrix - Validation Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
