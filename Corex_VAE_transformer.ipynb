{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_HACIxMBlMv",
        "outputId": "161e826e-6abc-44fb-ce13-ca7ef7e6b439",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of instances in the dataset: 2000\n",
            "Number of columns in the dataset: 50\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
            "0        2       0         3          1   4        1       4         1   \n",
            "1        2       0         1          2   2        1       4         3   \n",
            "2        3       0         2          1   3        1       4         3   \n",
            "3        2       0         1          1   1        0       4         1   \n",
            "4        2       0         1          1   3        0       4         3   \n",
            "\n",
            "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
            "0          2   2  ...         3        3          3           2    3   \n",
            "1          3   1  ...         2        3          2           2    3   \n",
            "2          3   0  ...         4        4          2           1    4   \n",
            "3          3   1  ...         1        2          2           1    3   \n",
            "4          2   0  ...         3        4          1           3    4   \n",
            "\n",
            "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
            "0         1        4          4           2    4  \n",
            "1         1        3          2           2    3  \n",
            "2         2        4          2           0    4  \n",
            "3         1        3          2           1    2  \n",
            "4         1        3          1           3    4  \n",
            "\n",
            "[5 rows x 50 columns]\n",
            "\n",
            "Data Types and Non-Null Counts:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 50 columns):\n",
            " #   Column      Non-Null Count  Dtype\n",
            "---  ------      --------------  -----\n",
            " 0   blue_q0     2000 non-null   int64\n",
            " 1   red_q1      2000 non-null   int64\n",
            " 2   green_q2    2000 non-null   int64\n",
            " 3   purple_q3   2000 non-null   int64\n",
            " 4   q4          2000 non-null   int64\n",
            " 5   blue_q5     2000 non-null   int64\n",
            " 6   red_q6      2000 non-null   int64\n",
            " 7   green_q7    2000 non-null   int64\n",
            " 8   purple_q8   2000 non-null   int64\n",
            " 9   q9          2000 non-null   int64\n",
            " 10  blue_q10    2000 non-null   int64\n",
            " 11  red_q11     2000 non-null   int64\n",
            " 12  green_q12   2000 non-null   int64\n",
            " 13  purple_q13  2000 non-null   int64\n",
            " 14  q14         2000 non-null   int64\n",
            " 15  blue_q15    2000 non-null   int64\n",
            " 16  red_q16     2000 non-null   int64\n",
            " 17  green_q17   2000 non-null   int64\n",
            " 18  purple_q18  2000 non-null   int64\n",
            " 19  q19         2000 non-null   int64\n",
            " 20  blue_q20    2000 non-null   int64\n",
            " 21  red_q21     2000 non-null   int64\n",
            " 22  green_q22   2000 non-null   int64\n",
            " 23  purple_q23  2000 non-null   int64\n",
            " 24  q24         2000 non-null   int64\n",
            " 25  blue_q25    2000 non-null   int64\n",
            " 26  red_q26     2000 non-null   int64\n",
            " 27  green_q27   2000 non-null   int64\n",
            " 28  purple_q28  2000 non-null   int64\n",
            " 29  q29         2000 non-null   int64\n",
            " 30  blue_q30    2000 non-null   int64\n",
            " 31  red_q31     2000 non-null   int64\n",
            " 32  green_q32   2000 non-null   int64\n",
            " 33  purple_q33  2000 non-null   int64\n",
            " 34  q34         2000 non-null   int64\n",
            " 35  blue_q35    2000 non-null   int64\n",
            " 36  red_q36     2000 non-null   int64\n",
            " 37  green_q37   2000 non-null   int64\n",
            " 38  purple_q38  2000 non-null   int64\n",
            " 39  q39         2000 non-null   int64\n",
            " 40  blue_q40    2000 non-null   int64\n",
            " 41  red_q41     2000 non-null   int64\n",
            " 42  green_q42   2000 non-null   int64\n",
            " 43  purple_q43  2000 non-null   int64\n",
            " 44  q44         2000 non-null   int64\n",
            " 45  blue_q45    2000 non-null   int64\n",
            " 46  red_q46     2000 non-null   int64\n",
            " 47  green_q47   2000 non-null   int64\n",
            " 48  purple_q48  2000 non-null   int64\n",
            " 49  q49         2000 non-null   int64\n",
            "dtypes: int64(50)\n",
            "memory usage: 781.4 KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the GitHub repository\n",
        "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Number of instances in the dataset:\", df.shape[0])\n",
        "print(\"Number of columns in the dataset:\", df.shape[1])\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display additional information\n",
        "print(\"\\nData Types and Non-Null Counts:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yaO1N84CB7_U"
      },
      "outputs": [],
      "source": [
        "df = df / 4.0\n",
        "# print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05sXYjo0G0zH",
        "outputId": "c14011fe-b8bc-4578-b826-56e4eb46e6ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (1600, 50)\n",
            "Test set shape: (400, 50)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into features (X) and target (Y) if necessary.\n",
        "# In the case of autoencoder-like models, we do not have target Y, so we'll treat the whole dataset as X.\n",
        "X = df.values  # Convert the DataFrame into a NumPy array for model input\n",
        "\n",
        "# Split the dataset into training (80%) and testing sets (20%)\n",
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the shapes to verify\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6jA29S2CJQV",
        "outputId": "2494525f-8894-4281-df95-c7a978680195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data tensor shape: torch.Size([2000, 50])\n",
            "Number of training batches: 50\n",
            "Number of validation batches: 13\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "data_array = df.to_numpy()\n",
        "\n",
        "# Convert the data to a PyTorch tensor\n",
        "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
        "\n",
        "# Create a PyTorch dataset\n",
        "dataset = TensorDataset(data_tensor)\n",
        "\n",
        "# Split the dataset into training and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "batch_size = 32  # You can adjust the batch size as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Display the shape of the tensor to verify\n",
        "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rywEvDazCYkL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dims=[]):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Define the layers of the MLP for shared processing\n",
        "        dims = [input_dim] + hidden_dims\n",
        "        self.shared_layers = nn.ModuleList()\n",
        "        for i in range(len(dims) - 1):\n",
        "            self.shared_layers.append(nn.Sequential(\n",
        "                nn.Linear(dims[i], dims[i + 1]),\n",
        "                nn.ReLU()\n",
        "            ))\n",
        "\n",
        "        # Output layers for mu and log_var\n",
        "        self.fc_mu = nn.Linear(dims[-1], output_dim)\n",
        "        self.fc_log_var = nn.Linear(dims[-1], output_dim)\n",
        "\n",
        "        # Learnable embedding vectors e_i for each z_i\n",
        "        self.e = nn.Parameter(torch.randn(output_dim, embedding_dim))\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the shared MLP layers\n",
        "        for layer in self.shared_layers:\n",
        "            x = layer(x)  # Shape: (batch_size, hidden_dim)\n",
        "\n",
        "        # Compute mu and log_var with activation functions\n",
        "        mu = self.fc_mu(x)             # Shape: (batch_size, output_dim)\n",
        "        mu = torch.tanh(mu)            # Constrain mu between -1 and 1\n",
        "\n",
        "        log_var = self.fc_log_var(x)   # Shape: (batch_size, output_dim)\n",
        "        log_var = torch.sigmoid(log_var)  # Constrain log_var between 0 and 1\n",
        "\n",
        "        # Reparameterization trick\n",
        "        std = torch.exp(0.5 * log_var)   # std = exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + std * eps              # Shape: (batch_size, output_dim)\n",
        "\n",
        "        # Convert z to \\hat{Z} by multiplying each scalar z_i with its own embedding vector e_i\n",
        "        batch_size = z.size(0)\n",
        "        z_expanded = z.unsqueeze(2)                       # Shape: (batch_size, output_dim, 1)\n",
        "        e_expanded = self.e.unsqueeze(0)                  # Shape: (1, output_dim, embedding_dim)\n",
        "        hat_Z = z_expanded * e_expanded                   # Shape: (batch_size, output_dim, embedding_dim)\n",
        "\n",
        "        return hat_Z, mu, log_var  # Return mu and log_var for KL divergence computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tUXzn0AHHq1T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dims=[]):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.input_dim = input_dim      # Number of observed variables (n)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Learnable query embeddings (e1, e2, ..., en)\n",
        "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
        "\n",
        "        # MultiheadAttention module with 1 head\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        # Define individual MLPs for each observed variable\n",
        "        dims = [embedding_dim] + hidden_dims + [1]\n",
        "        self.mlp_layers = nn.ModuleList([\n",
        "            nn.Sequential(*[\n",
        "                # Add activation functions\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(dims[i], dims[i + 1]),\n",
        "                    nn.Sigmoid() if i == len(dims) - 2 else nn.ReLU()\n",
        "                )\n",
        "                for i in range(len(dims) - 1)\n",
        "            ])\n",
        "            for _ in range(input_dim)\n",
        "        ])\n",
        "\n",
        "    def forward(self, hat_Z):\n",
        "        \"\"\"\n",
        "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
        "        \"\"\"\n",
        "        batch_size = hat_Z.size(0)\n",
        "\n",
        "        # Prepare query embeddings and expand to batch size\n",
        "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, embedding_dim)\n",
        "\n",
        "        # Apply scaled dot-product attention\n",
        "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)        # Output shape: (batch_size, input_dim, embedding_dim)\n",
        "\n",
        "        # Add residual connection and apply layer normalization\n",
        "        out = self.layer_norm(attn_output + query_embeddings)                             # Shape: (batch_size, input_dim, embedding_dim)\n",
        "\n",
        "        # Pass each context vector through its corresponding MLP\n",
        "        x_hat = []\n",
        "        for i in range(self.input_dim):\n",
        "            x_i = out[:, i, :]        # Shape: (batch_size, embedding_dim)\n",
        "            x_i_hat = self.mlp_layers[i](x_i)  # Shape: (batch_size, 1)\n",
        "            x_hat.append(x_i_hat)\n",
        "        x_hat = torch.cat(x_hat, dim=1)  # Shape: (batch_size, input_dim)\n",
        "\n",
        "        return x_hat, attn_weights  # Return attention weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xnBqmgVjIat0"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder(\n",
        "            input_dim=input_dim,\n",
        "            output_dim=output_dim,\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_dims=encoder_hidden_dims\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            input_dim=input_dim,\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_dims=decoder_hidden_dims\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        hat_Z, mu, log_var = self.encoder(x)  # Obtain \\hat{Z}, mu, and log_var from the encoder\n",
        "        x_hat, attn_weights = self.decoder(hat_Z)  # Reconstruct x from \\hat{Z} using the decoder\n",
        "        return x_hat, attn_weights, mu, log_var  # Return mu and log_var for KL divergence computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki1licN-PTun",
        "outputId": "74b11f58-1e63-4e85-abd1-0e6ef5ac3285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Lambda: 0.000000, Train Total Loss: 4.7854, Train Recon Loss: 4.6788, Train KL Loss: 0.1066, Val Total Loss: 4.5429, Val Recon Loss: 4.5408, Val KL Loss: 0.0020\n",
            "Epoch [2/25], Lambda: 0.000009, Train Total Loss: 4.4557, Train Recon Loss: 4.4535, Train KL Loss: 0.0015, Val Total Loss: 4.5333, Val Recon Loss: 4.5313, Val KL Loss: 0.0013\n",
            "Epoch [3/25], Lambda: 0.000016, Train Total Loss: 4.4518, Train Recon Loss: 4.4495, Train KL Loss: 0.0011, Val Total Loss: 4.5281, Val Recon Loss: 4.5261, Val KL Loss: 0.0009\n",
            "Epoch [4/25], Lambda: 0.000023, Train Total Loss: 4.4488, Train Recon Loss: 4.4462, Train KL Loss: 0.0009, Val Total Loss: 4.5354, Val Recon Loss: 4.5330, Val KL Loss: 0.0007\n",
            "Epoch [5/25], Lambda: 0.000028, Train Total Loss: 4.4520, Train Recon Loss: 4.4492, Train KL Loss: 0.0008, Val Total Loss: 4.5298, Val Recon Loss: 4.5271, Val KL Loss: 0.0007\n",
            "Epoch [6/25], Lambda: 0.000032, Train Total Loss: 4.4522, Train Recon Loss: 4.4493, Train KL Loss: 0.0006, Val Total Loss: 4.5328, Val Recon Loss: 4.5298, Val KL Loss: 0.0007\n",
            "Epoch [7/25], Lambda: 0.000035, Train Total Loss: 4.4504, Train Recon Loss: 4.4474, Train KL Loss: 0.0005, Val Total Loss: 4.5297, Val Recon Loss: 4.5267, Val KL Loss: 0.0004\n",
            "Epoch [8/25], Lambda: 0.000038, Train Total Loss: 4.4500, Train Recon Loss: 4.4467, Train KL Loss: 0.0005, Val Total Loss: 4.5338, Val Recon Loss: 4.5307, Val KL Loss: 0.0004\n",
            "Epoch [9/25], Lambda: 0.000040, Train Total Loss: 4.4501, Train Recon Loss: 4.4467, Train KL Loss: 0.0006, Val Total Loss: 4.5238, Val Recon Loss: 4.5202, Val KL Loss: 0.0007\n",
            "Epoch [10/25], Lambda: 0.000042, Train Total Loss: 4.4478, Train Recon Loss: 4.4444, Train KL Loss: 0.0004, Val Total Loss: 4.5256, Val Recon Loss: 4.5224, Val KL Loss: 0.0002\n",
            "Epoch [11/25], Lambda: 0.000043, Train Total Loss: 4.4481, Train Recon Loss: 4.4448, Train KL Loss: 0.0003, Val Total Loss: 4.5251, Val Recon Loss: 4.5217, Val KL Loss: 0.0003\n",
            "Epoch [12/25], Lambda: 0.000044, Train Total Loss: 4.4522, Train Recon Loss: 4.4488, Train KL Loss: 0.0002, Val Total Loss: 4.5225, Val Recon Loss: 4.5192, Val KL Loss: 0.0003\n",
            "Epoch [13/25], Lambda: 0.000045, Train Total Loss: 4.4490, Train Recon Loss: 4.4456, Train KL Loss: 0.0002, Val Total Loss: 4.5313, Val Recon Loss: 4.5280, Val KL Loss: 0.0003\n",
            "Epoch [14/25], Lambda: 0.000046, Train Total Loss: 4.4466, Train Recon Loss: 4.4433, Train KL Loss: 0.0002, Val Total Loss: 4.5196, Val Recon Loss: 4.5162, Val KL Loss: 0.0003\n",
            "Epoch [15/25], Lambda: 0.000047, Train Total Loss: 4.4456, Train Recon Loss: 4.4423, Train KL Loss: 0.0002, Val Total Loss: 4.5300, Val Recon Loss: 4.5268, Val KL Loss: 0.0002\n",
            "Epoch [16/25], Lambda: 0.000048, Train Total Loss: 4.4483, Train Recon Loss: 4.4451, Train KL Loss: 0.0002, Val Total Loss: 4.5247, Val Recon Loss: 4.5215, Val KL Loss: 0.0003\n",
            "Epoch [17/25], Lambda: 0.000048, Train Total Loss: 4.4489, Train Recon Loss: 4.4458, Train KL Loss: 0.0002, Val Total Loss: 4.5281, Val Recon Loss: 4.5250, Val KL Loss: 0.0003\n",
            "Epoch [18/25], Lambda: 0.000048, Train Total Loss: 4.4512, Train Recon Loss: 4.4481, Train KL Loss: 0.0002, Val Total Loss: 4.5240, Val Recon Loss: 4.5211, Val KL Loss: 0.0001\n",
            "Epoch [19/25], Lambda: 0.000049, Train Total Loss: 4.4515, Train Recon Loss: 4.4486, Train KL Loss: 0.0002, Val Total Loss: 4.5217, Val Recon Loss: 4.5189, Val KL Loss: 0.0002\n",
            "Epoch [20/25], Lambda: 0.000049, Train Total Loss: 4.4459, Train Recon Loss: 4.4431, Train KL Loss: 0.0002, Val Total Loss: 4.5247, Val Recon Loss: 4.5219, Val KL Loss: 0.0002\n",
            "Epoch [21/25], Lambda: 0.000049, Train Total Loss: 4.4461, Train Recon Loss: 4.4434, Train KL Loss: 0.0002, Val Total Loss: 4.5200, Val Recon Loss: 4.5172, Val KL Loss: 0.0005\n",
            "Epoch [22/25], Lambda: 0.000049, Train Total Loss: 4.4498, Train Recon Loss: 4.4471, Train KL Loss: 0.0004, Val Total Loss: 4.5172, Val Recon Loss: 4.5149, Val KL Loss: 0.0001\n",
            "Epoch [23/25], Lambda: 0.000049, Train Total Loss: 4.4485, Train Recon Loss: 4.4462, Train KL Loss: 0.0002, Val Total Loss: 4.5354, Val Recon Loss: 4.5331, Val KL Loss: 0.0002\n",
            "Epoch [24/25], Lambda: 0.000049, Train Total Loss: 4.4497, Train Recon Loss: 4.4474, Train KL Loss: 0.0004, Val Total Loss: 4.5232, Val Recon Loss: 4.5212, Val KL Loss: 0.0002\n",
            "Epoch [25/25], Lambda: 0.000050, Train Total Loss: 4.4471, Train Recon Loss: 4.4451, Train KL Loss: 0.0002, Val Total Loss: 4.5200, Val Recon Loss: 4.5181, Val KL Loss: 0.0002\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assume that the Encoder, Decoder, and Model classes are already defined\n",
        "\n",
        "# Define dimensions\n",
        "input_dim = 50        # Number of observed variables\n",
        "output_dim = 5        # Output dimension of the encoder (dimension of Z)\n",
        "embedding_dim = 64    # Embedding dimension for the embeddings e and e_i's\n",
        "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
        "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
        "\n",
        "# Instantiate the model\n",
        "model = Model(\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    embedding_dim=embedding_dim,\n",
        "    encoder_hidden_dims=encoder_hidden_dims,\n",
        "    decoder_hidden_dims=decoder_hidden_dims\n",
        ")\n",
        "\n",
        "# Move the model to the appropriate device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss(reduction='sum')  # Use sum reduction for reconstruction loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 25          # Number of epochs\n",
        "batch_size = 32          # Batch size (already set in the DataLoader)\n",
        "print_every = 1          # How often to print loss (in epochs)\n",
        "\n",
        "# Define the maximum value for the entropy and KL divergence regularization coefficient\n",
        "max_lambda = 0.5*1e-4  # Adjust this value as needed\n",
        "\n",
        "# Flag to enable or disable entropy regularizer\n",
        "use_entropy_regularizer = True  # Set to True to enable, False to disable\n",
        "\n",
        "# Scheduler function for lambda\n",
        "def get_lambda(epoch, num_epochs, max_lambda, schedule_type='linear', use_regularizer=True):\n",
        "    if not use_regularizer:\n",
        "        return 0.0\n",
        "    if schedule_type == 'constant':\n",
        "        # Always return max_lambda\n",
        "        return max_lambda\n",
        "    elif schedule_type == 'linear':\n",
        "        # Linear increase from 0 to max_lambda\n",
        "        return max_lambda * (epoch / num_epochs)\n",
        "    elif schedule_type == 'exponential':\n",
        "        # Exponential increase from 0 to max_lambda\n",
        "        k = 5  # Adjust this value to control the speed of increase\n",
        "        return max_lambda * (1 - math.exp(-k * epoch / num_epochs))\n",
        "    elif schedule_type == 'logarithmic':\n",
        "        # Logarithmic increase from 0 to max_lambda\n",
        "        if epoch == 0:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return max_lambda * math.log(epoch + 1) / math.log(num_epochs + 1)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
        "\n",
        "# Initialize a list to store the average attention matrices per epoch\n",
        "attention_matrices = []\n",
        "\n",
        "# Training loop with validation\n",
        "for epoch in range(num_epochs):\n",
        "    # Compute lambda for the current epoch\n",
        "    lambda_reg = get_lambda(\n",
        "        epoch, num_epochs, max_lambda, schedule_type='exponential', use_regularizer=use_entropy_regularizer)\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0          # Accumulates total loss (reconstruction + regularizers)\n",
        "    running_recon_loss = 0.0    # Accumulates reconstruction loss\n",
        "    running_kl_loss = 0.0       # Accumulates KL divergence loss\n",
        "    epoch_attn_weights = []     # List to store attention weights for all batches in the epoch\n",
        "\n",
        "    for batch_idx, (batch,) in enumerate(train_loader):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute predicted x_hat, attention weights, mu, and log_var\n",
        "        x_hat, attn_weights, mu, log_var = model(batch)\n",
        "\n",
        "        # Squeeze attn_weights to get shape: (batch_size, input_dim, output_dim)\n",
        "        attn_weights = attn_weights.squeeze(1)\n",
        "\n",
        "        # Collect attention weights for the epoch\n",
        "        epoch_attn_weights.append(attn_weights.detach().cpu())\n",
        "\n",
        "        # Compute the reconstruction loss\n",
        "        recon_loss = criterion(x_hat, batch) / batch.size(0)  # Average over batch\n",
        "\n",
        "        # Compute KL divergence loss\n",
        "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - torch.exp(log_var)) / batch.size(0)  # Average over batch\n",
        "\n",
        "        # Initialize entropy_regularizer to zero\n",
        "        entropy_regularizer = 0.0\n",
        "\n",
        "        # Compute the entropy regularizer if enabled\n",
        "        if use_entropy_regularizer:\n",
        "            # Add a small epsilon to prevent log(0)\n",
        "            epsilon = 1e-8\n",
        "\n",
        "            # Compute entropy for each query (input_dim)\n",
        "            entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
        "\n",
        "            # Sum entropies over queries and average over batch\n",
        "            entropy_regularizer = torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
        "\n",
        "        # Total loss\n",
        "        loss = recon_loss + kl_loss + lambda_reg * entropy_regularizer\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate training losses\n",
        "        running_loss += loss.item()\n",
        "        running_recon_loss += recon_loss.item()\n",
        "        running_kl_loss += kl_loss.item()\n",
        "\n",
        "    # Compute average losses for training\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    avg_train_recon_loss = running_recon_loss / len(train_loader)\n",
        "    avg_train_kl_loss = running_kl_loss / len(train_loader)\n",
        "\n",
        "    # Compute the average attention matrix for the epoch\n",
        "    epoch_attn_weights_tensor = torch.cat(epoch_attn_weights, dim=0)  # Shape: (num_samples_in_epoch, input_dim, output_dim)\n",
        "    avg_attn_weights_epoch = epoch_attn_weights_tensor.mean(dim=0)    # Shape: (input_dim, output_dim)\n",
        "    avg_attn_weights_epoch_np = avg_attn_weights_epoch.numpy()\n",
        "\n",
        "    # Transpose to have shape (output_dim, input_dim) so that queries are on x-axis and keys on y-axis\n",
        "    attention_matrices.append(avg_attn_weights_epoch_np.T)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0          # Accumulates total loss (reconstruction + regularizers)\n",
        "    val_recon_loss = 0.0    # Accumulates reconstruction loss\n",
        "    val_kl_loss = 0.0       # Accumulates KL divergence loss\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (batch,) in enumerate(val_loader):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass for validation\n",
        "            x_hat, attn_weights, mu, log_var = model(batch)\n",
        "\n",
        "            # Compute the reconstruction loss\n",
        "            recon_loss = criterion(x_hat, batch) / batch.size(0)  # Average over batch\n",
        "\n",
        "            # Compute KL divergence loss\n",
        "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - torch.exp(log_var)) / batch.size(0)  # Average over batch\n",
        "\n",
        "            # Initialize entropy_regularizer to zero\n",
        "            entropy_regularizer = 0.0\n",
        "\n",
        "            # Compute the entropy regularizer if enabled\n",
        "            if use_entropy_regularizer:\n",
        "                attn_weights = attn_weights.squeeze(1)  # Shape: (batch_size, input_dim, output_dim)\n",
        "\n",
        "                # Add a small epsilon to prevent log(0)\n",
        "                epsilon = 1e-8\n",
        "\n",
        "                # Compute entropy for each query (input_dim)\n",
        "                entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
        "\n",
        "                # Sum entropies over queries and average over batch\n",
        "                entropy_regularizer = torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
        "\n",
        "            # Total loss\n",
        "            loss = recon_loss + kl_loss + lambda_reg * entropy_regularizer\n",
        "\n",
        "            # Accumulate validation losses\n",
        "            val_loss += loss.item()\n",
        "            val_recon_loss += recon_loss.item()\n",
        "            val_kl_loss += kl_loss.item()\n",
        "\n",
        "    # Compute average losses for validation\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    avg_val_recon_loss = val_recon_loss / len(val_loader)\n",
        "    avg_val_kl_loss = val_kl_loss / len(val_loader)\n",
        "\n",
        "    # Print average losses for the epoch\n",
        "    if (epoch + 1) % print_every == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
        "              f'Lambda: {lambda_reg:.6f}, '\n",
        "              f'Train Total Loss: {avg_train_loss:.4f}, '\n",
        "              f'Train Recon Loss: {avg_train_recon_loss:.4f}, '\n",
        "              f'Train KL Loss: {avg_train_kl_loss:.4f}, '\n",
        "              f'Val Total Loss: {avg_val_loss:.4f}, '\n",
        "              f'Val Recon Loss: {avg_val_recon_loss:.4f}, '\n",
        "              f'Val KL Loss: {avg_val_kl_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdW23l5zG0zJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "# Assuming 'df' is your original dataframe\n",
        "\n",
        "# Identify column prefixes for each true factor\n",
        "factor_columns = {\n",
        "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
        "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
        "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
        "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
        "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
        "}\n",
        "\n",
        "# Map factor names to column indices\n",
        "factor_indices = {}\n",
        "for factor_name, columns in factor_columns.items():\n",
        "    indices = [df.columns.get_loc(col) for col in columns]\n",
        "    factor_indices[factor_name] = indices\n",
        "\n",
        "# Create a new ordering of indices\n",
        "new_order = []\n",
        "for factor_name in factor_columns.keys():\n",
        "    new_order.extend(factor_indices[factor_name])\n",
        "\n",
        "# Ensure all indices are included\n",
        "assert len(new_order) == df.shape[1], \"Not all indices are included in the new order.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2zKWb8rG0zJ"
      },
      "outputs": [],
      "source": [
        "# Rearranging attention matrices for the animation\n",
        "reordered_attention_matrices = []\n",
        "\n",
        "for attn_matrix in attention_matrices:\n",
        "    # attn_matrix has shape (output_dim, input_dim)\n",
        "    # Rearrange the columns (observed variables) according to new_order\n",
        "    attn_matrix_reordered = attn_matrix[:, new_order]  # Shape: (output_dim, input_dim)\n",
        "    reordered_attention_matrices.append(attn_matrix_reordered)\n",
        "\n",
        "# Convert the list to a NumPy array for animation\n",
        "attention_matrices_array = np.stack(reordered_attention_matrices)  # Shape: (num_epochs, output_dim, input_dim)\n",
        "\n",
        "# Determine the number of frames (epochs)\n",
        "num_frames = attention_matrices_array.shape[0]\n",
        "\n",
        "# Set up the figure and axis for the animation\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Initialize the heatmap with empty data\n",
        "im = ax.imshow(np.zeros_like(attention_matrices_array[0]), aspect='auto', cmap='viridis', vmin=0, vmax=1)\n",
        "ax.set_xlabel('Observed Variables')\n",
        "ax.set_ylabel('Latent Factors')\n",
        "title = ax.set_title('Attention Matrix at Epoch 1')\n",
        "\n",
        "# Function to initialize the heatmap\n",
        "def init():\n",
        "    data = attention_matrices_array[0]\n",
        "    im.set_data(data)\n",
        "    title.set_text('Attention Matrix at Epoch 1')\n",
        "    return [im, title]\n",
        "\n",
        "# Function to update the heatmap for each frame\n",
        "def update(frame):\n",
        "    data = attention_matrices_array[frame]\n",
        "    im.set_data(data)\n",
        "    title.set_text(f'Attention Matrix at Epoch {frame + 1}')\n",
        "    return [im, title]\n",
        "\n",
        "# Create the animation\n",
        "ani = FuncAnimation(fig, update, frames=num_frames, init_func=init, blit=True)\n",
        "\n",
        "# Close the figure to prevent the static plot from displaying\n",
        "plt.close(fig)\n",
        "\n",
        "# Display the animation in the notebook\n",
        "display(HTML(ani.to_jshtml()))\n",
        "\n",
        "# Save the animation to an MP4 file (optional)\n",
        "ani.save('attention_animation.mp4', writer='ffmpeg', fps=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rHXLE4kG0zJ"
      },
      "outputs": [],
      "source": [
        "# Compute average attention matrices (assuming you have 'model', 'train_loader', and 'val_loader')\n",
        "def compute_average_attention(model, dataloader, device):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    total_attn = None\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs = data[0].to(device)\n",
        "\n",
        "            # Forward pass up to obtaining attention weights\n",
        "            hat_Z = model.encoder(inputs)\n",
        "            _, attn_weights, _, _ = model.decoder(hat_Z)\n",
        "\n",
        "            # attn_weights shape: (batch_size, input_dim, output_dim)\n",
        "            batch_size = attn_weights.size(0)\n",
        "            if total_attn is None:\n",
        "                total_attn = attn_weights.sum(dim=0)  # Sum over batch dimension\n",
        "            else:\n",
        "                total_attn += attn_weights.sum(dim=0)\n",
        "            num_samples += batch_size\n",
        "\n",
        "    # Average the attention weights\n",
        "    avg_attn = total_attn / num_samples\n",
        "\n",
        "    return avg_attn.cpu().numpy()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Compute average attention matrices\n",
        "avg_attn_train = compute_average_attention(model, train_loader, device)\n",
        "avg_attn_val = compute_average_attention(model, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHe4ZC1TG0zJ"
      },
      "outputs": [],
      "source": [
        "# Calculating ARI\n",
        "input_dim = avg_attn_train.shape[0]   # Number of observed variables\n",
        "output_dim = avg_attn_train.shape[1]  # Number of latent factors\n",
        "\n",
        "# Step 1: Assign each observed variable to the latent factor with the highest attention weight\n",
        "predicted_labels = np.argmax(avg_attn_train, axis=1)  # Shape: (input_dim,)\n",
        "\n",
        "# Step 2: Create true labels based on ideal groups\n",
        "true_labels = np.full(input_dim, -1)  # Initialize with -1\n",
        "\n",
        "# Map factor names to indices\n",
        "factor_names = ['Factor1', 'Factor2', 'Factor3', 'Factor4', 'Factor5']\n",
        "factor_name_to_index = {name: idx for idx, name in enumerate(factor_names)}\n",
        "\n",
        "# Assign labels to observed variables based on factors\n",
        "for factor_name, indices in factor_indices.items():\n",
        "    factor_idx = factor_name_to_index[factor_name]\n",
        "    true_labels[indices] = factor_idx\n",
        "\n",
        "# Ensure all observed variables have been assigned\n",
        "assert np.all(true_labels >= 0), \"Some observed variables have not been assigned a true label\"\n",
        "\n",
        "# Step 3: Compute Adjusted Rand Index\n",
        "ari = adjusted_rand_score(true_labels, predicted_labels)\n",
        "\n",
        "print(\"Adjusted Rand Index:\", ari)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6V9zu9AwG0zJ"
      },
      "outputs": [],
      "source": [
        "# Transpose the average attention matrices\n",
        "avg_attn_train_transposed = avg_attn_train.T  # Shape: (output_dim, input_dim)\n",
        "avg_attn_val_transposed = avg_attn_val.T      # Shape: (output_dim, input_dim)\n",
        "\n",
        "# Rearrange the columns (observed variables) using new_order\n",
        "avg_attn_train_reordered = avg_attn_train_transposed[:, new_order]\n",
        "avg_attn_val_reordered = avg_attn_val_transposed[:, new_order]\n",
        "\n",
        "def plot_attention_heatmap(attn_matrix, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        attn_matrix,\n",
        "        cmap='viridis',\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        cbar_kws={'label': 'Attention Weight'}\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Observed Variables')\n",
        "    plt.ylabel('Latent Factors')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot average attention matrices with fixed color scaling\n",
        "plot_attention_heatmap(avg_attn_train_reordered, 'Average Attention Matrix - Training Set')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}