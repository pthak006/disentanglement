{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (50000, 60)\n",
      "Test Data Shape: (12500, 60)\n",
      "Number of training batches: 1563\n",
      "Number of test batches: 391\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "# Import the SyntheticData class from factor_eval.py\n",
    "# Make sure factor_eval.py is in the same directory as your notebook\n",
    "from factor_eval import SyntheticData\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "n_samples = 50000\n",
    "n_test = 12500\n",
    "n_sources = 5\n",
    "k = 10\n",
    "snr = 5\n",
    "correlate_sources = False\n",
    "get_covariance = False\n",
    "random_scale = False\n",
    "nuisance = 10\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Generate synthetic data\n",
    "# We want:\n",
    "# - 50,000 training samples, 12,500 test samples\n",
    "# - 5 latent factors (n_sources=5), k=10 variables per factor => 1170 observed variables in total\n",
    "# - SNR = 0.5 for moderate noise\n",
    "# - No correlation between sources (correlate_sources=False)\n",
    "# - No nuisance variables (nuisance=0)\n",
    "# - Use a fixed seed for reproducibility\n",
    "synthetic_data = SyntheticData(\n",
    "    n_samples=n_samples,\n",
    "    n_test=n_test, \n",
    "    n_sources=n_sources,\n",
    "    k=k,\n",
    "    snr=snr,             # Lower SNR to increase noise\n",
    "    correlate_sources=correlate_sources,  # Introduce correlation among latent factors\n",
    "    get_covariance=get_covariance,\n",
    "    random_scale=random_scale,    # Randomly scale variables for additional complexity\n",
    "    nuisance=nuisance,           # No nuisance variables\n",
    "    seed=seed               # Fixed seed for reproducibility\n",
    ")\n",
    "\n",
    "n_observed = synthetic_data.train.shape[1]\n",
    "# Extract training and test sets\n",
    "X_train = synthetic_data.train\n",
    "X_test = synthetic_data.test\n",
    "\n",
    "# -------------------------- Normalization Block (BEGIN) --------------------------\n",
    "# Compute min and max from the training data\n",
    "train_min = np.min(X_train, axis=0, keepdims=True)\n",
    "train_max = np.max(X_train, axis=0, keepdims=True)\n",
    "\n",
    "# Apply min-max normalization: (X - min) / (max - min)\n",
    "# Handle the case where max == min to avoid division by zero (if any variable is constant)\n",
    "denominator = (train_max - train_min)\n",
    "denominator[denominator == 0] = 1e-8  # A small number to avoid division by zero\n",
    "\n",
    "X_train = (X_train - train_min) / denominator\n",
    "X_test = (X_test - train_min) / denominator\n",
    "# -------------------------- Normalization Block (END) --------------------------\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets for training and test sets\n",
    "train_dataset = TensorDataset(train_tensor)\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "\n",
    "# Create a generator for deterministic shuffling\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "# We don't need to do a random split now since SyntheticData already provides train/test sets\n",
    "# We'll just create a RandomSampler for the train dataset to ensure reproducible shuffling\n",
    "train_sampler = RandomSampler(train_dataset, generator=generator)\n",
    "\n",
    "# Create DataLoaders for training and test sets\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Display shapes and verify\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Test Data Shape: {X_test.shape}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of training data:\n",
      "[[0.50868695 0.4987838  0.61679009 0.51139569 0.57808391 0.53119005\n",
      "  0.59510766 0.49915261 0.48816219 0.56378535 0.54389231 0.46161187\n",
      "  0.44176781 0.44011464 0.5212177  0.44182656 0.50323608 0.47318434\n",
      "  0.48656391 0.43129428 0.62553241 0.63957391 0.55248758 0.50820067\n",
      "  0.5742377  0.5785061  0.5395921  0.58961463 0.62899889 0.52459604\n",
      "  0.59427365 0.68768851 0.60007923 0.67818638 0.65084412 0.63100136\n",
      "  0.65105545 0.60424543 0.67327988 0.57388115 0.45481663 0.58123842\n",
      "  0.5387345  0.37032482 0.46125067 0.51695561 0.4722074  0.52507556\n",
      "  0.42781294 0.38517918 0.67877306 0.32525958 0.45856828 0.62778782\n",
      "  0.54594568 0.28734671 0.23028904 0.49383711 0.59874729 0.41008683]\n",
      " [0.48820029 0.41353023 0.45875492 0.43953774 0.46819887 0.50459208\n",
      "  0.4889965  0.47278801 0.51049108 0.49991677 0.63305502 0.63168452\n",
      "  0.68632706 0.72601394 0.65869075 0.69213919 0.69485117 0.65005208\n",
      "  0.52951655 0.60731036 0.63851802 0.54294673 0.591343   0.58300996\n",
      "  0.56239008 0.5920765  0.61931209 0.52943182 0.61405599 0.63433162\n",
      "  0.38440047 0.51977148 0.40141907 0.39123537 0.49510953 0.48944575\n",
      "  0.45425396 0.43832339 0.47266266 0.45655738 0.50610283 0.60363285\n",
      "  0.56628643 0.42117238 0.58289333 0.65146169 0.48652048 0.50339252\n",
      "  0.40926248 0.49582224 0.5291143  0.38954183 0.35432303 0.31665948\n",
      "  0.4432734  0.46263957 0.24836179 0.47198199 0.33131363 0.60493025]\n",
      " [0.48147637 0.45589568 0.49956868 0.49070448 0.43052276 0.4151708\n",
      "  0.55507923 0.51957538 0.45185397 0.42118945 0.46856721 0.47383979\n",
      "  0.46014611 0.42999676 0.42412445 0.43945044 0.35801432 0.30829401\n",
      "  0.45092991 0.37948903 0.55296598 0.45044464 0.61399752 0.44212254\n",
      "  0.52276576 0.59201676 0.52445453 0.50389102 0.53186127 0.61597683\n",
      "  0.33380187 0.28558073 0.28977169 0.2728595  0.30993147 0.29823597\n",
      "  0.34530745 0.34815231 0.31216106 0.2720615  0.31140434 0.34655416\n",
      "  0.25315457 0.24899563 0.34501331 0.34439646 0.38616755 0.27713212\n",
      "  0.30393631 0.36241099 0.61708224 0.5947697  0.50754313 0.32796955\n",
      "  0.54731626 0.55747293 0.34438733 0.29901938 0.4148885  0.72807784]\n",
      " [0.48743644 0.43168662 0.47252165 0.46558521 0.43245762 0.46714296\n",
      "  0.47750878 0.44329325 0.44072348 0.45120618 0.39051557 0.36939755\n",
      "  0.40965503 0.41049236 0.41131896 0.40264042 0.35745904 0.4015953\n",
      "  0.42111845 0.36435036 0.53322874 0.63238612 0.52829614 0.49168098\n",
      "  0.48327992 0.46072422 0.54291703 0.43656055 0.49518802 0.4604278\n",
      "  0.44145454 0.45722381 0.38646667 0.48165979 0.40634871 0.36912124\n",
      "  0.47917115 0.53836064 0.49803943 0.47503083 0.35302483 0.34075754\n",
      "  0.25908798 0.32182878 0.28569193 0.37826652 0.37249523 0.32181662\n",
      "  0.50172057 0.38736368 0.45641426 0.49435347 0.39345797 0.40813447\n",
      "  0.45920334 0.5531218  0.43585326 0.49480666 0.58408912 0.29916632]\n",
      " [0.75022736 0.64869172 0.65626663 0.69999725 0.72872545 0.71270592\n",
      "  0.70099533 0.75061753 0.74431042 0.70943898 0.58096042 0.48555598\n",
      "  0.47318929 0.47290671 0.58755651 0.42923928 0.45525765 0.54740379\n",
      "  0.42504714 0.32098959 0.49648499 0.58129485 0.50171251 0.51892119\n",
      "  0.46769552 0.48364103 0.5441408  0.4653181  0.44218611 0.43948071\n",
      "  0.34003512 0.28291872 0.31166754 0.35040188 0.25756208 0.32681988\n",
      "  0.39966015 0.35153711 0.40331815 0.38692259 0.4224001  0.44098025\n",
      "  0.4364913  0.4108753  0.37019508 0.47893857 0.43128051 0.42748463\n",
      "  0.51297554 0.42755595 0.65336494 0.6726712  0.42641192 0.13735398\n",
      "  0.62937966 0.44998592 0.45196293 0.53809646 0.364333   0.6038133 ]]\n",
      "\n",
      "Summary statistics for the first 5 columns of the training data:\n",
      "Column 0: mean=0.5221, std=0.1226, min=0.0000, max=1.0000\n",
      "Column 1: mean=0.5110, std=0.1258, min=0.0000, max=1.0000\n",
      "Column 2: mean=0.5005, std=0.1233, min=0.0000, max=1.0000\n",
      "Column 3: mean=0.5086, std=0.1274, min=0.0000, max=1.0000\n",
      "Column 4: mean=0.5043, std=0.1163, min=0.0000, max=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first 5 rows of the training data\n",
    "print(\"First 5 rows of training data:\")\n",
    "print(X_train[:5])\n",
    "\n",
    "# Let's also print summary statistics of the first few columns to see their distribution\n",
    "num_cols_to_inspect = 5  # you can change this number\n",
    "cols_to_inspect = X_train[:, :num_cols_to_inspect]\n",
    "\n",
    "print(\"\\nSummary statistics for the first 5 columns of the training data:\")\n",
    "means = cols_to_inspect.mean(axis=0)\n",
    "stds = cols_to_inspect.std(axis=0)\n",
    "mins = cols_to_inspect.min(axis=0)\n",
    "maxs = cols_to_inspect.max(axis=0)\n",
    "\n",
    "for i in range(num_cols_to_inspect):\n",
    "    print(f\"Column {i}: mean={means[i]:.4f}, std={stds[i]:.4f}, min={mins[i]:.4f}, max={maxs[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=n_observed,         # Number of obaserved variables\n",
    "        hidden_dims=[128, 64],        # Shared hidden layers\n",
    "        output_dim=5,                 # Number of latent factors\n",
    "        embedding_dim=64\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 1. Sanity checks on dimensions (exactly as in your original code, but\n",
    "        #    with n_observed as a parameter).\n",
    "        # ---------------------------------------------------------------------\n",
    "        assert input_dim == n_observed, f\"Expected input_dim to be {n_observed}, but got {input_dim}\"\n",
    "        assert output_dim == 5,        f\"Expected output_dim to be 5, but got {output_dim}\"\n",
    "        assert embedding_dim == 64,    f\"Expected embedding_dim to be 64, but got {embedding_dim}\"\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 2. Define MLP \"shared\" layers that will feed into mu and log_var.\n",
    "        #    (We do NOT produce an output = '5' from these layers directly.)\n",
    "        # ---------------------------------------------------------------------\n",
    "        dims = [input_dim] + hidden_dims  # e.g. [n_observed, 128, 64]\n",
    "        self.shared_layers = nn.ModuleList()\n",
    "        for i in range(len(dims) - 1):\n",
    "            in_features = dims[i]\n",
    "            out_features = dims[i + 1]\n",
    "            # Each hidden layer can be [Linear -> ReLU]\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.shared_layers.append(layer)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3. Final \"heads\" for mu and log_var (each of dimension output_dim=5).\n",
    "        # ---------------------------------------------------------------------\n",
    "        last_dim = dims[-1]  # e.g. 64\n",
    "        self.fc_mu = nn.Linear(last_dim, output_dim)\n",
    "        self.fc_log_var = nn.Linear(last_dim, output_dim)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 4. Learnable embedding matrix e_i for each latent dimension z_i\n",
    "        #    shape: (5, 64).\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.e = nn.Parameter(torch.randn(output_dim, embedding_dim))\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Basic shape checks for the embedding matrix\n",
    "        assert self.e.shape == (5, 64), \\\n",
    "            f\"Expected embedding matrix e to have shape (5, 64), got {self.e.shape}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, n_observed)\n",
    "        Returns:\n",
    "          hat_Z: shape (batch_size, output_dim, embedding_dim)\n",
    "          mu, log_var: shape (batch_size, output_dim) each\n",
    "        \"\"\"\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 1. Basic checks on input shape\n",
    "        # ---------------------------------------------------------------------\n",
    "        assert x.dim() == 2, f\"Expected x to be a 2D tensor, but got {x.dim()}D.\"\n",
    "        batch_size = x.size(0)\n",
    "        assert x.size(1) == n_observed, \\\n",
    "            f\"Expected x to have {n_observed} features, got {x.shape[1]}\"\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 2. Pass through the shared hidden layers (MLP).\n",
    "        # ---------------------------------------------------------------------\n",
    "        for layer in self.shared_layers:\n",
    "            x = layer(x)  # [batch_size, hidden_dims[-1]]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3. Compute mu and log_var from the last hidden layer output.\n",
    "        #    - We apply some activation to each (just as an example).\n",
    "        # ---------------------------------------------------------------------\n",
    "        mu = self.fc_mu(x)             # shape: (batch_size, output_dim=5)\n",
    "        mu = torch.tanh(mu)            # e.g. constrain mu to -1..1\n",
    "\n",
    "        log_var = self.fc_log_var(x)   # shape: (batch_size, 5)\n",
    "        log_var = torch.sigmoid(log_var)  # e.g. constrain log_var to 0..1\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 4. Reparameterization trick:\n",
    "        #    std = exp(0.5 * log_var), then z = mu + eps * std\n",
    "        # ---------------------------------------------------------------------\n",
    "        std = torch.exp(0.5 * log_var)   # shape: (batch_size, 5)\n",
    "        eps = torch.randn_like(std)      # same shape\n",
    "        z = mu + std * eps               # shape: (batch_size, 5)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 5. Convert z (batch_size, 5) => hat_Z (batch_size, 5, 64)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Expand z to (batch_size, 5, 1)\n",
    "        z_expanded = z.unsqueeze(2)\n",
    "        assert z_expanded.shape == (batch_size, 5, 1), \\\n",
    "            f\"Expected z_expanded to have shape ({batch_size}, 5, 1), but got {z_expanded.shape}\"\n",
    "\n",
    "        # Expand e to (1, 5, 64)\n",
    "        e_expanded = self.e.unsqueeze(0)\n",
    "        assert e_expanded.shape == (1, 5, 64), \\\n",
    "            f\"Expected e_expanded to have shape (1, 5, 64), got {e_expanded.shape}\"\n",
    "\n",
    "        # Multiply => hat_Z (batch_size, 5, 64)\n",
    "        hat_Z = z_expanded * e_expanded\n",
    "        assert hat_Z.shape == (batch_size, 5, 64), \\\n",
    "            f\"Expected hat_Z to have shape ({batch_size}, 5, 64), but got {hat_Z.shape}\"\n",
    "\n",
    "        return hat_Z, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim=n_observed, embedding_dim=64, hidden_dims=[]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim      # Number of observed variables (n)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Assert input dimensions\n",
    "        assert input_dim == n_observed, f\"Expected input_dim to be {n_observed}, but got {input_dim}\"\n",
    "        assert embedding_dim == 64, f\"Expected embedding_dim to be 64, but got {embedding_dim}\"\n",
    "\n",
    "        # Learnable query embeddings (e1, e2, ..., e_n)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
    "\n",
    "        # Assert query_embeddings shape\n",
    "        assert self.query_embeddings.shape == (n_observed, 64), \\\n",
    "            f\"Expected query_embeddings to have shape ({n_observed}, 64), but got {self.query_embeddings.shape}\"\n",
    "\n",
    "        # MultiheadAttention module with 1 head\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Define individual MLPs for each observed variable\n",
    "        dims = [embedding_dim] + hidden_dims + [1]\n",
    "\n",
    "        # Create MLPs for each observed variable\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            nn.Sequential(*[\n",
    "                nn.Linear(dims[i], dims[i+1]) if i == len(dims) - 2 else nn.Sequential(\n",
    "                    nn.Linear(dims[i], dims[i+1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                for i in range(len(dims) - 1)\n",
    "            ])\n",
    "            for _ in range(input_dim)\n",
    "        ])\n",
    "\n",
    "        # Assert we have one MLP per observed variable\n",
    "        assert len(self.mlp_layers) == n_observed, \\\n",
    "            f\"Expected {n_observed} MLPs in mlp_layers, but got {len(self.mlp_layers)}\"\n",
    "\n",
    "        # Verify that MLPs do not share parameters\n",
    "        mlp_params = [set(mlp.parameters()) for mlp in self.mlp_layers]\n",
    "        for i in range(len(mlp_params)):\n",
    "            for j in range(i+1, len(mlp_params)):\n",
    "                assert mlp_params[i].isdisjoint(mlp_params[j]), \\\n",
    "                    f\"MLP {i} and MLP {j} share parameters\"\n",
    "\n",
    "    def forward(self, hat_Z):\n",
    "        \"\"\"\n",
    "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Assert the shape of hat_Z\n",
    "        assert hat_Z.dim() == 3, f\"Expected hat_Z to be 3D, got {hat_Z.dim()}D.\"\n",
    "        batch_size, output_dim, embedding_dim = hat_Z.shape\n",
    "        assert embedding_dim == 64, \\\n",
    "            f\"Expected hat_Z embedding_dim to be 64, but got {embedding_dim}\"\n",
    "        assert output_dim == 5, \\\n",
    "            f\"Expected hat_Z output_dim to be 5, but got {output_dim}\"\n",
    "\n",
    "        # Prepare query embeddings and expand to batch size\n",
    "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        assert query_embeddings.shape == (batch_size, self.input_dim, 64), \\\n",
    "            f\"Expected query_embeddings to have shape ({batch_size}, {self.input_dim}, 64), got {query_embeddings.shape}\"\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)\n",
    "        assert attn_output.shape == (batch_size, self.input_dim, 64), \\\n",
    "            f\"Expected attn_output to have shape ({batch_size}, {self.input_dim}, 64), got {attn_output.shape}\"\n",
    "        assert attn_weights.shape == (batch_size, self.input_dim, output_dim), \\\n",
    "            f\"Expected attn_weights to have shape ({batch_size}, {self.input_dim}, {output_dim}), got {attn_weights.shape}\"\n",
    "\n",
    "        # Add residual connection and apply layer normalization\n",
    "        out = self.layer_norm(attn_output + query_embeddings)\n",
    "        assert out.shape == (batch_size, self.input_dim, 64), \\\n",
    "            f\"Expected out to have shape ({batch_size}, {self.input_dim}, 64), got {out.shape}\"\n",
    "\n",
    "        # Pass each context vector through its corresponding MLP\n",
    "        x_hat = []\n",
    "        for i in range(self.input_dim):\n",
    "            x_i = out[:, i, :]  # (batch_size, 64)\n",
    "            x_i_hat = self.mlp_layers[i](x_i)  # (batch_size, 1)\n",
    "            x_hat.append(x_i_hat)\n",
    "        x_hat = torch.cat(x_hat, dim=1)  # (batch_size, self.input_dim)\n",
    "\n",
    "        assert x_hat.shape == (batch_size, self.input_dim), \\\n",
    "            f\"Expected x_hat to have shape ({batch_size}, {self.input_dim}), got {x_hat.shape}\"\n",
    "\n",
    "        return x_hat, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=encoder_hidden_dims\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            input_dim=input_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=decoder_hidden_dims\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder outputs: hat_Z (reparameterized latent factors), mu, log_var\n",
    "        hat_Z, mu, log_var = self.encoder(x)\n",
    "        \n",
    "        # Decoder reconstructs x from hat_Z\n",
    "        x_hat, attn_weights = self.decoder(hat_Z)\n",
    "        \n",
    "        # Return all necessary components: reconstruction, attention, and encoder outputs for KL divergence\n",
    "        return x_hat, attn_weights, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "No trained model found. Starting from scratch.\n",
      "Epoch [1/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0152, Recon=0.0150, KL=0.0040, Entropy=0.9692, ARI=0.0264 | Test: Loss=0.0144, Recon=0.0144, KL=0.0000, Entropy=0.9888, ARI=0.0426\n",
      "Epoch [2/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0142, Recon=0.0142, KL=0.0000, Entropy=0.9901, ARI=0.0478 | Test: Loss=0.0142, Recon=0.0142, KL=0.0000, Entropy=0.9915, ARI=0.0337\n",
      "Epoch [3/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0142, Recon=0.0142, KL=0.0000, Entropy=0.9922, ARI=0.0371 | Test: Loss=0.0142, Recon=0.0142, KL=0.0000, Entropy=0.9884, ARI=0.0526\n",
      "Epoch [4/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0142, Recon=0.0142, KL=0.0000, Entropy=0.9919, ARI=0.0397 | Test: Loss=0.0142, Recon=0.0142, KL=0.0000, Entropy=0.9933, ARI=0.0341\n",
      "Epoch [5/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0142, Recon=0.0142, KL=0.0000, Entropy=0.9945, ARI=0.0171 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9960, ARI=0.0075\n",
      "Epoch [6/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9952, ARI=0.0069 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9948, ARI=0.0031\n",
      "Epoch [7/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9940, ARI=0.0093 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9966, ARI=0.0001\n",
      "Epoch [8/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9962, ARI=-0.0034 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9979, ARI=-0.0155\n",
      "Epoch [9/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9912, ARI=0.0413 | Test: Loss=0.0142, Recon=0.0142, KL=0.0000, Entropy=0.9976, ARI=0.0245\n",
      "Epoch [10/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9954, ARI=0.0371 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9968, ARI=0.0381\n",
      "Epoch [11/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9948, ARI=0.0040 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9977, ARI=0.0302\n",
      "Epoch [12/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9713, ARI=0.0034 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9928, ARI=-0.0035\n",
      "Epoch [13/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9946, ARI=0.0012 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9963, ARI=0.0218\n",
      "Epoch [14/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9927, ARI=0.0043 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9915, ARI=0.0242\n",
      "Epoch [15/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9958, ARI=0.0269 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9986, ARI=0.0339\n",
      "Epoch [16/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9923, ARI=0.0359 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9802, ARI=0.0164\n",
      "Epoch [17/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9918, ARI=0.0108 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9969, ARI=0.0231\n",
      "Epoch [18/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0140, Recon=0.0140, KL=0.0000, Entropy=0.9954, ARI=0.0409 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9902, ARI=0.0575\n",
      "Epoch [19/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0140, Recon=0.0140, KL=0.0000, Entropy=0.9961, ARI=0.0159 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9932, ARI=-0.0219\n",
      "Epoch [20/20], lambda_entropy=0.000000, lambda_kl=0.050000, Train: Loss=0.0140, Recon=0.0140, KL=0.0000, Entropy=0.9912, ARI=0.0163 | Test: Loss=0.0141, Recon=0.0141, KL=0.0000, Entropy=0.9940, ARI=0.0260\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume 'synthetic_data' and 'model' are already defined.\n",
    "# Also assume 'train_loader' and 'test_loader' are defined from previous code.\n",
    "# The 'Model' now yields (x_hat, attn_weights, mu, log_var).\n",
    "\n",
    "# Extract parameters from synthetic_data\n",
    "input_dim = synthetic_data.train.shape[1]  # Number of observed variables\n",
    "output_dim = synthetic_data.n_sources      # Number of latent factors\n",
    "true_labels = np.array(synthetic_data.clusters, dtype=int)\n",
    "assert np.all(true_labels >= -1), \"Some observed variables have invalid cluster labels.\"\n",
    "\n",
    "# Define the network dimensions\n",
    "input_dim = n_observed\n",
    "output_dim = 5\n",
    "embedding_dim = 64\n",
    "encoder_hidden_dims = [128, 64]\n",
    "decoder_hidden_dims = [64, 32]\n",
    "\n",
    "# Instantiate the variational model\n",
    "model = Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims\n",
    ")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Reconstruction loss (mean-squared error)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "print_every = 1\n",
    "\n",
    "# 1) Entropy regularizer coefficient\n",
    "max_lambda_entropy = 0.0  # as before1\n",
    "use_entropy_regularizer = True\n",
    "\n",
    "# 2) KL divergence coefficient\n",
    "max_lambda_kl = 5*1e-2       # you can pick a different value if desired\n",
    "use_kl_regularizer = True\n",
    "\n",
    "# Function to schedule entropy weight\n",
    "def get_lambda_entropy(epoch, num_epochs, max_lambda_entropy, schedule_type='exponential',\n",
    "                       use_entropy_regularizer=True):\n",
    "    if not use_entropy_regularizer:\n",
    "        return 0.0\n",
    "    if schedule_type == 'constant':\n",
    "        return max_lambda_entropy\n",
    "    elif schedule_type == 'linear':\n",
    "        return max_lambda_entropy * (epoch / num_epochs)\n",
    "    elif schedule_type == 'exponential':\n",
    "        k = 5\n",
    "        numerator = math.exp(k * epoch / num_epochs) - 1\n",
    "        denominator = math.exp(k) - 1\n",
    "        return max_lambda_entropy * (numerator / denominator)\n",
    "    elif schedule_type == 'logarithmic':\n",
    "        if epoch == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return max_lambda_entropy * math.log(epoch + 1) / math.log(num_epochs + 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
    "\n",
    "# Function to schedule KL weight similarly (optional)\n",
    "def get_lambda_kl(epoch, num_epochs, max_lambda_kl, schedule_type='constant',\n",
    "                  use_kl_regularizer=True):\n",
    "    if not use_kl_regularizer:\n",
    "        return 0.0\n",
    "    # Example: keep it constant for demonstration\n",
    "    if schedule_type == 'constant':\n",
    "        return max_lambda_kl\n",
    "    elif schedule_type == 'linear':\n",
    "        return max_lambda_kl * (epoch / num_epochs)\n",
    "    # etc. — same logic as the entropy schedule\n",
    "    else:\n",
    "        return max_lambda_kl\n",
    "\n",
    "def compute_ari_per_sample(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Computes the ARI between true_labels and predicted_labels.\n",
    "    If there are -1 labels (nuisance), they will be filtered out.\n",
    "    \"\"\"\n",
    "    mask = true_labels != -1\n",
    "    filtered_true = true_labels[mask]\n",
    "    filtered_pred = predicted_labels[mask]\n",
    "    if len(filtered_true) == 0:\n",
    "        return 1.0\n",
    "    return adjusted_rand_score(filtered_true, filtered_pred)\n",
    "\n",
    "# Possibly load model\n",
    "model_path = \"trained_model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Trained model found. Loading the model.\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"No trained model found. Starting from scratch.\")\n",
    "\n",
    "# Initialize a list to store average attention matrices per epoch\n",
    "attention_matrices = []\n",
    "\n",
    "# For normalizing attention entropy\n",
    "ent_norm = 1.0 / (input_dim * math.log(output_dim))\n",
    "\n",
    "# Lists to store ARIs\n",
    "train_ari_list = []\n",
    "test_ari_list = []\n",
    "\n",
    "# ------------------ Training Loop ------------------\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute current lambda for entropy and KL\n",
    "    lambda_entropy = get_lambda_entropy(\n",
    "        epoch, num_epochs, max_lambda_entropy, schedule_type='exponential',\n",
    "        use_entropy_regularizer=use_entropy_regularizer)\n",
    "    lambda_kl = get_lambda_kl(\n",
    "        epoch, num_epochs, max_lambda_kl, schedule_type='constant',\n",
    "        use_kl_regularizer=use_kl_regularizer)\n",
    "\n",
    "    # ------ Training phase ------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_recon_loss = 0.0\n",
    "    running_kl_loss = 0.0\n",
    "    running_entropy_loss = 0.0\n",
    "\n",
    "    epoch_attn_weights = []\n",
    "    epoch_ari = []\n",
    "\n",
    "    for batch_idx, (batch,) in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.size(0)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass => (x_hat, attn_weights, mu, log_var)\n",
    "        x_hat, attn_weights, mu, log_var = model(batch)\n",
    "\n",
    "        # Compute MSE reconstruction loss (average over batch for convenience)\n",
    "        recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "        # KL divergence: -0.5 * sum(1 + log_var - mu^2 - exp(log_var))\n",
    "        # Usually aggregated per sample => we can average or sum\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        kl_loss = kl_loss / batch_size  # average\n",
    "\n",
    "        # Entropy regularizer (attention)\n",
    "        entropy_reg = 0.0\n",
    "        if use_entropy_regularizer:\n",
    "            epsilon = 1e-8\n",
    "            # attn_weights shape: (batch_size, input_dim, output_dim)\n",
    "            # sum over output_dim => sum over each row in the last dimension\n",
    "            entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)\n",
    "            # sum entropies over queries -> average over batch\n",
    "            entropy_reg = ent_norm * torch.mean(torch.sum(entropy, dim=1))\n",
    "\n",
    "        # Total loss\n",
    "        loss = recon_loss + lambda_kl * kl_loss + lambda_entropy * entropy_reg\n",
    "\n",
    "        # Backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate stats\n",
    "        running_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_kl_loss += kl_loss.item()\n",
    "        running_entropy_loss += entropy_reg.item()\n",
    "\n",
    "        # Compute ARI per sample\n",
    "        attn_np = attn_weights.detach().cpu().numpy()\n",
    "        batch_ari_vals = []\n",
    "        for i in range(batch_size):\n",
    "            pred_labels = np.argmax(attn_np[i], axis=1)\n",
    "            ari_val = compute_ari_per_sample(true_labels, pred_labels)\n",
    "            batch_ari_vals.append(ari_val)\n",
    "        avg_ari_batch = np.mean(batch_ari_vals)\n",
    "        epoch_ari.append(avg_ari_batch)\n",
    "\n",
    "        # Save attention for average attention matrix\n",
    "        epoch_attn_weights.append(attn_weights.detach().cpu())\n",
    "\n",
    "    # Compute epoch-level stats\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_train_recon_loss = running_recon_loss / len(train_loader)\n",
    "    avg_train_kl_loss = running_kl_loss / len(train_loader)\n",
    "    avg_train_entropy_loss = running_entropy_loss / len(train_loader)\n",
    "    avg_train_ari = np.mean(epoch_ari)\n",
    "    train_ari_list.append(avg_train_ari)\n",
    "\n",
    "    # Compute average attention matrix for the epoch\n",
    "    epoch_attn_weights_tensor = torch.cat(epoch_attn_weights, dim=0)\n",
    "    avg_attn_weights_epoch = epoch_attn_weights_tensor.mean(dim=0)  # shape (input_dim, output_dim)\n",
    "    attention_matrices.append(avg_attn_weights_epoch.cpu().numpy().T)\n",
    "\n",
    "    # ------ Testing phase ------\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_recon_loss = 0.0\n",
    "    test_kl_loss = 0.0\n",
    "    test_entropy_loss = 0.0\n",
    "    epoch_ari_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch,) in enumerate(test_loader):\n",
    "            batch = batch.to(device)\n",
    "            batch_size = batch.size(0)\n",
    "\n",
    "            x_hat, attn_weights, mu, log_var = model(batch)\n",
    "\n",
    "            recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "            # KL\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            kl_loss = kl_loss / batch_size\n",
    "\n",
    "            # Entropy\n",
    "            entropy_reg = 0.0\n",
    "            if use_entropy_regularizer:\n",
    "                epsilon = 1e-8\n",
    "                entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)\n",
    "                entropy_reg = ent_norm * torch.mean(torch.sum(entropy, dim=1))\n",
    "\n",
    "            # total\n",
    "            loss = recon_loss + lambda_kl * kl_loss + lambda_entropy * entropy_reg\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_recon_loss += recon_loss.item()\n",
    "            test_kl_loss += kl_loss.item()\n",
    "            test_entropy_loss += entropy_reg.item()\n",
    "\n",
    "            # ARI\n",
    "            attn_np = attn_weights.detach().cpu().numpy()\n",
    "            batch_ari_vals = []\n",
    "            for i in range(batch_size):\n",
    "                pred_labels = np.argmax(attn_np[i], axis=1)\n",
    "                ari_val = compute_ari_per_sample(true_labels, pred_labels)\n",
    "                batch_ari_vals.append(ari_val)\n",
    "            avg_ari_batch = np.mean(batch_ari_vals)\n",
    "            epoch_ari_test.append(avg_ari_batch)\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        avg_test_recon_loss = test_recon_loss / len(test_loader)\n",
    "        avg_test_kl_loss = test_kl_loss / len(test_loader)\n",
    "        avg_test_entropy_loss = test_entropy_loss / len(test_loader)\n",
    "        avg_test_ari = np.mean(epoch_ari_test)\n",
    "        test_ari_list.append(avg_test_ari)\n",
    "\n",
    "    # Print results\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "              f\"lambda_entropy={lambda_entropy:.6f}, lambda_kl={lambda_kl:.6f}, \"\n",
    "              f\"Train: Loss={avg_train_loss:.4f}, Recon={avg_train_recon_loss:.4f}, KL={avg_train_kl_loss:.4f}, Entropy={avg_train_entropy_loss:.4f}, ARI={avg_train_ari:.4f} | \"\n",
    "              f\"Test: Loss={avg_test_loss:.4f}, Recon={avg_test_recon_loss:.4f}, KL={avg_test_kl_loss:.4f}, Entropy={avg_test_entropy_loss:.4f}, ARI={avg_test_ari:.4f}\")\n",
    "\n",
    "# Optionally save model\n",
    "# torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "# print(\"Training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parthaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
