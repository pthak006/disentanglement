{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_HACIxMBlMv",
    "outputId": "a266a5c4-b111-46e8-b71d-77b930d36ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 2000\n",
      "Number of columns in the dataset: 50\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
      "0        2       0         3          1   4        1       4         1   \n",
      "1        2       0         1          2   2        1       4         3   \n",
      "2        3       0         2          1   3        1       4         3   \n",
      "3        2       0         1          1   1        0       4         1   \n",
      "4        2       0         1          1   3        0       4         3   \n",
      "\n",
      "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
      "0          2   2  ...         3        3          3           2    3   \n",
      "1          3   1  ...         2        3          2           2    3   \n",
      "2          3   0  ...         4        4          2           1    4   \n",
      "3          3   1  ...         1        2          2           1    3   \n",
      "4          2   0  ...         3        4          1           3    4   \n",
      "\n",
      "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
      "0         1        4          4           2    4  \n",
      "1         1        3          2           2    3  \n",
      "2         2        4          2           0    4  \n",
      "3         1        3          2           1    2  \n",
      "4         1        3          1           3    4  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Data Types and Non-Null Counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 50 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   blue_q0     2000 non-null   int64\n",
      " 1   red_q1      2000 non-null   int64\n",
      " 2   green_q2    2000 non-null   int64\n",
      " 3   purple_q3   2000 non-null   int64\n",
      " 4   q4          2000 non-null   int64\n",
      " 5   blue_q5     2000 non-null   int64\n",
      " 6   red_q6      2000 non-null   int64\n",
      " 7   green_q7    2000 non-null   int64\n",
      " 8   purple_q8   2000 non-null   int64\n",
      " 9   q9          2000 non-null   int64\n",
      " 10  blue_q10    2000 non-null   int64\n",
      " 11  red_q11     2000 non-null   int64\n",
      " 12  green_q12   2000 non-null   int64\n",
      " 13  purple_q13  2000 non-null   int64\n",
      " 14  q14         2000 non-null   int64\n",
      " 15  blue_q15    2000 non-null   int64\n",
      " 16  red_q16     2000 non-null   int64\n",
      " 17  green_q17   2000 non-null   int64\n",
      " 18  purple_q18  2000 non-null   int64\n",
      " 19  q19         2000 non-null   int64\n",
      " 20  blue_q20    2000 non-null   int64\n",
      " 21  red_q21     2000 non-null   int64\n",
      " 22  green_q22   2000 non-null   int64\n",
      " 23  purple_q23  2000 non-null   int64\n",
      " 24  q24         2000 non-null   int64\n",
      " 25  blue_q25    2000 non-null   int64\n",
      " 26  red_q26     2000 non-null   int64\n",
      " 27  green_q27   2000 non-null   int64\n",
      " 28  purple_q28  2000 non-null   int64\n",
      " 29  q29         2000 non-null   int64\n",
      " 30  blue_q30    2000 non-null   int64\n",
      " 31  red_q31     2000 non-null   int64\n",
      " 32  green_q32   2000 non-null   int64\n",
      " 33  purple_q33  2000 non-null   int64\n",
      " 34  q34         2000 non-null   int64\n",
      " 35  blue_q35    2000 non-null   int64\n",
      " 36  red_q36     2000 non-null   int64\n",
      " 37  green_q37   2000 non-null   int64\n",
      " 38  purple_q38  2000 non-null   int64\n",
      " 39  q39         2000 non-null   int64\n",
      " 40  blue_q40    2000 non-null   int64\n",
      " 41  red_q41     2000 non-null   int64\n",
      " 42  green_q42   2000 non-null   int64\n",
      " 43  purple_q43  2000 non-null   int64\n",
      " 44  q44         2000 non-null   int64\n",
      " 45  blue_q45    2000 non-null   int64\n",
      " 46  red_q46     2000 non-null   int64\n",
      " 47  green_q47   2000 non-null   int64\n",
      " 48  purple_q48  2000 non-null   int64\n",
      " 49  q49         2000 non-null   int64\n",
      "dtypes: int64(50)\n",
      "memory usage: 781.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of instances in the dataset:\", df.shape[0])\n",
    "print(\"Number of columns in the dataset:\", df.shape[1])\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display additional information\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81r2mDP3BzZ-",
    "outputId": "8faa8967-c63d-4230-c155-42b29b34e6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Factor1  Factor2  Factor3  Factor4  Factor5\n",
      "0       20       21       21       22       28\n",
      "1       21       20       21       26       23\n",
      "2       23       20       17       22       25\n",
      "3       17       15       11       22       15\n",
      "4       20       14       24       23       24\n"
     ]
    }
   ],
   "source": [
    "# Identify column prefixes for each true factor\n",
    "factor_columns = {\n",
    "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
    "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
    "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
    "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
    "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
    "}\n",
    "\n",
    "# Calculate true factors by summing the respective columns\n",
    "true_factors = pd.DataFrame()\n",
    "for factor_name, columns in factor_columns.items():\n",
    "    true_factors[factor_name] = df[columns].sum(axis=1)\n",
    "\n",
    "# Display the first few rows of the calculated true factors\n",
    "print(true_factors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yaO1N84CB7_U"
   },
   "outputs": [],
   "source": [
    "df = df / 4.0\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jA29S2CJQV",
    "outputId": "97c14234-90fb-43e0-944e-38113a3f8b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([2000, 50])\n",
      "Number of training batches: 50\n",
      "Number of validation batches: 13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "data_array = df.to_numpy()\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for the training and validation sets\n",
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display the shape of the tensor to verify\n",
    "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rywEvDazCYkL"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define the layers of the MLP\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # Learnable embedding vector e (moved from Decoder to Encoder)\n",
    "        self.e = nn.Parameter(torch.randn(embedding_dim))\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the MLP to get Z\n",
    "        Z = self.mlp(x)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Convert Z to \\hat Z by multiplying each scalar z_i with embedding vector e\n",
    "        batch_size = Z.size(0)\n",
    "        output_dim = Z.size(1)\n",
    "        e_expanded = self.e.unsqueeze(0).unsqueeze(0)      # Shape: (1, 1, embedding_dim)\n",
    "        Z_expanded = Z.unsqueeze(2)                        # Shape: (batch_size, output_dim, 1)\n",
    "        hat_Z = Z_expanded * e_expanded                    # Shape: (batch_size, output_dim, embedding_dim)\n",
    "\n",
    "        return hat_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tUXzn0AHHq1T"
   },
   "outputs": [],
   "source": [
    "# Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim      # Number of observed variables (n)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Learnable query embeddings (e1, e2, ..., en)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
    "\n",
    "        # MultiheadAttention module with 1 head\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # MLP to predict x_i's from embeddings\n",
    "        dims = [embedding_dim] + hidden_dims + [1]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, hat_Z):\n",
    "        \"\"\"\n",
    "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = hat_Z.size(0)\n",
    "\n",
    "        # Prepare query embeddings and expand to batch size\n",
    "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)        # Output shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Add residual connection and apply layer normalization\n",
    "        out = self.layer_norm(attn_output + query_embeddings)                             # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Flatten the embeddings and pass through MLP to predict x_i's\n",
    "        out_flat = out.reshape(-1, self.embedding_dim)                                    # Shape: (batch_size * input_dim, embedding_dim)\n",
    "        x_hat_flat = self.mlp(out_flat)                                                   # Shape: (batch_size * input_dim, 1)\n",
    "        x_hat = x_hat_flat.view(batch_size, self.input_dim)                               # Shape: (batch_size, input_dim)\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xnBqmgVjIat0"
   },
   "outputs": [],
   "source": [
    "# Complete model combining the encoder and decoder\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(input_dim=input_dim, output_dim=output_dim, embedding_dim=embedding_dim, hidden_dims=encoder_hidden_dims)\n",
    "        self.decoder = Decoder(input_dim=input_dim, embedding_dim=embedding_dim, hidden_dims=decoder_hidden_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hat_Z = self.encoder(x)     # Obtain \\hat Z from the encoder\n",
    "        x_hat = self.decoder(hat_Z) # Reconstruct x from \\hat Z using the decoder\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ki1licN-PTun",
    "outputId": "fb864687-79d0-4763-ec7f-18a71da1ed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model found. Loading the model.\n",
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2729/1299719334.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Train Loss: 0.0493, Val Loss: 0.0519\n",
      "Epoch [100/1000], Train Loss: 0.0491, Val Loss: 0.0524\n",
      "Epoch [150/1000], Train Loss: 0.0487, Val Loss: 0.0530\n",
      "Epoch [200/1000], Train Loss: 0.0483, Val Loss: 0.0542\n",
      "Epoch [250/1000], Train Loss: 0.0481, Val Loss: 0.0540\n",
      "Epoch [300/1000], Train Loss: 0.0480, Val Loss: 0.0541\n",
      "Epoch [350/1000], Train Loss: 0.0482, Val Loss: 0.0539\n",
      "Epoch [400/1000], Train Loss: 0.0483, Val Loss: 0.0552\n",
      "Epoch [450/1000], Train Loss: 0.0484, Val Loss: 0.0553\n",
      "Epoch [500/1000], Train Loss: 0.0483, Val Loss: 0.0556\n",
      "Epoch [550/1000], Train Loss: 0.0480, Val Loss: 0.0554\n",
      "Epoch [600/1000], Train Loss: 0.0481, Val Loss: 0.0564\n",
      "Epoch [650/1000], Train Loss: 0.0478, Val Loss: 0.0568\n",
      "Epoch [700/1000], Train Loss: 0.0476, Val Loss: 0.0562\n",
      "Epoch [750/1000], Train Loss: 0.0476, Val Loss: 0.0566\n",
      "Epoch [800/1000], Train Loss: 0.0476, Val Loss: 0.0566\n",
      "Epoch [850/1000], Train Loss: 0.0474, Val Loss: 0.0575\n",
      "Epoch [900/1000], Train Loss: 0.0472, Val Loss: 0.0571\n",
      "Epoch [950/1000], Train Loss: 0.0476, Val Loss: 0.0575\n",
      "Epoch [1000/1000], Train Loss: 0.0475, Val Loss: 0.0573\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Assume that the Encoder, Decoder, and Model classes are already defined\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 50        # Number of observed variables (same as the number of x_i's in the dataset)\n",
    "output_dim = 5        # Output dimension of the encoder (dimension of Z)\n",
    "embedding_dim = 64    # Embedding dimension for the embeddings e and e_i's\n",
    "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
    "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1000           # Number of epochs\n",
    "batch_size = 32             # Batch size (already set in the DataLoader)\n",
    "print_every = 50           # How often to print loss (in epochs)\n",
    "\n",
    "\n",
    "model_path = \"trained_model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Trained model found. Loading the model.\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (batch,) in enumerate(train_loader):  # Use train_loader for training\n",
    "        batch = batch.to(device)  # Move batch to device\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute predicted x_hat by passing input x through the model\n",
    "        x_hat = model(batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(x_hat, batch)\n",
    "\n",
    "        # Backward pass: Compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for batch_idx, (batch,) in enumerate(val_loader):  # Use val_loader for validation\n",
    "            batch = batch.to(device)  # Move batch to device\n",
    "\n",
    "            # Forward pass for validation\n",
    "            x_hat = model(batch)\n",
    "\n",
    "            # Compute the loss for validation\n",
    "            loss = criterion(x_hat, batch)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Example save the trained model after training\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoBeOpJnYZhC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
