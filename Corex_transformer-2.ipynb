{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_HACIxMBlMv",
    "outputId": "a266a5c4-b111-46e8-b71d-77b930d36ab0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 2000\n",
      "Number of columns in the dataset: 50\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
      "0        2       0         3          1   4        1       4         1   \n",
      "1        2       0         1          2   2        1       4         3   \n",
      "2        3       0         2          1   3        1       4         3   \n",
      "3        2       0         1          1   1        0       4         1   \n",
      "4        2       0         1          1   3        0       4         3   \n",
      "\n",
      "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
      "0          2   2  ...         3        3          3           2    3   \n",
      "1          3   1  ...         2        3          2           2    3   \n",
      "2          3   0  ...         4        4          2           1    4   \n",
      "3          3   1  ...         1        2          2           1    3   \n",
      "4          2   0  ...         3        4          1           3    4   \n",
      "\n",
      "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
      "0         1        4          4           2    4  \n",
      "1         1        3          2           2    3  \n",
      "2         2        4          2           0    4  \n",
      "3         1        3          2           1    2  \n",
      "4         1        3          1           3    4  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Data Types and Non-Null Counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 50 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   blue_q0     2000 non-null   int64\n",
      " 1   red_q1      2000 non-null   int64\n",
      " 2   green_q2    2000 non-null   int64\n",
      " 3   purple_q3   2000 non-null   int64\n",
      " 4   q4          2000 non-null   int64\n",
      " 5   blue_q5     2000 non-null   int64\n",
      " 6   red_q6      2000 non-null   int64\n",
      " 7   green_q7    2000 non-null   int64\n",
      " 8   purple_q8   2000 non-null   int64\n",
      " 9   q9          2000 non-null   int64\n",
      " 10  blue_q10    2000 non-null   int64\n",
      " 11  red_q11     2000 non-null   int64\n",
      " 12  green_q12   2000 non-null   int64\n",
      " 13  purple_q13  2000 non-null   int64\n",
      " 14  q14         2000 non-null   int64\n",
      " 15  blue_q15    2000 non-null   int64\n",
      " 16  red_q16     2000 non-null   int64\n",
      " 17  green_q17   2000 non-null   int64\n",
      " 18  purple_q18  2000 non-null   int64\n",
      " 19  q19         2000 non-null   int64\n",
      " 20  blue_q20    2000 non-null   int64\n",
      " 21  red_q21     2000 non-null   int64\n",
      " 22  green_q22   2000 non-null   int64\n",
      " 23  purple_q23  2000 non-null   int64\n",
      " 24  q24         2000 non-null   int64\n",
      " 25  blue_q25    2000 non-null   int64\n",
      " 26  red_q26     2000 non-null   int64\n",
      " 27  green_q27   2000 non-null   int64\n",
      " 28  purple_q28  2000 non-null   int64\n",
      " 29  q29         2000 non-null   int64\n",
      " 30  blue_q30    2000 non-null   int64\n",
      " 31  red_q31     2000 non-null   int64\n",
      " 32  green_q32   2000 non-null   int64\n",
      " 33  purple_q33  2000 non-null   int64\n",
      " 34  q34         2000 non-null   int64\n",
      " 35  blue_q35    2000 non-null   int64\n",
      " 36  red_q36     2000 non-null   int64\n",
      " 37  green_q37   2000 non-null   int64\n",
      " 38  purple_q38  2000 non-null   int64\n",
      " 39  q39         2000 non-null   int64\n",
      " 40  blue_q40    2000 non-null   int64\n",
      " 41  red_q41     2000 non-null   int64\n",
      " 42  green_q42   2000 non-null   int64\n",
      " 43  purple_q43  2000 non-null   int64\n",
      " 44  q44         2000 non-null   int64\n",
      " 45  blue_q45    2000 non-null   int64\n",
      " 46  red_q46     2000 non-null   int64\n",
      " 47  green_q47   2000 non-null   int64\n",
      " 48  purple_q48  2000 non-null   int64\n",
      " 49  q49         2000 non-null   int64\n",
      "dtypes: int64(50)\n",
      "memory usage: 781.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of instances in the dataset:\", df.shape[0])\n",
    "print(\"Number of columns in the dataset:\", df.shape[1])\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display additional information\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yaO1N84CB7_U"
   },
   "outputs": [],
   "source": [
    "df = df / 4.0\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1600, 50)\n",
      "Test set shape: (400, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target (Y) if necessary.\n",
    "# In the case of autoencoder-like models, we do not have target Y, so we'll treat the whole dataset as X.\n",
    "X = df.values  # Convert the DataFrame into a NumPy array for model input\n",
    "\n",
    "# Split the dataset into training (80%) and testing sets (20%)\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shapes to verify\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jA29S2CJQV",
    "outputId": "97c14234-90fb-43e0-944e-38113a3f8b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([2000, 50])\n",
      "Number of training batches: 50\n",
      "Number of validation batches: 13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "data_array = df.to_numpy()\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for the training and validation sets\n",
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display the shape of the tensor to verify\n",
    "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rywEvDazCYkL"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define the layers of the MLP\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # Learnable embedding vectors e_i for each z_i\n",
    "        self.e = nn.Parameter(torch.randn(output_dim, embedding_dim))\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the MLP to get Z\n",
    "        Z = self.mlp(x)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Convert Z to \\hat Z by multiplying each scalar z_i with its own embedding vector e_i\n",
    "        batch_size = Z.size(0)\n",
    "        Z_expanded = Z.unsqueeze(2)                         # Shape: (batch_size, output_dim, 1)\n",
    "        e_expanded = self.e.unsqueeze(0)                    # Shape: (1, output_dim, embedding_dim)\n",
    "        hat_Z = Z_expanded * e_expanded                     # Shape: (batch_size, output_dim, embedding_dim)\n",
    "\n",
    "        return hat_Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tUXzn0AHHq1T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim      # Number of observed variables (n)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Learnable query embeddings (e1, e2, ..., en)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
    "\n",
    "        # MultiheadAttention module with 1 head\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Define individual MLPs for each observed variable\n",
    "        dims = [embedding_dim] + hidden_dims + [1]\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            nn.Sequential(*[\n",
    "                nn.Linear(dims[i], dims[i + 1]) if i == len(dims) - 2 else nn.Sequential(\n",
    "                    nn.Linear(dims[i], dims[i + 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                for i in range(len(dims) - 1)\n",
    "            ])\n",
    "            for _ in range(input_dim)\n",
    "        ])\n",
    "\n",
    "    def forward(self, hat_Z):\n",
    "        \"\"\"\n",
    "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = hat_Z.size(0)\n",
    "\n",
    "        # Prepare query embeddings and expand to batch size\n",
    "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)        # Output shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Add residual connection and apply layer normalization\n",
    "        out = self.layer_norm(attn_output + query_embeddings)                             # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Pass each context vector through its corresponding MLP\n",
    "        x_hat = []\n",
    "        for i in range(self.input_dim):\n",
    "            x_i = out[:, i, :]        # Shape: (batch_size, embedding_dim)\n",
    "            x_i_hat = self.mlp_layers[i](x_i)  # Shape: (batch_size, 1)\n",
    "            x_hat.append(x_i_hat)\n",
    "        x_hat = torch.cat(x_hat, dim=1)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        return x_hat, attn_weights  # Return attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xnBqmgVjIat0"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=encoder_hidden_dims\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            input_dim=input_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=decoder_hidden_dims\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hat_Z = self.encoder(x)     # Obtain \\hat{Z} from the encoder\n",
    "        x_hat, attn_weights = self.decoder(hat_Z)  # Reconstruct x from \\hat{Z} using the decoder\n",
    "        return x_hat, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ki1licN-PTun",
    "outputId": "fb864687-79d0-4763-ec7f-18a71da1ed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Lambda Entropy: 0.000000, Train Total Loss: 0.1183, Train Recon Loss: 0.1183, Train Entropy Loss: 68.5714, Val Total Loss: 0.0874, Val Recon Loss: 0.0874, Val Entropy Loss: 68.8017\n",
      "Epoch [2/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0771, Train Recon Loss: 0.0771, Train Entropy Loss: 64.3709, Val Total Loss: 0.0727, Val Recon Loss: 0.0727, Val Entropy Loss: 62.2677\n",
      "Epoch [3/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0678, Train Recon Loss: 0.0678, Train Entropy Loss: 63.8929, Val Total Loss: 0.0675, Val Recon Loss: 0.0675, Val Entropy Loss: 64.6240\n",
      "Epoch [4/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0642, Train Recon Loss: 0.0642, Train Entropy Loss: 64.5979, Val Total Loss: 0.0629, Val Recon Loss: 0.0629, Val Entropy Loss: 66.0934\n",
      "Epoch [5/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0592, Train Recon Loss: 0.0592, Train Entropy Loss: 65.4313, Val Total Loss: 0.0581, Val Recon Loss: 0.0581, Val Entropy Loss: 67.4503\n",
      "Epoch [6/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0549, Train Recon Loss: 0.0549, Train Entropy Loss: 68.2066, Val Total Loss: 0.0540, Val Recon Loss: 0.0540, Val Entropy Loss: 70.0061\n",
      "Epoch [7/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0518, Train Recon Loss: 0.0518, Train Entropy Loss: 70.6280, Val Total Loss: 0.0529, Val Recon Loss: 0.0529, Val Entropy Loss: 71.5453\n",
      "Epoch [8/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0505, Train Recon Loss: 0.0505, Train Entropy Loss: 71.7676, Val Total Loss: 0.0516, Val Recon Loss: 0.0516, Val Entropy Loss: 72.0749\n",
      "Epoch [9/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0499, Train Recon Loss: 0.0499, Train Entropy Loss: 72.4129, Val Total Loss: 0.0517, Val Recon Loss: 0.0517, Val Entropy Loss: 72.8617\n",
      "Epoch [10/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0493, Train Recon Loss: 0.0493, Train Entropy Loss: 73.0714, Val Total Loss: 0.0510, Val Recon Loss: 0.0510, Val Entropy Loss: 73.5096\n",
      "Epoch [11/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0489, Train Recon Loss: 0.0489, Train Entropy Loss: 73.5726, Val Total Loss: 0.0505, Val Recon Loss: 0.0505, Val Entropy Loss: 74.2516\n",
      "Epoch [12/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0488, Train Recon Loss: 0.0488, Train Entropy Loss: 73.8815, Val Total Loss: 0.0501, Val Recon Loss: 0.0501, Val Entropy Loss: 74.3717\n",
      "Epoch [13/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0484, Train Recon Loss: 0.0484, Train Entropy Loss: 74.1419, Val Total Loss: 0.0495, Val Recon Loss: 0.0495, Val Entropy Loss: 74.6926\n",
      "Epoch [14/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0481, Train Recon Loss: 0.0481, Train Entropy Loss: 74.5070, Val Total Loss: 0.0495, Val Recon Loss: 0.0495, Val Entropy Loss: 74.8258\n",
      "Epoch [15/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0478, Train Recon Loss: 0.0478, Train Entropy Loss: 74.8411, Val Total Loss: 0.0494, Val Recon Loss: 0.0494, Val Entropy Loss: 75.5585\n",
      "Epoch [16/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0475, Train Recon Loss: 0.0475, Train Entropy Loss: 75.1967, Val Total Loss: 0.0495, Val Recon Loss: 0.0495, Val Entropy Loss: 75.3253\n",
      "Epoch [17/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0475, Train Recon Loss: 0.0475, Train Entropy Loss: 75.5706, Val Total Loss: 0.0491, Val Recon Loss: 0.0491, Val Entropy Loss: 75.9460\n",
      "Epoch [18/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0472, Train Recon Loss: 0.0472, Train Entropy Loss: 75.8292, Val Total Loss: 0.0490, Val Recon Loss: 0.0490, Val Entropy Loss: 76.0429\n",
      "Epoch [19/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0471, Train Recon Loss: 0.0471, Train Entropy Loss: 76.0494, Val Total Loss: 0.0490, Val Recon Loss: 0.0490, Val Entropy Loss: 76.5605\n",
      "Epoch [20/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0469, Train Recon Loss: 0.0469, Train Entropy Loss: 76.4271, Val Total Loss: 0.0489, Val Recon Loss: 0.0489, Val Entropy Loss: 76.7125\n",
      "Epoch [21/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0466, Train Recon Loss: 0.0466, Train Entropy Loss: 76.6746, Val Total Loss: 0.0487, Val Recon Loss: 0.0487, Val Entropy Loss: 76.9608\n",
      "Epoch [22/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0467, Train Recon Loss: 0.0467, Train Entropy Loss: 76.9705, Val Total Loss: 0.0489, Val Recon Loss: 0.0489, Val Entropy Loss: 77.3666\n",
      "Epoch [23/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0465, Train Recon Loss: 0.0465, Train Entropy Loss: 77.2428, Val Total Loss: 0.0486, Val Recon Loss: 0.0486, Val Entropy Loss: 77.4222\n",
      "Epoch [24/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0464, Train Recon Loss: 0.0464, Train Entropy Loss: 77.4231, Val Total Loss: 0.0484, Val Recon Loss: 0.0484, Val Entropy Loss: 77.5145\n",
      "Epoch [25/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0462, Train Recon Loss: 0.0462, Train Entropy Loss: 77.5956, Val Total Loss: 0.0486, Val Recon Loss: 0.0486, Val Entropy Loss: 77.6718\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume that the Encoder, Decoder, and Model classes are already defined\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 50        # Number of observed variables\n",
    "output_dim = 5        # Output dimension of the encoder (dimension of Z)\n",
    "embedding_dim = 64    # Embedding dimension for the embeddings e and e_i's\n",
    "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
    "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 25          # Number of epochs\n",
    "batch_size = 32          # Batch size (already set in the DataLoader)\n",
    "print_every = 1          # How often to print loss (in epochs)\n",
    "\n",
    "# Define the maximum value for the entropy regularization coefficient\n",
    "max_lambda_entropy = 0.5*1e-4  # Adjust this value as needed\n",
    "\n",
    "# Flag to enable or disable entropy regularizer\n",
    "use_entropy_regularizer = False  # Set to True to enable, False to disable\n",
    "\n",
    "# Scheduler function for lambda_entropy\n",
    "def get_lambda_entropy(epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=True):\n",
    "    if not use_entropy_regularizer:\n",
    "        return 0.0\n",
    "    if schedule_type == 'constant':\n",
    "        # Always return max_lambda_entropy\n",
    "        return max_lambda_entropy\n",
    "    elif schedule_type == 'linear':\n",
    "        # Linear increase from 0 to max_lambda_entropy\n",
    "        return max_lambda_entropy * (epoch / num_epochs)\n",
    "    elif schedule_type == 'exponential':\n",
    "        # Exponential increase from 0 to max_lambda_entropy\n",
    "        k = 5  # Adjust this value to control the speed of increase\n",
    "        return max_lambda_entropy * (1 - math.exp(-k * epoch / num_epochs))\n",
    "    elif schedule_type == 'logarithmic':\n",
    "        # Logarithmic increase from 0 to max_lambda_entropy\n",
    "        if epoch == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return max_lambda_entropy * math.log(epoch + 1) / math.log(num_epochs + 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
    "\n",
    "model_path = \"trained_model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Trained model found. Loading the model.\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize a list to store the average attention matrices per epoch\n",
    "attention_matrices = []\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute lambda_entropy for the current epoch\n",
    "    lambda_entropy = get_lambda_entropy(\n",
    "        epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=use_entropy_regularizer)\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0          # Accumulates total loss (reconstruction + regularizer)\n",
    "    running_recon_loss = 0.0    # Accumulates reconstruction loss\n",
    "    running_entropy_loss = 0.0  # Accumulates entrop loss\n",
    "    epoch_attn_weights = []     # List to store attention weights for all batches in the epoch\n",
    "\n",
    "    for batch_idx, (batch,) in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute predicted x_hat and attention weights\n",
    "        x_hat, attn_weights = model(batch)\n",
    "\n",
    "        # Squeeze attn_weights to get shape: (batch_size, input_dim, output_dim)\n",
    "        attn_weights = attn_weights.squeeze(1)\n",
    "\n",
    "        # Collect attention weights for the epoch\n",
    "        epoch_attn_weights.append(attn_weights.detach().cpu())\n",
    "\n",
    "        # Compute the reconstruction loss\n",
    "        recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "        # Initialize entropy_regularizer to zero\n",
    "        entropy_regularizer = 0.0\n",
    "\n",
    "        # Add a small epsilon to prevent log(0)\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        # Compute entropy for each query (input_dim)\n",
    "        entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Sum entropies over queries and average over batch\n",
    "        entropy_regularizer = torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
    "\n",
    "        # Total loss\n",
    "        loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training losses\n",
    "        running_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_entropy_loss += entropy_regularizer  # Accumulate entropy regularizer loss (before scaling)\n",
    "\n",
    "    # Compute average losses for training\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_train_recon_loss = running_recon_loss / len(train_loader)\n",
    "    avg_train_entropy_loss = running_entropy_loss / len(train_loader)\n",
    "\n",
    "    # Compute the average attention matrix for the epoch\n",
    "    epoch_attn_weights_tensor = torch.cat(epoch_attn_weights, dim=0)  # Shape: (num_samples_in_epoch, input_dim, output_dim)\n",
    "    avg_attn_weights_epoch = epoch_attn_weights_tensor.mean(dim=0)    # Shape: (input_dim, output_dim)\n",
    "    avg_attn_weights_epoch_np = avg_attn_weights_epoch.numpy()\n",
    "\n",
    "    # Transpose to have shape (output_dim, input_dim) so that queries are on x-axis and keys on y-axis\n",
    "    attention_matrices.append(avg_attn_weights_epoch_np.T)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0          # Accumulates total loss (reconstruction + regularizer)\n",
    "    val_recon_loss = 0.0    # Accumulates reconstruction loss\n",
    "    val_entropy_loss = 0.0  # Accumulates entropy regularizer loss\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch,) in enumerate(val_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            x_hat, attn_weights = model(batch)\n",
    "\n",
    "            # Compute the reconstruction loss\n",
    "            recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "            # Initialize entropy_regularizer to zero\n",
    "            entropy_regularizer = 0.0\n",
    "\n",
    "            attn_weights = attn_weights.squeeze(1)  # Shape: (batch_size, input_dim, output_dim)\n",
    "\n",
    "            # Add a small epsilon to prevent log(0)\n",
    "            epsilon = 1e-8\n",
    "\n",
    "            # Compute entropy for each query (input_dim)\n",
    "            entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "            # Sum entropies over queries and average over batch\n",
    "            entropy_regularizer = torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "            # Accumulate validation losses\n",
    "            val_loss += loss.item()\n",
    "            val_recon_loss += recon_loss.item()\n",
    "            val_entropy_loss += entropy_regularizer  # Accumulate entropy regularizer loss (before scaling)\n",
    "\n",
    "    # Compute average losses for validation\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_recon_loss = val_recon_loss / len(val_loader)\n",
    "    avg_val_entropy_loss = val_entropy_loss / len(val_loader)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Lambda Entropy: {lambda_entropy:.6f}, '\n",
    "              f'Train Total Loss: {avg_train_loss:.4f}, Train Recon Loss: {avg_train_recon_loss:.4f}, Train Entropy Loss: {avg_train_entropy_loss:.4f}, '\n",
    "              f'Val Total Loss: {avg_val_loss:.4f}, Val Recon Loss: {avg_val_recon_loss:.4f}, Val Entropy Loss: {avg_val_entropy_loss:.4f}')\n",
    "\n",
    "# # Save the trained model after training\n",
    "# torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "# print(\"Training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Assuming 'df' is your original dataframe\n",
    "\n",
    "# Identify column prefixes for each true factor\n",
    "factor_columns = {\n",
    "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
    "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
    "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
    "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
    "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
    "}\n",
    "\n",
    "# Map factor names to column indices\n",
    "factor_indices = {}\n",
    "for factor_name, columns in factor_columns.items():\n",
    "    indices = [df.columns.get_loc(col) for col in columns]\n",
    "    factor_indices[factor_name] = indices\n",
    "\n",
    "# Create a new ordering of indices\n",
    "new_order = []\n",
    "for factor_name in factor_columns.keys():\n",
    "    new_order.extend(factor_indices[factor_name])\n",
    "\n",
    "# Ensure all indices are included\n",
    "assert len(new_order) == df.shape[1], \"Not all indices are included in the new order.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rearranging attention matrices for the animation\n",
    "# reordered_attention_matrices = []\n",
    "\n",
    "# for attn_matrix in attention_matrices:\n",
    "#     # attn_matrix has shape (output_dim, input_dim)\n",
    "#     # Rearrange the columns (observed variables) according to new_order\n",
    "#     attn_matrix_reordered = attn_matrix[:, new_order]  # Shape: (output_dim, input_dim)\n",
    "#     reordered_attention_matrices.append(attn_matrix_reordered)\n",
    "\n",
    "# # Convert the list to a NumPy array for animation\n",
    "# attention_matrices_array = np.stack(reordered_attention_matrices)  # Shape: (num_epochs, output_dim, input_dim)\n",
    "\n",
    "# # Determine the number of frames (epochs)\n",
    "# num_frames = attention_matrices_array.shape[0]\n",
    "\n",
    "# # Set up the figure and axis for the animation\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# # Initialize the heatmap with empty data\n",
    "# im = ax.imshow(np.zeros_like(attention_matrices_array[0]), aspect='auto', cmap='viridis', vmin=0, vmax=1)\n",
    "# ax.set_xlabel('Observed Variables')\n",
    "# ax.set_ylabel('Latent Factors')\n",
    "# title = ax.set_title('Attention Matrix at Epoch 1')\n",
    "\n",
    "# # Function to initialize the heatmap\n",
    "# def init():\n",
    "#     data = attention_matrices_array[0]\n",
    "#     im.set_data(data)\n",
    "#     title.set_text('Attention Matrix at Epoch 1')\n",
    "#     return [im, title]\n",
    "\n",
    "# # Function to update the heatmap for each frame\n",
    "# def update(frame):\n",
    "#     data = attention_matrices_array[frame]\n",
    "#     im.set_data(data)\n",
    "#     title.set_text(f'Attention Matrix at Epoch {frame + 1}')\n",
    "#     return [im, title]\n",
    "\n",
    "# # Create the animation\n",
    "# ani = FuncAnimation(fig, update, frames=num_frames, init_func=init, blit=True)\n",
    "\n",
    "# # Close the figure to prevent the static plot from displaying\n",
    "# plt.close(fig)\n",
    "\n",
    "# # Display the animation in the notebook\n",
    "# display(HTML(ani.to_jshtml()))\n",
    "\n",
    "# # Save the animation to an MP4 file (optional)\n",
    "# ani.save('attention_animation.mp4', writer='ffmpeg', fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average attention matrices (assuming you have 'model', 'train_loader', and 'val_loader')\n",
    "def compute_average_attention(model, dataloader, device):\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "    total_attn = None\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs = data[0].to(device)\n",
    "\n",
    "            # Forward pass up to obtaining attention weights\n",
    "            hat_Z = model.encoder(inputs)\n",
    "            _, attn_weights = model.decoder(hat_Z)\n",
    "\n",
    "            # attn_weights shape: (batch_size, input_dim, output_dim)\n",
    "            batch_size = attn_weights.size(0)\n",
    "            if total_attn is None:\n",
    "                total_attn = attn_weights.sum(dim=0)  # Sum over batch dimension\n",
    "            else:\n",
    "                total_attn += attn_weights.sum(dim=0)\n",
    "            num_samples += batch_size\n",
    "\n",
    "    # Average the attention weights\n",
    "    avg_attn = total_attn / num_samples\n",
    "\n",
    "    return avg_attn.cpu().numpy()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Compute average attention matrices\n",
    "avg_attn_train = compute_average_attention(model, train_loader, device)\n",
    "avg_attn_val = compute_average_attention(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index: 0.23548335209908317\n"
     ]
    }
   ],
   "source": [
    "# Calculating ARI\n",
    "input_dim = avg_attn_train.shape[0]   # Number of observed variables\n",
    "output_dim = avg_attn_train.shape[1]  # Number of latent factors\n",
    "\n",
    "# Step 1: Assign each observed variable to the latent factor with the highest attention weight\n",
    "predicted_labels = np.argmax(avg_attn_train, axis=1)  # Shape: (input_dim,)\n",
    "\n",
    "# Step 2: Create true labels based on ideal groups\n",
    "true_labels = np.full(input_dim, -1)  # Initialize with -1\n",
    "\n",
    "# Map factor names to indices\n",
    "factor_names = ['Factor1', 'Factor2', 'Factor3', 'Factor4', 'Factor5']\n",
    "factor_name_to_index = {name: idx for idx, name in enumerate(factor_names)}\n",
    "\n",
    "# Assign labels to observed variables based on factors\n",
    "for factor_name, indices in factor_indices.items():\n",
    "    factor_idx = factor_name_to_index[factor_name]\n",
    "    true_labels[indices] = factor_idx\n",
    "\n",
    "# Ensure all observed variables have been assigned\n",
    "assert np.all(true_labels >= 0), \"Some observed variables have not been assigned a true label\"\n",
    "\n",
    "# Step 3: Compute Adjusted Rand Index\n",
    "ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Adjusted Rand Index:\", ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transpose the average attention matrices\n",
    "avg_attn_train_transposed = avg_attn_train.T  # Shape: (output_dim, input_dim)\n",
    "avg_attn_val_transposed = avg_attn_val.T      # Shape: (output_dim, input_dim)\n",
    "\n",
    "# Rearrange the columns (observed variables) using new_order\n",
    "avg_attn_train_reordered = avg_attn_train_transposed[:, new_order]\n",
    "avg_attn_val_reordered = avg_attn_val_transposed[:, new_order]\n",
    "\n",
    "def plot_attention_heatmap(attn_matrix, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        attn_matrix,\n",
    "        cmap='viridis',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Observed Variables')\n",
    "    plt.ylabel('Latent Factors')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot average attention matrices with fixed color scaling\n",
    "plot_attention_heatmap(avg_attn_train_reordered, 'Average Attention Matrix - Training Set')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
