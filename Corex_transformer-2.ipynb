{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_HACIxMBlMv",
    "outputId": "a266a5c4-b111-46e8-b71d-77b930d36ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 2000\n",
      "Number of columns in the dataset: 50\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
      "0        2       0         3          1   4        1       4         1   \n",
      "1        2       0         1          2   2        1       4         3   \n",
      "2        3       0         2          1   3        1       4         3   \n",
      "3        2       0         1          1   1        0       4         1   \n",
      "4        2       0         1          1   3        0       4         3   \n",
      "\n",
      "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
      "0          2   2  ...         3        3          3           2    3   \n",
      "1          3   1  ...         2        3          2           2    3   \n",
      "2          3   0  ...         4        4          2           1    4   \n",
      "3          3   1  ...         1        2          2           1    3   \n",
      "4          2   0  ...         3        4          1           3    4   \n",
      "\n",
      "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
      "0         1        4          4           2    4  \n",
      "1         1        3          2           2    3  \n",
      "2         2        4          2           0    4  \n",
      "3         1        3          2           1    2  \n",
      "4         1        3          1           3    4  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Data Types and Non-Null Counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 50 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   blue_q0     2000 non-null   int64\n",
      " 1   red_q1      2000 non-null   int64\n",
      " 2   green_q2    2000 non-null   int64\n",
      " 3   purple_q3   2000 non-null   int64\n",
      " 4   q4          2000 non-null   int64\n",
      " 5   blue_q5     2000 non-null   int64\n",
      " 6   red_q6      2000 non-null   int64\n",
      " 7   green_q7    2000 non-null   int64\n",
      " 8   purple_q8   2000 non-null   int64\n",
      " 9   q9          2000 non-null   int64\n",
      " 10  blue_q10    2000 non-null   int64\n",
      " 11  red_q11     2000 non-null   int64\n",
      " 12  green_q12   2000 non-null   int64\n",
      " 13  purple_q13  2000 non-null   int64\n",
      " 14  q14         2000 non-null   int64\n",
      " 15  blue_q15    2000 non-null   int64\n",
      " 16  red_q16     2000 non-null   int64\n",
      " 17  green_q17   2000 non-null   int64\n",
      " 18  purple_q18  2000 non-null   int64\n",
      " 19  q19         2000 non-null   int64\n",
      " 20  blue_q20    2000 non-null   int64\n",
      " 21  red_q21     2000 non-null   int64\n",
      " 22  green_q22   2000 non-null   int64\n",
      " 23  purple_q23  2000 non-null   int64\n",
      " 24  q24         2000 non-null   int64\n",
      " 25  blue_q25    2000 non-null   int64\n",
      " 26  red_q26     2000 non-null   int64\n",
      " 27  green_q27   2000 non-null   int64\n",
      " 28  purple_q28  2000 non-null   int64\n",
      " 29  q29         2000 non-null   int64\n",
      " 30  blue_q30    2000 non-null   int64\n",
      " 31  red_q31     2000 non-null   int64\n",
      " 32  green_q32   2000 non-null   int64\n",
      " 33  purple_q33  2000 non-null   int64\n",
      " 34  q34         2000 non-null   int64\n",
      " 35  blue_q35    2000 non-null   int64\n",
      " 36  red_q36     2000 non-null   int64\n",
      " 37  green_q37   2000 non-null   int64\n",
      " 38  purple_q38  2000 non-null   int64\n",
      " 39  q39         2000 non-null   int64\n",
      " 40  blue_q40    2000 non-null   int64\n",
      " 41  red_q41     2000 non-null   int64\n",
      " 42  green_q42   2000 non-null   int64\n",
      " 43  purple_q43  2000 non-null   int64\n",
      " 44  q44         2000 non-null   int64\n",
      " 45  blue_q45    2000 non-null   int64\n",
      " 46  red_q46     2000 non-null   int64\n",
      " 47  green_q47   2000 non-null   int64\n",
      " 48  purple_q48  2000 non-null   int64\n",
      " 49  q49         2000 non-null   int64\n",
      "dtypes: int64(50)\n",
      "memory usage: 781.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of instances in the dataset:\", df.shape[0])\n",
    "print(\"Number of columns in the dataset:\", df.shape[1])\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display additional information\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81r2mDP3BzZ-",
    "outputId": "8faa8967-c63d-4230-c155-42b29b34e6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Factor1  Factor2  Factor3  Factor4  Factor5\n",
      "0       20       21       21       22       28\n",
      "1       21       20       21       26       23\n",
      "2       23       20       17       22       25\n",
      "3       17       15       11       22       15\n",
      "4       20       14       24       23       24\n"
     ]
    }
   ],
   "source": [
    "# Identify column prefixes for each true factor\n",
    "factor_columns = {\n",
    "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
    "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
    "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
    "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
    "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
    "}\n",
    "\n",
    "# Calculate true factors by summing the respective columns\n",
    "true_factors = pd.DataFrame()\n",
    "for factor_name, columns in factor_columns.items():\n",
    "    true_factors[factor_name] = df[columns].sum(axis=1)\n",
    "\n",
    "# Display the first few rows of the calculated true factors\n",
    "print(true_factors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yaO1N84CB7_U"
   },
   "outputs": [],
   "source": [
    "df = df / 4.0\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1600, 50)\n",
      "Test set shape: (400, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target (Y) if necessary.\n",
    "# In the case of autoencoder-like models, we do not have target Y, so we'll treat the whole dataset as X.\n",
    "X = df.values  # Convert the DataFrame into a NumPy array for model input\n",
    "\n",
    "# Split the dataset into training (80%) and testing sets (20%)\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shapes to verify\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jA29S2CJQV",
    "outputId": "97c14234-90fb-43e0-944e-38113a3f8b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([2000, 50])\n",
      "Number of training batches: 50\n",
      "Number of validation batches: 13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "data_array = df.to_numpy()\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for the training and validation sets\n",
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display the shape of the tensor to verify\n",
    "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rywEvDazCYkL"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define the layers of the MLP\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # Learnable embedding vectors e_i for each z_i\n",
    "        self.e = nn.Parameter(torch.randn(output_dim, embedding_dim))\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the MLP to get Z\n",
    "        Z = self.mlp(x)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Convert Z to \\hat Z by multiplying each scalar z_i with its own embedding vector e_i\n",
    "        batch_size = Z.size(0)\n",
    "        Z_expanded = Z.unsqueeze(2)                         # Shape: (batch_size, output_dim, 1)\n",
    "        e_expanded = self.e.unsqueeze(0)                    # Shape: (1, output_dim, embedding_dim)\n",
    "        hat_Z = Z_expanded * e_expanded                     # Shape: (batch_size, output_dim, embedding_dim)\n",
    "\n",
    "        return hat_Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tUXzn0AHHq1T"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim      # Number of observed variables (n)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Learnable query embeddings (e1, e2, ..., en)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
    "\n",
    "        # MultiheadAttention module with 1 head\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # MLP to predict x_i's from embeddings\n",
    "        dims = [embedding_dim] + hidden_dims + [1]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, hat_Z):\n",
    "        \"\"\"\n",
    "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = hat_Z.size(0)\n",
    "\n",
    "        # Prepare query embeddings and expand to batch size\n",
    "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)        # Output shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Add residual connection and apply layer normalization\n",
    "        out = self.layer_norm(attn_output + query_embeddings)                             # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Flatten the embeddings and pass through MLP to predict x_i's\n",
    "        out_flat = out.reshape(-1, self.embedding_dim)                                    # Shape: (batch_size * input_dim, embedding_dim)\n",
    "        x_hat_flat = self.mlp(out_flat)                                                   # Shape: (batch_size * input_dim, 1)\n",
    "        x_hat = x_hat_flat.view(batch_size, self.input_dim)                               # Shape: (batch_size, input_dim)\n",
    "\n",
    "        return x_hat, attn_weights  # Return attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xnBqmgVjIat0"
   },
   "outputs": [],
   "source": [
    "# Complete model combining the encoder and decoder\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(input_dim=input_dim, output_dim=output_dim, embedding_dim=embedding_dim, hidden_dims=encoder_hidden_dims)\n",
    "        self.decoder = Decoder(input_dim=input_dim, embedding_dim=embedding_dim, hidden_dims=decoder_hidden_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hat_Z = self.encoder(x)     # Obtain \\hat Z from the encoder\n",
    "        x_hat, attn_weights = self.decoder(hat_Z) # Reconstruct x from \\hat Z using the decoder\n",
    "        return x_hat, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ki1licN-PTun",
    "outputId": "fb864687-79d0-4763-ec7f-18a71da1ed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Lambda Entropy: 0.000000, Train Total Loss: 0.1156, Train Recon Loss: 0.1156, Val Total Loss: 0.0906, Val Recon Loss: 0.0906\n",
      "Epoch [2/200], Lambda Entropy: 0.000025, Train Total Loss: 0.0903, Train Recon Loss: 0.0884, Val Total Loss: 0.0891, Val Recon Loss: 0.0874\n",
      "Epoch [3/200], Lambda Entropy: 0.000049, Train Total Loss: 0.0805, Train Recon Loss: 0.0775, Val Total Loss: 0.0722, Val Recon Loss: 0.0697\n",
      "Epoch [4/200], Lambda Entropy: 0.000072, Train Total Loss: 0.0701, Train Recon Loss: 0.0670, Val Total Loss: 0.0675, Val Recon Loss: 0.0650\n",
      "Epoch [5/200], Lambda Entropy: 0.000095, Train Total Loss: 0.0669, Train Recon Loss: 0.0641, Val Total Loss: 0.0657, Val Recon Loss: 0.0634\n",
      "Epoch [6/200], Lambda Entropy: 0.000118, Train Total Loss: 0.0651, Train Recon Loss: 0.0626, Val Total Loss: 0.0645, Val Recon Loss: 0.0625\n",
      "Epoch [7/200], Lambda Entropy: 0.000139, Train Total Loss: 0.0639, Train Recon Loss: 0.0616, Val Total Loss: 0.0630, Val Recon Loss: 0.0610\n",
      "Epoch [8/200], Lambda Entropy: 0.000161, Train Total Loss: 0.0625, Train Recon Loss: 0.0604, Val Total Loss: 0.0622, Val Recon Loss: 0.0604\n",
      "Epoch [9/200], Lambda Entropy: 0.000181, Train Total Loss: 0.0617, Train Recon Loss: 0.0598, Val Total Loss: 0.0614, Val Recon Loss: 0.0596\n",
      "Epoch [10/200], Lambda Entropy: 0.000201, Train Total Loss: 0.0609, Train Recon Loss: 0.0590, Val Total Loss: 0.0608, Val Recon Loss: 0.0591\n",
      "Epoch [11/200], Lambda Entropy: 0.000221, Train Total Loss: 0.0600, Train Recon Loss: 0.0582, Val Total Loss: 0.0605, Val Recon Loss: 0.0588\n",
      "Epoch [12/200], Lambda Entropy: 0.000240, Train Total Loss: 0.0597, Train Recon Loss: 0.0580, Val Total Loss: 0.0605, Val Recon Loss: 0.0588\n",
      "Epoch [13/200], Lambda Entropy: 0.000259, Train Total Loss: 0.0595, Train Recon Loss: 0.0579, Val Total Loss: 0.0597, Val Recon Loss: 0.0582\n",
      "Epoch [14/200], Lambda Entropy: 0.000277, Train Total Loss: 0.0591, Train Recon Loss: 0.0574, Val Total Loss: 0.0600, Val Recon Loss: 0.0585\n",
      "Epoch [15/200], Lambda Entropy: 0.000295, Train Total Loss: 0.0587, Train Recon Loss: 0.0571, Val Total Loss: 0.0597, Val Recon Loss: 0.0583\n",
      "Epoch [16/200], Lambda Entropy: 0.000313, Train Total Loss: 0.0586, Train Recon Loss: 0.0571, Val Total Loss: 0.0593, Val Recon Loss: 0.0579\n",
      "Epoch [17/200], Lambda Entropy: 0.000330, Train Total Loss: 0.0585, Train Recon Loss: 0.0570, Val Total Loss: 0.0591, Val Recon Loss: 0.0577\n",
      "Epoch [18/200], Lambda Entropy: 0.000346, Train Total Loss: 0.0584, Train Recon Loss: 0.0569, Val Total Loss: 0.0597, Val Recon Loss: 0.0583\n",
      "Epoch [19/200], Lambda Entropy: 0.000362, Train Total Loss: 0.0582, Train Recon Loss: 0.0568, Val Total Loss: 0.0591, Val Recon Loss: 0.0578\n",
      "Epoch [20/200], Lambda Entropy: 0.000378, Train Total Loss: 0.0581, Train Recon Loss: 0.0568, Val Total Loss: 0.0590, Val Recon Loss: 0.0578\n",
      "Epoch [21/200], Lambda Entropy: 0.000393, Train Total Loss: 0.0580, Train Recon Loss: 0.0567, Val Total Loss: 0.0588, Val Recon Loss: 0.0576\n",
      "Epoch [22/200], Lambda Entropy: 0.000408, Train Total Loss: 0.0580, Train Recon Loss: 0.0567, Val Total Loss: 0.0593, Val Recon Loss: 0.0582\n",
      "Epoch [23/200], Lambda Entropy: 0.000423, Train Total Loss: 0.0581, Train Recon Loss: 0.0569, Val Total Loss: 0.0591, Val Recon Loss: 0.0580\n",
      "Epoch [24/200], Lambda Entropy: 0.000437, Train Total Loss: 0.0579, Train Recon Loss: 0.0567, Val Total Loss: 0.0586, Val Recon Loss: 0.0575\n",
      "Epoch [25/200], Lambda Entropy: 0.000451, Train Total Loss: 0.0575, Train Recon Loss: 0.0564, Val Total Loss: 0.0587, Val Recon Loss: 0.0575\n",
      "Epoch [26/200], Lambda Entropy: 0.000465, Train Total Loss: 0.0576, Train Recon Loss: 0.0564, Val Total Loss: 0.0587, Val Recon Loss: 0.0576\n",
      "Epoch [27/200], Lambda Entropy: 0.000478, Train Total Loss: 0.0577, Train Recon Loss: 0.0566, Val Total Loss: 0.0587, Val Recon Loss: 0.0577\n",
      "Epoch [28/200], Lambda Entropy: 0.000491, Train Total Loss: 0.0577, Train Recon Loss: 0.0567, Val Total Loss: 0.0589, Val Recon Loss: 0.0579\n",
      "Epoch [29/200], Lambda Entropy: 0.000503, Train Total Loss: 0.0573, Train Recon Loss: 0.0563, Val Total Loss: 0.0583, Val Recon Loss: 0.0573\n",
      "Epoch [30/200], Lambda Entropy: 0.000516, Train Total Loss: 0.0571, Train Recon Loss: 0.0561, Val Total Loss: 0.0589, Val Recon Loss: 0.0579\n",
      "Epoch [31/200], Lambda Entropy: 0.000528, Train Total Loss: 0.0573, Train Recon Loss: 0.0563, Val Total Loss: 0.0586, Val Recon Loss: 0.0577\n",
      "Epoch [32/200], Lambda Entropy: 0.000539, Train Total Loss: 0.0572, Train Recon Loss: 0.0563, Val Total Loss: 0.0589, Val Recon Loss: 0.0580\n",
      "Epoch [33/200], Lambda Entropy: 0.000551, Train Total Loss: 0.0572, Train Recon Loss: 0.0564, Val Total Loss: 0.0583, Val Recon Loss: 0.0575\n",
      "Epoch [34/200], Lambda Entropy: 0.000562, Train Total Loss: 0.0572, Train Recon Loss: 0.0564, Val Total Loss: 0.0585, Val Recon Loss: 0.0576\n",
      "Epoch [35/200], Lambda Entropy: 0.000573, Train Total Loss: 0.0574, Train Recon Loss: 0.0565, Val Total Loss: 0.0585, Val Recon Loss: 0.0576\n",
      "Epoch [36/200], Lambda Entropy: 0.000583, Train Total Loss: 0.0573, Train Recon Loss: 0.0564, Val Total Loss: 0.0583, Val Recon Loss: 0.0574\n",
      "Epoch [37/200], Lambda Entropy: 0.000593, Train Total Loss: 0.0571, Train Recon Loss: 0.0562, Val Total Loss: 0.0581, Val Recon Loss: 0.0573\n",
      "Epoch [38/200], Lambda Entropy: 0.000603, Train Total Loss: 0.0571, Train Recon Loss: 0.0563, Val Total Loss: 0.0585, Val Recon Loss: 0.0578\n",
      "Epoch [39/200], Lambda Entropy: 0.000613, Train Total Loss: 0.0573, Train Recon Loss: 0.0565, Val Total Loss: 0.0585, Val Recon Loss: 0.0577\n",
      "Epoch [40/200], Lambda Entropy: 0.000623, Train Total Loss: 0.0570, Train Recon Loss: 0.0562, Val Total Loss: 0.0586, Val Recon Loss: 0.0579\n",
      "Epoch [41/200], Lambda Entropy: 0.000632, Train Total Loss: 0.0570, Train Recon Loss: 0.0563, Val Total Loss: 0.0581, Val Recon Loss: 0.0573\n",
      "Epoch [42/200], Lambda Entropy: 0.000641, Train Total Loss: 0.0567, Train Recon Loss: 0.0560, Val Total Loss: 0.0582, Val Recon Loss: 0.0574\n",
      "Epoch [43/200], Lambda Entropy: 0.000650, Train Total Loss: 0.0569, Train Recon Loss: 0.0561, Val Total Loss: 0.0582, Val Recon Loss: 0.0575\n",
      "Epoch [44/200], Lambda Entropy: 0.000659, Train Total Loss: 0.0568, Train Recon Loss: 0.0560, Val Total Loss: 0.0585, Val Recon Loss: 0.0578\n",
      "Epoch [45/200], Lambda Entropy: 0.000667, Train Total Loss: 0.0567, Train Recon Loss: 0.0560, Val Total Loss: 0.0578, Val Recon Loss: 0.0570\n",
      "Epoch [46/200], Lambda Entropy: 0.000675, Train Total Loss: 0.0567, Train Recon Loss: 0.0560, Val Total Loss: 0.0579, Val Recon Loss: 0.0572\n",
      "Epoch [47/200], Lambda Entropy: 0.000683, Train Total Loss: 0.0566, Train Recon Loss: 0.0560, Val Total Loss: 0.0580, Val Recon Loss: 0.0573\n",
      "Epoch [48/200], Lambda Entropy: 0.000691, Train Total Loss: 0.0567, Train Recon Loss: 0.0561, Val Total Loss: 0.0580, Val Recon Loss: 0.0573\n",
      "Epoch [49/200], Lambda Entropy: 0.000699, Train Total Loss: 0.0565, Train Recon Loss: 0.0558, Val Total Loss: 0.0579, Val Recon Loss: 0.0572\n",
      "Epoch [50/200], Lambda Entropy: 0.000706, Train Total Loss: 0.0564, Train Recon Loss: 0.0558, Val Total Loss: 0.0581, Val Recon Loss: 0.0575\n",
      "Epoch [51/200], Lambda Entropy: 0.000713, Train Total Loss: 0.0566, Train Recon Loss: 0.0560, Val Total Loss: 0.0580, Val Recon Loss: 0.0573\n",
      "Epoch [52/200], Lambda Entropy: 0.000721, Train Total Loss: 0.0567, Train Recon Loss: 0.0561, Val Total Loss: 0.0586, Val Recon Loss: 0.0579\n",
      "Epoch [53/200], Lambda Entropy: 0.000727, Train Total Loss: 0.0566, Train Recon Loss: 0.0560, Val Total Loss: 0.0581, Val Recon Loss: 0.0575\n",
      "Epoch [54/200], Lambda Entropy: 0.000734, Train Total Loss: 0.0566, Train Recon Loss: 0.0560, Val Total Loss: 0.0581, Val Recon Loss: 0.0574\n",
      "Epoch [55/200], Lambda Entropy: 0.000741, Train Total Loss: 0.0566, Train Recon Loss: 0.0560, Val Total Loss: 0.0579, Val Recon Loss: 0.0573\n",
      "Epoch [56/200], Lambda Entropy: 0.000747, Train Total Loss: 0.0564, Train Recon Loss: 0.0558, Val Total Loss: 0.0579, Val Recon Loss: 0.0573\n",
      "Epoch [57/200], Lambda Entropy: 0.000753, Train Total Loss: 0.0563, Train Recon Loss: 0.0557, Val Total Loss: 0.0580, Val Recon Loss: 0.0574\n",
      "Epoch [58/200], Lambda Entropy: 0.000759, Train Total Loss: 0.0566, Train Recon Loss: 0.0560, Val Total Loss: 0.0580, Val Recon Loss: 0.0574\n",
      "Epoch [59/200], Lambda Entropy: 0.000765, Train Total Loss: 0.0563, Train Recon Loss: 0.0557, Val Total Loss: 0.0581, Val Recon Loss: 0.0575\n",
      "Epoch [60/200], Lambda Entropy: 0.000771, Train Total Loss: 0.0566, Train Recon Loss: 0.0560, Val Total Loss: 0.0578, Val Recon Loss: 0.0572\n",
      "Epoch [61/200], Lambda Entropy: 0.000777, Train Total Loss: 0.0566, Train Recon Loss: 0.0560, Val Total Loss: 0.0578, Val Recon Loss: 0.0573\n",
      "Epoch [62/200], Lambda Entropy: 0.000782, Train Total Loss: 0.0564, Train Recon Loss: 0.0558, Val Total Loss: 0.0581, Val Recon Loss: 0.0575\n",
      "Epoch [63/200], Lambda Entropy: 0.000788, Train Total Loss: 0.0563, Train Recon Loss: 0.0558, Val Total Loss: 0.0581, Val Recon Loss: 0.0575\n",
      "Epoch [64/200], Lambda Entropy: 0.000793, Train Total Loss: 0.0564, Train Recon Loss: 0.0558, Val Total Loss: 0.0579, Val Recon Loss: 0.0573\n",
      "Epoch [65/200], Lambda Entropy: 0.000798, Train Total Loss: 0.0561, Train Recon Loss: 0.0555, Val Total Loss: 0.0578, Val Recon Loss: 0.0573\n",
      "Epoch [66/200], Lambda Entropy: 0.000803, Train Total Loss: 0.0563, Train Recon Loss: 0.0558, Val Total Loss: 0.0587, Val Recon Loss: 0.0582\n",
      "Epoch [67/200], Lambda Entropy: 0.000808, Train Total Loss: 0.0562, Train Recon Loss: 0.0557, Val Total Loss: 0.0579, Val Recon Loss: 0.0573\n",
      "Epoch [68/200], Lambda Entropy: 0.000813, Train Total Loss: 0.0562, Train Recon Loss: 0.0557, Val Total Loss: 0.0576, Val Recon Loss: 0.0571\n",
      "Epoch [69/200], Lambda Entropy: 0.000817, Train Total Loss: 0.0562, Train Recon Loss: 0.0557, Val Total Loss: 0.0576, Val Recon Loss: 0.0571\n",
      "Epoch [70/200], Lambda Entropy: 0.000822, Train Total Loss: 0.0560, Train Recon Loss: 0.0555, Val Total Loss: 0.0574, Val Recon Loss: 0.0569\n",
      "Epoch [71/200], Lambda Entropy: 0.000826, Train Total Loss: 0.0561, Train Recon Loss: 0.0556, Val Total Loss: 0.0576, Val Recon Loss: 0.0571\n",
      "Epoch [72/200], Lambda Entropy: 0.000831, Train Total Loss: 0.0560, Train Recon Loss: 0.0556, Val Total Loss: 0.0576, Val Recon Loss: 0.0571\n",
      "Epoch [73/200], Lambda Entropy: 0.000835, Train Total Loss: 0.0560, Train Recon Loss: 0.0555, Val Total Loss: 0.0579, Val Recon Loss: 0.0574\n",
      "Epoch [74/200], Lambda Entropy: 0.000839, Train Total Loss: 0.0560, Train Recon Loss: 0.0556, Val Total Loss: 0.0575, Val Recon Loss: 0.0570\n",
      "Epoch [75/200], Lambda Entropy: 0.000843, Train Total Loss: 0.0559, Train Recon Loss: 0.0555, Val Total Loss: 0.0574, Val Recon Loss: 0.0569\n",
      "Epoch [76/200], Lambda Entropy: 0.000847, Train Total Loss: 0.0560, Train Recon Loss: 0.0555, Val Total Loss: 0.0577, Val Recon Loss: 0.0572\n",
      "Epoch [77/200], Lambda Entropy: 0.000850, Train Total Loss: 0.0559, Train Recon Loss: 0.0554, Val Total Loss: 0.0582, Val Recon Loss: 0.0577\n",
      "Epoch [78/200], Lambda Entropy: 0.000854, Train Total Loss: 0.0561, Train Recon Loss: 0.0557, Val Total Loss: 0.0581, Val Recon Loss: 0.0576\n",
      "Epoch [79/200], Lambda Entropy: 0.000858, Train Total Loss: 0.0560, Train Recon Loss: 0.0555, Val Total Loss: 0.0576, Val Recon Loss: 0.0571\n",
      "Epoch [80/200], Lambda Entropy: 0.000861, Train Total Loss: 0.0559, Train Recon Loss: 0.0555, Val Total Loss: 0.0575, Val Recon Loss: 0.0570\n",
      "Epoch [81/200], Lambda Entropy: 0.000865, Train Total Loss: 0.0560, Train Recon Loss: 0.0556, Val Total Loss: 0.0576, Val Recon Loss: 0.0572\n",
      "Epoch [82/200], Lambda Entropy: 0.000868, Train Total Loss: 0.0558, Train Recon Loss: 0.0554, Val Total Loss: 0.0577, Val Recon Loss: 0.0573\n",
      "Epoch [83/200], Lambda Entropy: 0.000871, Train Total Loss: 0.0558, Train Recon Loss: 0.0555, Val Total Loss: 0.0572, Val Recon Loss: 0.0568\n",
      "Epoch [84/200], Lambda Entropy: 0.000874, Train Total Loss: 0.0559, Train Recon Loss: 0.0555, Val Total Loss: 0.0575, Val Recon Loss: 0.0570\n",
      "Epoch [85/200], Lambda Entropy: 0.000878, Train Total Loss: 0.0557, Train Recon Loss: 0.0553, Val Total Loss: 0.0578, Val Recon Loss: 0.0574\n",
      "Epoch [86/200], Lambda Entropy: 0.000881, Train Total Loss: 0.0558, Train Recon Loss: 0.0554, Val Total Loss: 0.0578, Val Recon Loss: 0.0574\n",
      "Epoch [87/200], Lambda Entropy: 0.000884, Train Total Loss: 0.0558, Train Recon Loss: 0.0555, Val Total Loss: 0.0574, Val Recon Loss: 0.0570\n",
      "Epoch [88/200], Lambda Entropy: 0.000886, Train Total Loss: 0.0559, Train Recon Loss: 0.0555, Val Total Loss: 0.0571, Val Recon Loss: 0.0568\n",
      "Epoch [89/200], Lambda Entropy: 0.000889, Train Total Loss: 0.0556, Train Recon Loss: 0.0552, Val Total Loss: 0.0578, Val Recon Loss: 0.0573\n",
      "Epoch [90/200], Lambda Entropy: 0.000892, Train Total Loss: 0.0556, Train Recon Loss: 0.0553, Val Total Loss: 0.0572, Val Recon Loss: 0.0568\n",
      "Epoch [91/200], Lambda Entropy: 0.000895, Train Total Loss: 0.0556, Train Recon Loss: 0.0552, Val Total Loss: 0.0575, Val Recon Loss: 0.0571\n",
      "Epoch [92/200], Lambda Entropy: 0.000897, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0572, Val Recon Loss: 0.0568\n",
      "Epoch [93/200], Lambda Entropy: 0.000900, Train Total Loss: 0.0557, Train Recon Loss: 0.0553, Val Total Loss: 0.0576, Val Recon Loss: 0.0571\n",
      "Epoch [94/200], Lambda Entropy: 0.000902, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0579, Val Recon Loss: 0.0574\n",
      "Epoch [95/200], Lambda Entropy: 0.000905, Train Total Loss: 0.0560, Train Recon Loss: 0.0557, Val Total Loss: 0.0572, Val Recon Loss: 0.0567\n",
      "Epoch [96/200], Lambda Entropy: 0.000907, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0578, Val Recon Loss: 0.0574\n",
      "Epoch [97/200], Lambda Entropy: 0.000909, Train Total Loss: 0.0556, Train Recon Loss: 0.0553, Val Total Loss: 0.0572, Val Recon Loss: 0.0568\n",
      "Epoch [98/200], Lambda Entropy: 0.000912, Train Total Loss: 0.0556, Train Recon Loss: 0.0553, Val Total Loss: 0.0575, Val Recon Loss: 0.0571\n",
      "Epoch [99/200], Lambda Entropy: 0.000914, Train Total Loss: 0.0556, Train Recon Loss: 0.0552, Val Total Loss: 0.0575, Val Recon Loss: 0.0572\n",
      "Epoch [100/200], Lambda Entropy: 0.000916, Train Total Loss: 0.0557, Train Recon Loss: 0.0554, Val Total Loss: 0.0573, Val Recon Loss: 0.0569\n",
      "Epoch [101/200], Lambda Entropy: 0.000918, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0579, Val Recon Loss: 0.0575\n",
      "Epoch [102/200], Lambda Entropy: 0.000920, Train Total Loss: 0.0557, Train Recon Loss: 0.0554, Val Total Loss: 0.0573, Val Recon Loss: 0.0570\n",
      "Epoch [103/200], Lambda Entropy: 0.000922, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0569, Val Recon Loss: 0.0567\n",
      "Epoch [104/200], Lambda Entropy: 0.000924, Train Total Loss: 0.0556, Train Recon Loss: 0.0553, Val Total Loss: 0.0574, Val Recon Loss: 0.0570\n",
      "Epoch [105/200], Lambda Entropy: 0.000926, Train Total Loss: 0.0557, Train Recon Loss: 0.0553, Val Total Loss: 0.0570, Val Recon Loss: 0.0566\n",
      "Epoch [106/200], Lambda Entropy: 0.000928, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0568, Val Recon Loss: 0.0565\n",
      "Epoch [107/200], Lambda Entropy: 0.000929, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0572, Val Recon Loss: 0.0569\n",
      "Epoch [108/200], Lambda Entropy: 0.000931, Train Total Loss: 0.0556, Train Recon Loss: 0.0553, Val Total Loss: 0.0569, Val Recon Loss: 0.0566\n",
      "Epoch [109/200], Lambda Entropy: 0.000933, Train Total Loss: 0.0553, Train Recon Loss: 0.0551, Val Total Loss: 0.0572, Val Recon Loss: 0.0569\n",
      "Epoch [110/200], Lambda Entropy: 0.000934, Train Total Loss: 0.0554, Train Recon Loss: 0.0551, Val Total Loss: 0.0575, Val Recon Loss: 0.0571\n",
      "Epoch [111/200], Lambda Entropy: 0.000936, Train Total Loss: 0.0554, Train Recon Loss: 0.0551, Val Total Loss: 0.0569, Val Recon Loss: 0.0566\n",
      "Epoch [112/200], Lambda Entropy: 0.000938, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0572, Val Recon Loss: 0.0568\n",
      "Epoch [113/200], Lambda Entropy: 0.000939, Train Total Loss: 0.0556, Train Recon Loss: 0.0552, Val Total Loss: 0.0572, Val Recon Loss: 0.0569\n",
      "Epoch [114/200], Lambda Entropy: 0.000941, Train Total Loss: 0.0553, Train Recon Loss: 0.0550, Val Total Loss: 0.0576, Val Recon Loss: 0.0572\n",
      "Epoch [115/200], Lambda Entropy: 0.000942, Train Total Loss: 0.0559, Train Recon Loss: 0.0556, Val Total Loss: 0.0571, Val Recon Loss: 0.0568\n",
      "Epoch [116/200], Lambda Entropy: 0.000944, Train Total Loss: 0.0557, Train Recon Loss: 0.0554, Val Total Loss: 0.0570, Val Recon Loss: 0.0567\n",
      "Epoch [117/200], Lambda Entropy: 0.000945, Train Total Loss: 0.0555, Train Recon Loss: 0.0553, Val Total Loss: 0.0570, Val Recon Loss: 0.0567\n",
      "Epoch [118/200], Lambda Entropy: 0.000946, Train Total Loss: 0.0556, Train Recon Loss: 0.0553, Val Total Loss: 0.0574, Val Recon Loss: 0.0571\n",
      "Epoch [119/200], Lambda Entropy: 0.000948, Train Total Loss: 0.0561, Train Recon Loss: 0.0558, Val Total Loss: 0.0576, Val Recon Loss: 0.0573\n",
      "Epoch [120/200], Lambda Entropy: 0.000949, Train Total Loss: 0.0554, Train Recon Loss: 0.0552, Val Total Loss: 0.0570, Val Recon Loss: 0.0567\n",
      "Epoch [121/200], Lambda Entropy: 0.000950, Train Total Loss: 0.0555, Train Recon Loss: 0.0553, Val Total Loss: 0.0572, Val Recon Loss: 0.0570\n",
      "Epoch [122/200], Lambda Entropy: 0.000951, Train Total Loss: 0.0555, Train Recon Loss: 0.0552, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [123/200], Lambda Entropy: 0.000953, Train Total Loss: 0.0552, Train Recon Loss: 0.0550, Val Total Loss: 0.0569, Val Recon Loss: 0.0566\n",
      "Epoch [124/200], Lambda Entropy: 0.000954, Train Total Loss: 0.0553, Train Recon Loss: 0.0550, Val Total Loss: 0.0570, Val Recon Loss: 0.0567\n",
      "Epoch [125/200], Lambda Entropy: 0.000955, Train Total Loss: 0.0554, Train Recon Loss: 0.0551, Val Total Loss: 0.0573, Val Recon Loss: 0.0570\n",
      "Epoch [126/200], Lambda Entropy: 0.000956, Train Total Loss: 0.0555, Train Recon Loss: 0.0553, Val Total Loss: 0.0570, Val Recon Loss: 0.0567\n",
      "Epoch [127/200], Lambda Entropy: 0.000957, Train Total Loss: 0.0552, Train Recon Loss: 0.0550, Val Total Loss: 0.0575, Val Recon Loss: 0.0572\n",
      "Epoch [128/200], Lambda Entropy: 0.000958, Train Total Loss: 0.0551, Train Recon Loss: 0.0549, Val Total Loss: 0.0568, Val Recon Loss: 0.0566\n",
      "Epoch [129/200], Lambda Entropy: 0.000959, Train Total Loss: 0.0551, Train Recon Loss: 0.0549, Val Total Loss: 0.0569, Val Recon Loss: 0.0568\n",
      "Epoch [130/200], Lambda Entropy: 0.000960, Train Total Loss: 0.0553, Train Recon Loss: 0.0551, Val Total Loss: 0.0566, Val Recon Loss: 0.0564\n",
      "Epoch [131/200], Lambda Entropy: 0.000961, Train Total Loss: 0.0551, Train Recon Loss: 0.0549, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [132/200], Lambda Entropy: 0.000962, Train Total Loss: 0.0551, Train Recon Loss: 0.0549, Val Total Loss: 0.0572, Val Recon Loss: 0.0570\n",
      "Epoch [133/200], Lambda Entropy: 0.000963, Train Total Loss: 0.0549, Train Recon Loss: 0.0547, Val Total Loss: 0.0568, Val Recon Loss: 0.0565\n",
      "Epoch [134/200], Lambda Entropy: 0.000964, Train Total Loss: 0.0550, Train Recon Loss: 0.0548, Val Total Loss: 0.0571, Val Recon Loss: 0.0568\n",
      "Epoch [135/200], Lambda Entropy: 0.000965, Train Total Loss: 0.0550, Train Recon Loss: 0.0548, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [136/200], Lambda Entropy: 0.000966, Train Total Loss: 0.0551, Train Recon Loss: 0.0548, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [137/200], Lambda Entropy: 0.000967, Train Total Loss: 0.0549, Train Recon Loss: 0.0547, Val Total Loss: 0.0576, Val Recon Loss: 0.0573\n",
      "Epoch [138/200], Lambda Entropy: 0.000967, Train Total Loss: 0.0552, Train Recon Loss: 0.0550, Val Total Loss: 0.0566, Val Recon Loss: 0.0563\n",
      "Epoch [139/200], Lambda Entropy: 0.000968, Train Total Loss: 0.0550, Train Recon Loss: 0.0548, Val Total Loss: 0.0569, Val Recon Loss: 0.0567\n",
      "Epoch [140/200], Lambda Entropy: 0.000969, Train Total Loss: 0.0548, Train Recon Loss: 0.0546, Val Total Loss: 0.0568, Val Recon Loss: 0.0565\n",
      "Epoch [141/200], Lambda Entropy: 0.000970, Train Total Loss: 0.0549, Train Recon Loss: 0.0547, Val Total Loss: 0.0572, Val Recon Loss: 0.0569\n",
      "Epoch [142/200], Lambda Entropy: 0.000971, Train Total Loss: 0.0552, Train Recon Loss: 0.0550, Val Total Loss: 0.0571, Val Recon Loss: 0.0568\n",
      "Epoch [143/200], Lambda Entropy: 0.000971, Train Total Loss: 0.0552, Train Recon Loss: 0.0550, Val Total Loss: 0.0571, Val Recon Loss: 0.0568\n",
      "Epoch [144/200], Lambda Entropy: 0.000972, Train Total Loss: 0.0549, Train Recon Loss: 0.0547, Val Total Loss: 0.0575, Val Recon Loss: 0.0572\n",
      "Epoch [145/200], Lambda Entropy: 0.000973, Train Total Loss: 0.0553, Train Recon Loss: 0.0551, Val Total Loss: 0.0568, Val Recon Loss: 0.0566\n",
      "Epoch [146/200], Lambda Entropy: 0.000973, Train Total Loss: 0.0549, Train Recon Loss: 0.0547, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [147/200], Lambda Entropy: 0.000974, Train Total Loss: 0.0550, Train Recon Loss: 0.0548, Val Total Loss: 0.0573, Val Recon Loss: 0.0571\n",
      "Epoch [148/200], Lambda Entropy: 0.000975, Train Total Loss: 0.0551, Train Recon Loss: 0.0549, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [149/200], Lambda Entropy: 0.000975, Train Total Loss: 0.0548, Train Recon Loss: 0.0546, Val Total Loss: 0.0569, Val Recon Loss: 0.0567\n",
      "Epoch [150/200], Lambda Entropy: 0.000976, Train Total Loss: 0.0548, Train Recon Loss: 0.0546, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [151/200], Lambda Entropy: 0.000976, Train Total Loss: 0.0548, Train Recon Loss: 0.0546, Val Total Loss: 0.0569, Val Recon Loss: 0.0566\n",
      "Epoch [152/200], Lambda Entropy: 0.000977, Train Total Loss: 0.0549, Train Recon Loss: 0.0547, Val Total Loss: 0.0570, Val Recon Loss: 0.0567\n",
      "Epoch [153/200], Lambda Entropy: 0.000978, Train Total Loss: 0.0547, Train Recon Loss: 0.0545, Val Total Loss: 0.0562, Val Recon Loss: 0.0560\n",
      "Epoch [154/200], Lambda Entropy: 0.000978, Train Total Loss: 0.0546, Train Recon Loss: 0.0545, Val Total Loss: 0.0567, Val Recon Loss: 0.0564\n",
      "Epoch [155/200], Lambda Entropy: 0.000979, Train Total Loss: 0.0546, Train Recon Loss: 0.0544, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [156/200], Lambda Entropy: 0.000979, Train Total Loss: 0.0549, Train Recon Loss: 0.0547, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [157/200], Lambda Entropy: 0.000980, Train Total Loss: 0.0551, Train Recon Loss: 0.0549, Val Total Loss: 0.0565, Val Recon Loss: 0.0563\n",
      "Epoch [158/200], Lambda Entropy: 0.000980, Train Total Loss: 0.0548, Train Recon Loss: 0.0546, Val Total Loss: 0.0572, Val Recon Loss: 0.0570\n",
      "Epoch [159/200], Lambda Entropy: 0.000981, Train Total Loss: 0.0551, Train Recon Loss: 0.0549, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [160/200], Lambda Entropy: 0.000981, Train Total Loss: 0.0547, Train Recon Loss: 0.0545, Val Total Loss: 0.0567, Val Recon Loss: 0.0564\n",
      "Epoch [161/200], Lambda Entropy: 0.000982, Train Total Loss: 0.0546, Train Recon Loss: 0.0544, Val Total Loss: 0.0568, Val Recon Loss: 0.0565\n",
      "Epoch [162/200], Lambda Entropy: 0.000982, Train Total Loss: 0.0552, Train Recon Loss: 0.0550, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [163/200], Lambda Entropy: 0.000983, Train Total Loss: 0.0547, Train Recon Loss: 0.0545, Val Total Loss: 0.0569, Val Recon Loss: 0.0567\n",
      "Epoch [164/200], Lambda Entropy: 0.000983, Train Total Loss: 0.0549, Train Recon Loss: 0.0547, Val Total Loss: 0.0567, Val Recon Loss: 0.0564\n",
      "Epoch [165/200], Lambda Entropy: 0.000983, Train Total Loss: 0.0547, Train Recon Loss: 0.0545, Val Total Loss: 0.0566, Val Recon Loss: 0.0564\n",
      "Epoch [166/200], Lambda Entropy: 0.000984, Train Total Loss: 0.0547, Train Recon Loss: 0.0545, Val Total Loss: 0.0568, Val Recon Loss: 0.0566\n",
      "Epoch [167/200], Lambda Entropy: 0.000984, Train Total Loss: 0.0545, Train Recon Loss: 0.0543, Val Total Loss: 0.0569, Val Recon Loss: 0.0567\n",
      "Epoch [168/200], Lambda Entropy: 0.000985, Train Total Loss: 0.0548, Train Recon Loss: 0.0546, Val Total Loss: 0.0566, Val Recon Loss: 0.0563\n",
      "Epoch [169/200], Lambda Entropy: 0.000985, Train Total Loss: 0.0545, Train Recon Loss: 0.0543, Val Total Loss: 0.0571, Val Recon Loss: 0.0569\n",
      "Epoch [170/200], Lambda Entropy: 0.000985, Train Total Loss: 0.0546, Train Recon Loss: 0.0544, Val Total Loss: 0.0564, Val Recon Loss: 0.0561\n",
      "Epoch [171/200], Lambda Entropy: 0.000986, Train Total Loss: 0.0546, Train Recon Loss: 0.0544, Val Total Loss: 0.0564, Val Recon Loss: 0.0562\n",
      "Epoch [172/200], Lambda Entropy: 0.000986, Train Total Loss: 0.0547, Train Recon Loss: 0.0545, Val Total Loss: 0.0570, Val Recon Loss: 0.0567\n",
      "Epoch [173/200], Lambda Entropy: 0.000986, Train Total Loss: 0.0546, Train Recon Loss: 0.0544, Val Total Loss: 0.0568, Val Recon Loss: 0.0566\n",
      "Epoch [174/200], Lambda Entropy: 0.000987, Train Total Loss: 0.0545, Train Recon Loss: 0.0543, Val Total Loss: 0.0566, Val Recon Loss: 0.0565\n",
      "Epoch [175/200], Lambda Entropy: 0.000987, Train Total Loss: 0.0547, Train Recon Loss: 0.0545, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [176/200], Lambda Entropy: 0.000987, Train Total Loss: 0.0544, Train Recon Loss: 0.0542, Val Total Loss: 0.0564, Val Recon Loss: 0.0562\n",
      "Epoch [177/200], Lambda Entropy: 0.000988, Train Total Loss: 0.0543, Train Recon Loss: 0.0542, Val Total Loss: 0.0563, Val Recon Loss: 0.0561\n",
      "Epoch [178/200], Lambda Entropy: 0.000988, Train Total Loss: 0.0545, Train Recon Loss: 0.0543, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [179/200], Lambda Entropy: 0.000988, Train Total Loss: 0.0544, Train Recon Loss: 0.0542, Val Total Loss: 0.0565, Val Recon Loss: 0.0563\n",
      "Epoch [180/200], Lambda Entropy: 0.000989, Train Total Loss: 0.0544, Train Recon Loss: 0.0542, Val Total Loss: 0.0568, Val Recon Loss: 0.0566\n",
      "Epoch [181/200], Lambda Entropy: 0.000989, Train Total Loss: 0.0546, Train Recon Loss: 0.0544, Val Total Loss: 0.0565, Val Recon Loss: 0.0563\n",
      "Epoch [182/200], Lambda Entropy: 0.000989, Train Total Loss: 0.0543, Train Recon Loss: 0.0542, Val Total Loss: 0.0565, Val Recon Loss: 0.0562\n",
      "Epoch [183/200], Lambda Entropy: 0.000989, Train Total Loss: 0.0542, Train Recon Loss: 0.0541, Val Total Loss: 0.0566, Val Recon Loss: 0.0564\n",
      "Epoch [184/200], Lambda Entropy: 0.000990, Train Total Loss: 0.0544, Train Recon Loss: 0.0542, Val Total Loss: 0.0566, Val Recon Loss: 0.0564\n",
      "Epoch [185/200], Lambda Entropy: 0.000990, Train Total Loss: 0.0543, Train Recon Loss: 0.0541, Val Total Loss: 0.0568, Val Recon Loss: 0.0565\n",
      "Epoch [186/200], Lambda Entropy: 0.000990, Train Total Loss: 0.0543, Train Recon Loss: 0.0541, Val Total Loss: 0.0565, Val Recon Loss: 0.0563\n",
      "Epoch [187/200], Lambda Entropy: 0.000990, Train Total Loss: 0.0541, Train Recon Loss: 0.0540, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [188/200], Lambda Entropy: 0.000991, Train Total Loss: 0.0544, Train Recon Loss: 0.0542, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [189/200], Lambda Entropy: 0.000991, Train Total Loss: 0.0542, Train Recon Loss: 0.0540, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [190/200], Lambda Entropy: 0.000991, Train Total Loss: 0.0540, Train Recon Loss: 0.0539, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [191/200], Lambda Entropy: 0.000991, Train Total Loss: 0.0544, Train Recon Loss: 0.0543, Val Total Loss: 0.0570, Val Recon Loss: 0.0568\n",
      "Epoch [192/200], Lambda Entropy: 0.000992, Train Total Loss: 0.0547, Train Recon Loss: 0.0545, Val Total Loss: 0.0564, Val Recon Loss: 0.0562\n",
      "Epoch [193/200], Lambda Entropy: 0.000992, Train Total Loss: 0.0544, Train Recon Loss: 0.0542, Val Total Loss: 0.0566, Val Recon Loss: 0.0564\n",
      "Epoch [194/200], Lambda Entropy: 0.000992, Train Total Loss: 0.0544, Train Recon Loss: 0.0542, Val Total Loss: 0.0572, Val Recon Loss: 0.0570\n",
      "Epoch [195/200], Lambda Entropy: 0.000992, Train Total Loss: 0.0548, Train Recon Loss: 0.0546, Val Total Loss: 0.0568, Val Recon Loss: 0.0566\n",
      "Epoch [196/200], Lambda Entropy: 0.000992, Train Total Loss: 0.0546, Train Recon Loss: 0.0544, Val Total Loss: 0.0566, Val Recon Loss: 0.0564\n",
      "Epoch [197/200], Lambda Entropy: 0.000993, Train Total Loss: 0.0541, Train Recon Loss: 0.0539, Val Total Loss: 0.0565, Val Recon Loss: 0.0563\n",
      "Epoch [198/200], Lambda Entropy: 0.000993, Train Total Loss: 0.0546, Train Recon Loss: 0.0544, Val Total Loss: 0.0568, Val Recon Loss: 0.0566\n",
      "Epoch [199/200], Lambda Entropy: 0.000993, Train Total Loss: 0.0543, Train Recon Loss: 0.0541, Val Total Loss: 0.0567, Val Recon Loss: 0.0565\n",
      "Epoch [200/200], Lambda Entropy: 0.000993, Train Total Loss: 0.0542, Train Recon Loss: 0.0540, Val Total Loss: 0.0564, Val Recon Loss: 0.0562\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume that the Encoder, Decoder, and Model classes are already defined\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 50        # Number of observed variables\n",
    "output_dim = 5        # Output dimension of the encoder (dimension of Z)\n",
    "embedding_dim = 64    # Embedding dimension for the embeddings e and e_i's\n",
    "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
    "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 200          # Number of epochs\n",
    "batch_size = 32          # Batch size (already set in the DataLoader)\n",
    "print_every = 1          # How often to print loss (in epochs)\n",
    "\n",
    "# Define the maximum value for the entropy regularization coefficient\n",
    "max_lambda_entropy = 1e-3  # Adjust this value as needed\n",
    "\n",
    "# Flag to enable or disable entropy regularizer\n",
    "use_entropy_regularizer = True  # Set to True to enable, False to disable\n",
    "\n",
    "# Scheduler function for lambda_entropy\n",
    "def get_lambda_entropy(epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=True):\n",
    "    if not use_entropy_regularizer:\n",
    "        return 0.0\n",
    "    if schedule_type == 'linear':\n",
    "        # Linear increase from 0 to max_lambda_entropy\n",
    "        return max_lambda_entropy * (epoch / num_epochs)\n",
    "    elif schedule_type == 'exponential':\n",
    "        # Exponential increase from 0 to max_lambda_entropy\n",
    "        k = 5  # Adjust this value to control the speed of increase\n",
    "        return max_lambda_entropy * (1 - math.exp(-k * epoch / num_epochs))\n",
    "    elif schedule_type == 'logarithmic':\n",
    "        # Logarithmic increase from 0 to max_lambda_entropy\n",
    "        if epoch == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return max_lambda_entropy * math.log(epoch + 1) / math.log(num_epochs + 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
    "\n",
    "model_path = \"trained_model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Trained model found. Loading the model.\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute lambda_entropy for the current epoch\n",
    "    lambda_entropy = get_lambda_entropy(\n",
    "        epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=use_entropy_regularizer)\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0          # Accumulates total loss (reconstruction + regularizer)\n",
    "    running_recon_loss = 0.0    # Accumulates reconstruction loss\n",
    "    for batch_idx, (batch,) in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute predicted x_hat and attention weights\n",
    "        x_hat, attn_weights = model(batch)\n",
    "\n",
    "        # Compute the reconstruction loss\n",
    "        recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "        # Initialize entropy_regularizer to zero\n",
    "        entropy_regularizer = 0.0\n",
    "\n",
    "        # Compute the entropy regularizer if enabled\n",
    "        if use_entropy_regularizer:\n",
    "            attn_weights = attn_weights.squeeze(1)  # Shape: (batch_size, input_dim, output_dim)\n",
    "\n",
    "            # Add a small epsilon to prevent log(0)\n",
    "            epsilon = 1e-8\n",
    "\n",
    "            # Compute entropy for each query (input_dim)\n",
    "            entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "            # Sum entropies over queries and average over batch\n",
    "            entropy_regularizer = torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
    "\n",
    "        # Total loss\n",
    "        loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training losses\n",
    "        running_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "\n",
    "    # Compute average losses for training\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_train_recon_loss = running_recon_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0          # Accumulates total loss (reconstruction + regularizer)\n",
    "    val_recon_loss = 0.0    # Accumulates reconstruction loss\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch,) in enumerate(val_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            x_hat, attn_weights = model(batch)\n",
    "\n",
    "            # Compute the reconstruction loss\n",
    "            recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "            # Initialize entropy_regularizer to zero\n",
    "            entropy_regularizer = 0.0\n",
    "\n",
    "            # Compute the entropy regularizer if enabled\n",
    "            if use_entropy_regularizer:\n",
    "                attn_weights = attn_weights.squeeze(1)  # Shape: (batch_size, input_dim, output_dim)\n",
    "\n",
    "                # Add a small epsilon to prevent log(0)\n",
    "                epsilon = 1e-8\n",
    "\n",
    "                # Compute entropy for each query (input_dim)\n",
    "                entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "                # Sum entropies over queries and average over batch\n",
    "                entropy_regularizer = torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "            # Accumulate validation losses\n",
    "            val_loss += loss.item()\n",
    "            val_recon_loss += recon_loss.item()\n",
    "\n",
    "    # Compute average losses for validation\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_recon_loss = val_recon_loss / len(val_loader)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Lambda Entropy: {lambda_entropy:.6f}, '\n",
    "              f'Train Total Loss: {avg_train_loss:.4f}, Train Recon Loss: {avg_train_recon_loss:.4f}, '\n",
    "              f'Val Total Loss: {avg_val_loss:.4f}, Val Recon Loss: {avg_val_recon_loss:.4f}')\n",
    "\n",
    "# Save the trained model after training\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoBeOpJnYZhC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute average attention matrix\n",
    "def compute_average_attention(model, dataloader, device):\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "    total_attn = None\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            # Assuming data is a tuple (inputs, labels), adjust as needed\n",
    "            inputs = data[0].to(device)\n",
    "\n",
    "            # Forward pass up to obtaining attention weights\n",
    "            hat_Z = model.encoder(inputs)\n",
    "            _, attn_weights = model.decoder(hat_Z)\n",
    "\n",
    "            # attn_weights shape: (batch_size, input_dim, output_dim)\n",
    "            # Sum attention weights over batches\n",
    "            if total_attn is None:\n",
    "                total_attn = attn_weights.sum(dim=0)  # Sum over batch dimension\n",
    "            else:\n",
    "                total_attn += attn_weights.sum(dim=0)\n",
    "            num_batches += attn_weights.size(0)  # Accumulate total number of samples\n",
    "\n",
    "    # Average the attention weights\n",
    "    avg_attn = total_attn / num_batches\n",
    "\n",
    "    return avg_attn.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your dataloaders defined as train_loader and val_loader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Compute average attention matrices\n",
    "avg_attn_train = compute_average_attention(model, train_loader, device)\n",
    "avg_attn_val = compute_average_attention(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your original dataframe\n",
    "\n",
    "# Identify column prefixes for each true factor\n",
    "factor_columns = {\n",
    "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
    "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
    "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
    "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
    "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
    "}\n",
    "\n",
    "# Map factor names to column indices\n",
    "factor_indices = {}\n",
    "for factor_name, columns in factor_columns.items():\n",
    "    indices = [df.columns.get_loc(col) for col in columns]\n",
    "    factor_indices[factor_name] = indices\n",
    "\n",
    "# Create a new ordering of indices\n",
    "new_order = []\n",
    "for factor_name in factor_columns.keys():\n",
    "    new_order.extend(factor_indices[factor_name])\n",
    "\n",
    "# Ensure all indices are included\n",
    "assert len(new_order) == df.shape[1], \"Not all indices are included in the new order.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the average attention matrices\n",
    "avg_attn_train_transposed = avg_attn_train.T  # Shape: (output_dim, input_dim)\n",
    "avg_attn_val_transposed = avg_attn_val.T      # Shape: (output_dim, input_dim)\n",
    "\n",
    "# Rearrange the columns (queries) using new_order\n",
    "avg_attn_train_reordered = avg_attn_train_transposed[:, new_order]\n",
    "avg_attn_val_reordered = avg_attn_val_transposed[:, new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attention_heatmap(attn_matrix, title):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(attn_matrix, cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Query Elements')  # Queries are now along X-axis\n",
    "    plt.ylabel('Key Elements')    # Keys are now along Y-axis\n",
    "    plt.show()\n",
    "\n",
    "# Plot average attention matrices\n",
    "plot_attention_heatmap(avg_attn_train_reordered, 'Average Attention Matrix - Training Set')\n",
    "plot_attention_heatmap(avg_attn_val_reordered, 'Average Attention Matrix - Validation Set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
