{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_HACIxMBlMv",
    "outputId": "a266a5c4-b111-46e8-b71d-77b930d36ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 2000\n",
      "Number of columns in the dataset: 50\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
      "0        2       0         3          1   4        1       4         1   \n",
      "1        2       0         1          2   2        1       4         3   \n",
      "2        3       0         2          1   3        1       4         3   \n",
      "3        2       0         1          1   1        0       4         1   \n",
      "4        2       0         1          1   3        0       4         3   \n",
      "\n",
      "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
      "0          2   2  ...         3        3          3           2    3   \n",
      "1          3   1  ...         2        3          2           2    3   \n",
      "2          3   0  ...         4        4          2           1    4   \n",
      "3          3   1  ...         1        2          2           1    3   \n",
      "4          2   0  ...         3        4          1           3    4   \n",
      "\n",
      "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
      "0         1        4          4           2    4  \n",
      "1         1        3          2           2    3  \n",
      "2         2        4          2           0    4  \n",
      "3         1        3          2           1    2  \n",
      "4         1        3          1           3    4  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Data Types and Non-Null Counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 50 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   blue_q0     2000 non-null   int64\n",
      " 1   red_q1      2000 non-null   int64\n",
      " 2   green_q2    2000 non-null   int64\n",
      " 3   purple_q3   2000 non-null   int64\n",
      " 4   q4          2000 non-null   int64\n",
      " 5   blue_q5     2000 non-null   int64\n",
      " 6   red_q6      2000 non-null   int64\n",
      " 7   green_q7    2000 non-null   int64\n",
      " 8   purple_q8   2000 non-null   int64\n",
      " 9   q9          2000 non-null   int64\n",
      " 10  blue_q10    2000 non-null   int64\n",
      " 11  red_q11     2000 non-null   int64\n",
      " 12  green_q12   2000 non-null   int64\n",
      " 13  purple_q13  2000 non-null   int64\n",
      " 14  q14         2000 non-null   int64\n",
      " 15  blue_q15    2000 non-null   int64\n",
      " 16  red_q16     2000 non-null   int64\n",
      " 17  green_q17   2000 non-null   int64\n",
      " 18  purple_q18  2000 non-null   int64\n",
      " 19  q19         2000 non-null   int64\n",
      " 20  blue_q20    2000 non-null   int64\n",
      " 21  red_q21     2000 non-null   int64\n",
      " 22  green_q22   2000 non-null   int64\n",
      " 23  purple_q23  2000 non-null   int64\n",
      " 24  q24         2000 non-null   int64\n",
      " 25  blue_q25    2000 non-null   int64\n",
      " 26  red_q26     2000 non-null   int64\n",
      " 27  green_q27   2000 non-null   int64\n",
      " 28  purple_q28  2000 non-null   int64\n",
      " 29  q29         2000 non-null   int64\n",
      " 30  blue_q30    2000 non-null   int64\n",
      " 31  red_q31     2000 non-null   int64\n",
      " 32  green_q32   2000 non-null   int64\n",
      " 33  purple_q33  2000 non-null   int64\n",
      " 34  q34         2000 non-null   int64\n",
      " 35  blue_q35    2000 non-null   int64\n",
      " 36  red_q36     2000 non-null   int64\n",
      " 37  green_q37   2000 non-null   int64\n",
      " 38  purple_q38  2000 non-null   int64\n",
      " 39  q39         2000 non-null   int64\n",
      " 40  blue_q40    2000 non-null   int64\n",
      " 41  red_q41     2000 non-null   int64\n",
      " 42  green_q42   2000 non-null   int64\n",
      " 43  purple_q43  2000 non-null   int64\n",
      " 44  q44         2000 non-null   int64\n",
      " 45  blue_q45    2000 non-null   int64\n",
      " 46  red_q46     2000 non-null   int64\n",
      " 47  green_q47   2000 non-null   int64\n",
      " 48  purple_q48  2000 non-null   int64\n",
      " 49  q49         2000 non-null   int64\n",
      "dtypes: int64(50)\n",
      "memory usage: 781.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of instances in the dataset:\", df.shape[0])\n",
    "print(\"Number of columns in the dataset:\", df.shape[1])\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display additional information\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81r2mDP3BzZ-",
    "outputId": "8faa8967-c63d-4230-c155-42b29b34e6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Factor1  Factor2  Factor3  Factor4  Factor5\n",
      "0       20       21       21       22       28\n",
      "1       21       20       21       26       23\n",
      "2       23       20       17       22       25\n",
      "3       17       15       11       22       15\n",
      "4       20       14       24       23       24\n"
     ]
    }
   ],
   "source": [
    "# Identify column prefixes for each true factor\n",
    "factor_columns = {\n",
    "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
    "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
    "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
    "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
    "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
    "}\n",
    "\n",
    "# Calculate true factors by summing the respective columns\n",
    "true_factors = pd.DataFrame()\n",
    "for factor_name, columns in factor_columns.items():\n",
    "    true_factors[factor_name] = df[columns].sum(axis=1)\n",
    "\n",
    "# Display the first few rows of the calculated true factors\n",
    "print(true_factors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yaO1N84CB7_U"
   },
   "outputs": [],
   "source": [
    "df = df / 4.0\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /root/miniconda3/envs/parthaenv/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /root/miniconda3/envs/parthaenv/lib/python3.10/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /root/miniconda3/envs/parthaenv/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /root/miniconda3/envs/parthaenv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/miniconda3/envs/parthaenv/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1600, 50)\n",
      "Test set shape: (400, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target (Y) if necessary.\n",
    "# In the case of autoencoder-like models, we do not have target Y, so we'll treat the whole dataset as X.\n",
    "X = df.values  # Convert the DataFrame into a NumPy array for model input\n",
    "\n",
    "# Split the dataset into training (80%) and testing sets (20%)\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shapes to verify\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA MSE on training set: 0.047004582672498525\n",
      "PCA MSE on test set: 0.04556389134945836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming X_train and X_test are your training and testing data from the Big Five dataset\n",
    "\n",
    "# Define the PCA model, with the number of components matching the desired dimension (same as your autoencoder output dimension)\n",
    "n_components = 5  # Set this based on your autoencoder's latent space dimensionality\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA on the training data\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# Reconstruct the data from the reduced components\n",
    "X_train_reconstructed = pca.inverse_transform(X_train_reduced)\n",
    "\n",
    "# Calculate the MSE on the training set\n",
    "mse_train_pca = mean_squared_error(X_train, X_train_reconstructed)\n",
    "print(f\"PCA MSE on training set: {mse_train_pca}\")\n",
    "\n",
    "# Do the same for the test data\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "X_test_reconstructed = pca.inverse_transform(X_test_reduced)\n",
    "mse_test_pca = mean_squared_error(X_test, X_test_reconstructed)\n",
    "print(f\"PCA MSE on test set: {mse_test_pca}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FA MSE on training set: 0.048124038024241804\n",
      "FA MSE on test set: 0.04686598878562906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the FA model, with the number of components matching the dimensionality of your autoencoder's latent space\n",
    "n_components = 5  # Use your autoencoder's latent dimension here\n",
    "fa = FactorAnalysis(n_components=n_components)\n",
    "\n",
    "# Fit FA on the training data\n",
    "X_train_reduced_fa = fa.fit_transform(X_train)\n",
    "\n",
    "# Manually reconstruct the data using the latent factors and the factor loadings\n",
    "X_train_reconstructed_fa = np.dot(X_train_reduced_fa, fa.components_) + np.mean(X_train, axis=0)\n",
    "\n",
    "# Calculate the MSE on the training set\n",
    "mse_train_fa = mean_squared_error(X_train, X_train_reconstructed_fa)\n",
    "print(f\"FA MSE on training set: {mse_train_fa}\")\n",
    "\n",
    "# Do the same for the test data\n",
    "X_test_reduced_fa = fa.transform(X_test)\n",
    "X_test_reconstructed_fa = np.dot(X_test_reduced_fa, fa.components_) + np.mean(X_train, axis=0)  # Use train mean for consistent reconstruction\n",
    "\n",
    "mse_test_fa = mean_squared_error(X_test, X_test_reconstructed_fa)\n",
    "print(f\"FA MSE on test set: {mse_test_fa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jA29S2CJQV",
    "outputId": "97c14234-90fb-43e0-944e-38113a3f8b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([2000, 50])\n",
      "Number of training batches: 50\n",
      "Number of validation batches: 13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "data_array = df.to_numpy()\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for the training and validation sets\n",
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display the shape of the tensor to verify\n",
    "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rywEvDazCYkL"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define the layers of the MLP\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # Learnable embedding vector e (moved from Decoder to Encoder)\n",
    "        self.e = nn.Parameter(torch.randn(embedding_dim))\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the MLP to get Z\n",
    "        Z = self.mlp(x)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Convert Z to \\hat Z by multiplying each scalar z_i with embedding vector e\n",
    "        batch_size = Z.size(0)\n",
    "        output_dim = Z.size(1)\n",
    "        e_expanded = self.e.unsqueeze(0).unsqueeze(0)      # Shape: (1, 1, embedding_dim)\n",
    "        Z_expanded = Z.unsqueeze(2)                        # Shape: (batch_size, output_dim, 1)\n",
    "        hat_Z = Z_expanded * e_expanded                    # Shape: (batch_size, output_dim, embedding_dim)\n",
    "\n",
    "        return hat_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tUXzn0AHHq1T"
   },
   "outputs": [],
   "source": [
    "# Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim      # Number of observed variables (n)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Learnable query embeddings (e1, e2, ..., en)\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
    "\n",
    "        # MultiheadAttention module with 1 head\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # MLP to predict x_i's from embeddings\n",
    "        dims = [embedding_dim] + hidden_dims + [1]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, hat_Z):\n",
    "        \"\"\"\n",
    "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = hat_Z.size(0)\n",
    "\n",
    "        # Prepare query embeddings and expand to batch size\n",
    "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)        # Output shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Add residual connection and apply layer normalization\n",
    "        out = self.layer_norm(attn_output + query_embeddings)                             # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Flatten the embeddings and pass through MLP to predict x_i's\n",
    "        out_flat = out.reshape(-1, self.embedding_dim)                                    # Shape: (batch_size * input_dim, embedding_dim)\n",
    "        x_hat_flat = self.mlp(out_flat)                                                   # Shape: (batch_size * input_dim, 1)\n",
    "        x_hat = x_hat_flat.view(batch_size, self.input_dim)                               # Shape: (batch_size, input_dim)\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xnBqmgVjIat0"
   },
   "outputs": [],
   "source": [
    "# Complete model combining the encoder and decoder\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(input_dim=input_dim, output_dim=output_dim, embedding_dim=embedding_dim, hidden_dims=encoder_hidden_dims)\n",
    "        self.decoder = Decoder(input_dim=input_dim, embedding_dim=embedding_dim, hidden_dims=decoder_hidden_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hat_Z = self.encoder(x)     # Obtain \\hat Z from the encoder\n",
    "        x_hat = self.decoder(hat_Z) # Reconstruct x from \\hat Z using the decoder\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ki1licN-PTun",
    "outputId": "fb864687-79d0-4763-ec7f-18a71da1ed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Train Loss: 0.1138, Val Loss: 0.0913\n",
      "Epoch [2/400], Train Loss: 0.0865, Val Loss: 0.0870\n",
      "Epoch [3/400], Train Loss: 0.0811, Val Loss: 0.0814\n",
      "Epoch [4/400], Train Loss: 0.0766, Val Loss: 0.0774\n",
      "Epoch [5/400], Train Loss: 0.0737, Val Loss: 0.0747\n",
      "Epoch [6/400], Train Loss: 0.0716, Val Loss: 0.0726\n",
      "Epoch [7/400], Train Loss: 0.0698, Val Loss: 0.0711\n",
      "Epoch [8/400], Train Loss: 0.0683, Val Loss: 0.0703\n",
      "Epoch [9/400], Train Loss: 0.0677, Val Loss: 0.0698\n",
      "Epoch [10/400], Train Loss: 0.0671, Val Loss: 0.0693\n",
      "Epoch [11/400], Train Loss: 0.0667, Val Loss: 0.0689\n",
      "Epoch [12/400], Train Loss: 0.0665, Val Loss: 0.0686\n",
      "Epoch [13/400], Train Loss: 0.0660, Val Loss: 0.0685\n",
      "Epoch [14/400], Train Loss: 0.0658, Val Loss: 0.0683\n",
      "Epoch [15/400], Train Loss: 0.0658, Val Loss: 0.0682\n",
      "Epoch [16/400], Train Loss: 0.0658, Val Loss: 0.0683\n",
      "Epoch [17/400], Train Loss: 0.0655, Val Loss: 0.0680\n",
      "Epoch [18/400], Train Loss: 0.0654, Val Loss: 0.0683\n",
      "Epoch [19/400], Train Loss: 0.0654, Val Loss: 0.0677\n",
      "Epoch [20/400], Train Loss: 0.0650, Val Loss: 0.0677\n",
      "Epoch [21/400], Train Loss: 0.0649, Val Loss: 0.0678\n",
      "Epoch [22/400], Train Loss: 0.0648, Val Loss: 0.0674\n",
      "Epoch [23/400], Train Loss: 0.0644, Val Loss: 0.0671\n",
      "Epoch [24/400], Train Loss: 0.0644, Val Loss: 0.0671\n",
      "Epoch [25/400], Train Loss: 0.0642, Val Loss: 0.0664\n",
      "Epoch [26/400], Train Loss: 0.0640, Val Loss: 0.0666\n",
      "Epoch [27/400], Train Loss: 0.0639, Val Loss: 0.0662\n",
      "Epoch [28/400], Train Loss: 0.0637, Val Loss: 0.0663\n",
      "Epoch [29/400], Train Loss: 0.0634, Val Loss: 0.0666\n",
      "Epoch [30/400], Train Loss: 0.0633, Val Loss: 0.0664\n",
      "Epoch [31/400], Train Loss: 0.0632, Val Loss: 0.0657\n",
      "Epoch [32/400], Train Loss: 0.0628, Val Loss: 0.0656\n",
      "Epoch [33/400], Train Loss: 0.0626, Val Loss: 0.0655\n",
      "Epoch [34/400], Train Loss: 0.0624, Val Loss: 0.0656\n",
      "Epoch [35/400], Train Loss: 0.0625, Val Loss: 0.0653\n",
      "Epoch [36/400], Train Loss: 0.0621, Val Loss: 0.0665\n",
      "Epoch [37/400], Train Loss: 0.0622, Val Loss: 0.0652\n",
      "Epoch [38/400], Train Loss: 0.0618, Val Loss: 0.0649\n",
      "Epoch [39/400], Train Loss: 0.0618, Val Loss: 0.0653\n",
      "Epoch [40/400], Train Loss: 0.0615, Val Loss: 0.0648\n",
      "Epoch [41/400], Train Loss: 0.0615, Val Loss: 0.0651\n",
      "Epoch [42/400], Train Loss: 0.0613, Val Loss: 0.0647\n",
      "Epoch [43/400], Train Loss: 0.0611, Val Loss: 0.0644\n",
      "Epoch [44/400], Train Loss: 0.0610, Val Loss: 0.0642\n",
      "Epoch [45/400], Train Loss: 0.0611, Val Loss: 0.0644\n",
      "Epoch [46/400], Train Loss: 0.0608, Val Loss: 0.0645\n",
      "Epoch [47/400], Train Loss: 0.0611, Val Loss: 0.0646\n",
      "Epoch [48/400], Train Loss: 0.0612, Val Loss: 0.0644\n",
      "Epoch [49/400], Train Loss: 0.0607, Val Loss: 0.0641\n",
      "Epoch [50/400], Train Loss: 0.0608, Val Loss: 0.0643\n",
      "Epoch [51/400], Train Loss: 0.0606, Val Loss: 0.0640\n",
      "Epoch [52/400], Train Loss: 0.0605, Val Loss: 0.0644\n",
      "Epoch [53/400], Train Loss: 0.0604, Val Loss: 0.0640\n",
      "Epoch [54/400], Train Loss: 0.0604, Val Loss: 0.0642\n",
      "Epoch [55/400], Train Loss: 0.0603, Val Loss: 0.0640\n",
      "Epoch [56/400], Train Loss: 0.0602, Val Loss: 0.0639\n",
      "Epoch [57/400], Train Loss: 0.0603, Val Loss: 0.0640\n",
      "Epoch [58/400], Train Loss: 0.0601, Val Loss: 0.0642\n",
      "Epoch [59/400], Train Loss: 0.0601, Val Loss: 0.0634\n",
      "Epoch [60/400], Train Loss: 0.0598, Val Loss: 0.0636\n",
      "Epoch [61/400], Train Loss: 0.0598, Val Loss: 0.0636\n",
      "Epoch [62/400], Train Loss: 0.0598, Val Loss: 0.0636\n",
      "Epoch [63/400], Train Loss: 0.0600, Val Loss: 0.0636\n",
      "Epoch [64/400], Train Loss: 0.0599, Val Loss: 0.0635\n",
      "Epoch [65/400], Train Loss: 0.0600, Val Loss: 0.0639\n",
      "Epoch [66/400], Train Loss: 0.0598, Val Loss: 0.0634\n",
      "Epoch [67/400], Train Loss: 0.0597, Val Loss: 0.0638\n",
      "Epoch [68/400], Train Loss: 0.0596, Val Loss: 0.0632\n",
      "Epoch [69/400], Train Loss: 0.0597, Val Loss: 0.0633\n",
      "Epoch [70/400], Train Loss: 0.0596, Val Loss: 0.0635\n",
      "Epoch [71/400], Train Loss: 0.0596, Val Loss: 0.0635\n",
      "Epoch [72/400], Train Loss: 0.0595, Val Loss: 0.0632\n",
      "Epoch [73/400], Train Loss: 0.0595, Val Loss: 0.0636\n",
      "Epoch [74/400], Train Loss: 0.0595, Val Loss: 0.0639\n",
      "Epoch [75/400], Train Loss: 0.0595, Val Loss: 0.0638\n",
      "Epoch [76/400], Train Loss: 0.0594, Val Loss: 0.0630\n",
      "Epoch [77/400], Train Loss: 0.0593, Val Loss: 0.0631\n",
      "Epoch [78/400], Train Loss: 0.0593, Val Loss: 0.0632\n",
      "Epoch [79/400], Train Loss: 0.0593, Val Loss: 0.0632\n",
      "Epoch [80/400], Train Loss: 0.0593, Val Loss: 0.0635\n",
      "Epoch [81/400], Train Loss: 0.0591, Val Loss: 0.0631\n",
      "Epoch [82/400], Train Loss: 0.0592, Val Loss: 0.0634\n",
      "Epoch [83/400], Train Loss: 0.0591, Val Loss: 0.0634\n",
      "Epoch [84/400], Train Loss: 0.0591, Val Loss: 0.0632\n",
      "Epoch [85/400], Train Loss: 0.0591, Val Loss: 0.0632\n",
      "Epoch [86/400], Train Loss: 0.0591, Val Loss: 0.0632\n",
      "Epoch [87/400], Train Loss: 0.0592, Val Loss: 0.0635\n",
      "Epoch [88/400], Train Loss: 0.0590, Val Loss: 0.0634\n",
      "Epoch [89/400], Train Loss: 0.0591, Val Loss: 0.0626\n",
      "Epoch [90/400], Train Loss: 0.0593, Val Loss: 0.0631\n",
      "Epoch [91/400], Train Loss: 0.0591, Val Loss: 0.0629\n",
      "Epoch [92/400], Train Loss: 0.0592, Val Loss: 0.0630\n",
      "Epoch [93/400], Train Loss: 0.0591, Val Loss: 0.0627\n",
      "Epoch [94/400], Train Loss: 0.0590, Val Loss: 0.0633\n",
      "Epoch [95/400], Train Loss: 0.0590, Val Loss: 0.0632\n",
      "Epoch [96/400], Train Loss: 0.0589, Val Loss: 0.0632\n",
      "Epoch [97/400], Train Loss: 0.0589, Val Loss: 0.0627\n",
      "Epoch [98/400], Train Loss: 0.0587, Val Loss: 0.0628\n",
      "Epoch [99/400], Train Loss: 0.0589, Val Loss: 0.0629\n",
      "Epoch [100/400], Train Loss: 0.0588, Val Loss: 0.0630\n",
      "Epoch [101/400], Train Loss: 0.0587, Val Loss: 0.0631\n",
      "Epoch [102/400], Train Loss: 0.0588, Val Loss: 0.0627\n",
      "Epoch [103/400], Train Loss: 0.0591, Val Loss: 0.0627\n",
      "Epoch [104/400], Train Loss: 0.0587, Val Loss: 0.0626\n",
      "Epoch [105/400], Train Loss: 0.0586, Val Loss: 0.0628\n",
      "Epoch [106/400], Train Loss: 0.0586, Val Loss: 0.0628\n",
      "Epoch [107/400], Train Loss: 0.0586, Val Loss: 0.0627\n",
      "Epoch [108/400], Train Loss: 0.0586, Val Loss: 0.0636\n",
      "Epoch [109/400], Train Loss: 0.0587, Val Loss: 0.0628\n",
      "Epoch [110/400], Train Loss: 0.0586, Val Loss: 0.0629\n",
      "Epoch [111/400], Train Loss: 0.0585, Val Loss: 0.0627\n",
      "Epoch [112/400], Train Loss: 0.0587, Val Loss: 0.0626\n",
      "Epoch [113/400], Train Loss: 0.0586, Val Loss: 0.0627\n",
      "Epoch [114/400], Train Loss: 0.0586, Val Loss: 0.0629\n",
      "Epoch [115/400], Train Loss: 0.0584, Val Loss: 0.0628\n",
      "Epoch [116/400], Train Loss: 0.0584, Val Loss: 0.0626\n",
      "Epoch [117/400], Train Loss: 0.0584, Val Loss: 0.0628\n",
      "Epoch [118/400], Train Loss: 0.0582, Val Loss: 0.0623\n",
      "Epoch [119/400], Train Loss: 0.0583, Val Loss: 0.0622\n",
      "Epoch [120/400], Train Loss: 0.0586, Val Loss: 0.0631\n",
      "Epoch [121/400], Train Loss: 0.0584, Val Loss: 0.0622\n",
      "Epoch [122/400], Train Loss: 0.0583, Val Loss: 0.0624\n",
      "Epoch [123/400], Train Loss: 0.0582, Val Loss: 0.0628\n",
      "Epoch [124/400], Train Loss: 0.0583, Val Loss: 0.0621\n",
      "Epoch [125/400], Train Loss: 0.0582, Val Loss: 0.0622\n",
      "Epoch [126/400], Train Loss: 0.0585, Val Loss: 0.0623\n",
      "Epoch [127/400], Train Loss: 0.0582, Val Loss: 0.0621\n",
      "Epoch [128/400], Train Loss: 0.0582, Val Loss: 0.0624\n",
      "Epoch [129/400], Train Loss: 0.0582, Val Loss: 0.0626\n",
      "Epoch [130/400], Train Loss: 0.0580, Val Loss: 0.0621\n",
      "Epoch [131/400], Train Loss: 0.0580, Val Loss: 0.0624\n",
      "Epoch [132/400], Train Loss: 0.0580, Val Loss: 0.0621\n",
      "Epoch [133/400], Train Loss: 0.0579, Val Loss: 0.0623\n",
      "Epoch [134/400], Train Loss: 0.0578, Val Loss: 0.0621\n",
      "Epoch [135/400], Train Loss: 0.0578, Val Loss: 0.0622\n",
      "Epoch [136/400], Train Loss: 0.0579, Val Loss: 0.0620\n",
      "Epoch [137/400], Train Loss: 0.0581, Val Loss: 0.0623\n",
      "Epoch [138/400], Train Loss: 0.0580, Val Loss: 0.0631\n",
      "Epoch [139/400], Train Loss: 0.0581, Val Loss: 0.0620\n",
      "Epoch [140/400], Train Loss: 0.0577, Val Loss: 0.0619\n",
      "Epoch [141/400], Train Loss: 0.0576, Val Loss: 0.0619\n",
      "Epoch [142/400], Train Loss: 0.0577, Val Loss: 0.0622\n",
      "Epoch [143/400], Train Loss: 0.0578, Val Loss: 0.0621\n",
      "Epoch [144/400], Train Loss: 0.0578, Val Loss: 0.0620\n",
      "Epoch [145/400], Train Loss: 0.0578, Val Loss: 0.0617\n",
      "Epoch [146/400], Train Loss: 0.0576, Val Loss: 0.0614\n",
      "Epoch [147/400], Train Loss: 0.0578, Val Loss: 0.0621\n",
      "Epoch [148/400], Train Loss: 0.0574, Val Loss: 0.0616\n",
      "Epoch [149/400], Train Loss: 0.0575, Val Loss: 0.0622\n",
      "Epoch [150/400], Train Loss: 0.0577, Val Loss: 0.0623\n",
      "Epoch [151/400], Train Loss: 0.0576, Val Loss: 0.0622\n",
      "Epoch [152/400], Train Loss: 0.0575, Val Loss: 0.0618\n",
      "Epoch [153/400], Train Loss: 0.0575, Val Loss: 0.0617\n",
      "Epoch [154/400], Train Loss: 0.0573, Val Loss: 0.0620\n",
      "Epoch [155/400], Train Loss: 0.0576, Val Loss: 0.0626\n",
      "Epoch [156/400], Train Loss: 0.0576, Val Loss: 0.0619\n",
      "Epoch [157/400], Train Loss: 0.0574, Val Loss: 0.0619\n",
      "Epoch [158/400], Train Loss: 0.0573, Val Loss: 0.0621\n",
      "Epoch [159/400], Train Loss: 0.0572, Val Loss: 0.0618\n",
      "Epoch [160/400], Train Loss: 0.0572, Val Loss: 0.0618\n",
      "Epoch [161/400], Train Loss: 0.0573, Val Loss: 0.0616\n",
      "Epoch [162/400], Train Loss: 0.0571, Val Loss: 0.0619\n",
      "Epoch [163/400], Train Loss: 0.0572, Val Loss: 0.0616\n",
      "Epoch [164/400], Train Loss: 0.0572, Val Loss: 0.0617\n",
      "Epoch [165/400], Train Loss: 0.0570, Val Loss: 0.0618\n",
      "Epoch [166/400], Train Loss: 0.0572, Val Loss: 0.0618\n",
      "Epoch [167/400], Train Loss: 0.0569, Val Loss: 0.0616\n",
      "Epoch [168/400], Train Loss: 0.0571, Val Loss: 0.0618\n",
      "Epoch [169/400], Train Loss: 0.0571, Val Loss: 0.0618\n",
      "Epoch [170/400], Train Loss: 0.0571, Val Loss: 0.0617\n",
      "Epoch [171/400], Train Loss: 0.0570, Val Loss: 0.0617\n",
      "Epoch [172/400], Train Loss: 0.0571, Val Loss: 0.0614\n",
      "Epoch [173/400], Train Loss: 0.0567, Val Loss: 0.0617\n",
      "Epoch [174/400], Train Loss: 0.0568, Val Loss: 0.0616\n",
      "Epoch [175/400], Train Loss: 0.0570, Val Loss: 0.0616\n",
      "Epoch [176/400], Train Loss: 0.0567, Val Loss: 0.0614\n",
      "Epoch [177/400], Train Loss: 0.0568, Val Loss: 0.0617\n",
      "Epoch [178/400], Train Loss: 0.0569, Val Loss: 0.0611\n",
      "Epoch [179/400], Train Loss: 0.0568, Val Loss: 0.0614\n",
      "Epoch [180/400], Train Loss: 0.0567, Val Loss: 0.0616\n",
      "Epoch [181/400], Train Loss: 0.0566, Val Loss: 0.0613\n",
      "Epoch [182/400], Train Loss: 0.0569, Val Loss: 0.0613\n",
      "Epoch [183/400], Train Loss: 0.0568, Val Loss: 0.0615\n",
      "Epoch [184/400], Train Loss: 0.0571, Val Loss: 0.0617\n",
      "Epoch [185/400], Train Loss: 0.0569, Val Loss: 0.0614\n",
      "Epoch [186/400], Train Loss: 0.0568, Val Loss: 0.0618\n",
      "Epoch [187/400], Train Loss: 0.0568, Val Loss: 0.0613\n",
      "Epoch [188/400], Train Loss: 0.0564, Val Loss: 0.0612\n",
      "Epoch [189/400], Train Loss: 0.0565, Val Loss: 0.0616\n",
      "Epoch [190/400], Train Loss: 0.0566, Val Loss: 0.0614\n",
      "Epoch [191/400], Train Loss: 0.0565, Val Loss: 0.0616\n",
      "Epoch [192/400], Train Loss: 0.0568, Val Loss: 0.0613\n",
      "Epoch [193/400], Train Loss: 0.0566, Val Loss: 0.0615\n",
      "Epoch [194/400], Train Loss: 0.0567, Val Loss: 0.0612\n",
      "Epoch [195/400], Train Loss: 0.0564, Val Loss: 0.0614\n",
      "Epoch [196/400], Train Loss: 0.0564, Val Loss: 0.0614\n",
      "Epoch [197/400], Train Loss: 0.0565, Val Loss: 0.0613\n",
      "Epoch [198/400], Train Loss: 0.0564, Val Loss: 0.0614\n",
      "Epoch [199/400], Train Loss: 0.0565, Val Loss: 0.0617\n",
      "Epoch [200/400], Train Loss: 0.0564, Val Loss: 0.0614\n",
      "Epoch [201/400], Train Loss: 0.0565, Val Loss: 0.0614\n",
      "Epoch [202/400], Train Loss: 0.0565, Val Loss: 0.0615\n",
      "Epoch [203/400], Train Loss: 0.0564, Val Loss: 0.0617\n",
      "Epoch [204/400], Train Loss: 0.0562, Val Loss: 0.0616\n",
      "Epoch [205/400], Train Loss: 0.0565, Val Loss: 0.0619\n",
      "Epoch [206/400], Train Loss: 0.0564, Val Loss: 0.0614\n",
      "Epoch [207/400], Train Loss: 0.0561, Val Loss: 0.0614\n",
      "Epoch [208/400], Train Loss: 0.0562, Val Loss: 0.0615\n",
      "Epoch [209/400], Train Loss: 0.0562, Val Loss: 0.0614\n",
      "Epoch [210/400], Train Loss: 0.0560, Val Loss: 0.0613\n",
      "Epoch [211/400], Train Loss: 0.0563, Val Loss: 0.0614\n",
      "Epoch [212/400], Train Loss: 0.0562, Val Loss: 0.0613\n",
      "Epoch [213/400], Train Loss: 0.0560, Val Loss: 0.0615\n",
      "Epoch [214/400], Train Loss: 0.0560, Val Loss: 0.0612\n",
      "Epoch [215/400], Train Loss: 0.0560, Val Loss: 0.0617\n",
      "Epoch [216/400], Train Loss: 0.0558, Val Loss: 0.0619\n",
      "Epoch [217/400], Train Loss: 0.0560, Val Loss: 0.0620\n",
      "Epoch [218/400], Train Loss: 0.0561, Val Loss: 0.0615\n",
      "Epoch [219/400], Train Loss: 0.0561, Val Loss: 0.0616\n",
      "Epoch [220/400], Train Loss: 0.0561, Val Loss: 0.0614\n",
      "Epoch [221/400], Train Loss: 0.0560, Val Loss: 0.0613\n",
      "Epoch [222/400], Train Loss: 0.0562, Val Loss: 0.0615\n",
      "Epoch [223/400], Train Loss: 0.0558, Val Loss: 0.0615\n",
      "Epoch [224/400], Train Loss: 0.0559, Val Loss: 0.0621\n",
      "Epoch [225/400], Train Loss: 0.0559, Val Loss: 0.0616\n",
      "Epoch [226/400], Train Loss: 0.0559, Val Loss: 0.0614\n",
      "Epoch [227/400], Train Loss: 0.0560, Val Loss: 0.0614\n",
      "Epoch [228/400], Train Loss: 0.0558, Val Loss: 0.0614\n",
      "Epoch [229/400], Train Loss: 0.0559, Val Loss: 0.0616\n",
      "Epoch [230/400], Train Loss: 0.0559, Val Loss: 0.0616\n",
      "Epoch [231/400], Train Loss: 0.0558, Val Loss: 0.0613\n",
      "Epoch [232/400], Train Loss: 0.0558, Val Loss: 0.0625\n",
      "Epoch [233/400], Train Loss: 0.0559, Val Loss: 0.0615\n",
      "Epoch [234/400], Train Loss: 0.0559, Val Loss: 0.0615\n",
      "Epoch [235/400], Train Loss: 0.0558, Val Loss: 0.0613\n",
      "Epoch [236/400], Train Loss: 0.0558, Val Loss: 0.0613\n",
      "Epoch [237/400], Train Loss: 0.0557, Val Loss: 0.0617\n",
      "Epoch [238/400], Train Loss: 0.0557, Val Loss: 0.0615\n",
      "Epoch [239/400], Train Loss: 0.0557, Val Loss: 0.0617\n",
      "Epoch [240/400], Train Loss: 0.0557, Val Loss: 0.0622\n",
      "Epoch [241/400], Train Loss: 0.0558, Val Loss: 0.0619\n",
      "Epoch [242/400], Train Loss: 0.0555, Val Loss: 0.0614\n",
      "Epoch [243/400], Train Loss: 0.0557, Val Loss: 0.0611\n",
      "Epoch [244/400], Train Loss: 0.0555, Val Loss: 0.0611\n",
      "Epoch [245/400], Train Loss: 0.0557, Val Loss: 0.0613\n",
      "Epoch [246/400], Train Loss: 0.0558, Val Loss: 0.0621\n",
      "Epoch [247/400], Train Loss: 0.0556, Val Loss: 0.0611\n",
      "Epoch [248/400], Train Loss: 0.0556, Val Loss: 0.0616\n",
      "Epoch [249/400], Train Loss: 0.0554, Val Loss: 0.0616\n",
      "Epoch [250/400], Train Loss: 0.0556, Val Loss: 0.0616\n",
      "Epoch [251/400], Train Loss: 0.0554, Val Loss: 0.0618\n",
      "Epoch [252/400], Train Loss: 0.0557, Val Loss: 0.0622\n",
      "Epoch [253/400], Train Loss: 0.0557, Val Loss: 0.0621\n",
      "Epoch [254/400], Train Loss: 0.0554, Val Loss: 0.0620\n",
      "Epoch [255/400], Train Loss: 0.0555, Val Loss: 0.0620\n",
      "Epoch [256/400], Train Loss: 0.0556, Val Loss: 0.0615\n",
      "Epoch [257/400], Train Loss: 0.0554, Val Loss: 0.0614\n",
      "Epoch [258/400], Train Loss: 0.0551, Val Loss: 0.0617\n",
      "Epoch [259/400], Train Loss: 0.0554, Val Loss: 0.0621\n",
      "Epoch [260/400], Train Loss: 0.0558, Val Loss: 0.0618\n",
      "Epoch [261/400], Train Loss: 0.0554, Val Loss: 0.0622\n",
      "Epoch [262/400], Train Loss: 0.0554, Val Loss: 0.0618\n",
      "Epoch [263/400], Train Loss: 0.0554, Val Loss: 0.0621\n",
      "Epoch [264/400], Train Loss: 0.0556, Val Loss: 0.0615\n",
      "Epoch [265/400], Train Loss: 0.0554, Val Loss: 0.0615\n",
      "Epoch [266/400], Train Loss: 0.0551, Val Loss: 0.0619\n",
      "Epoch [267/400], Train Loss: 0.0552, Val Loss: 0.0617\n",
      "Epoch [268/400], Train Loss: 0.0552, Val Loss: 0.0619\n",
      "Epoch [269/400], Train Loss: 0.0552, Val Loss: 0.0616\n",
      "Epoch [270/400], Train Loss: 0.0551, Val Loss: 0.0617\n",
      "Epoch [271/400], Train Loss: 0.0550, Val Loss: 0.0620\n",
      "Epoch [272/400], Train Loss: 0.0553, Val Loss: 0.0618\n",
      "Epoch [273/400], Train Loss: 0.0553, Val Loss: 0.0619\n",
      "Epoch [274/400], Train Loss: 0.0556, Val Loss: 0.0618\n",
      "Epoch [275/400], Train Loss: 0.0551, Val Loss: 0.0616\n",
      "Epoch [276/400], Train Loss: 0.0551, Val Loss: 0.0617\n",
      "Epoch [277/400], Train Loss: 0.0551, Val Loss: 0.0621\n",
      "Epoch [278/400], Train Loss: 0.0554, Val Loss: 0.0615\n",
      "Epoch [279/400], Train Loss: 0.0550, Val Loss: 0.0617\n",
      "Epoch [280/400], Train Loss: 0.0550, Val Loss: 0.0623\n",
      "Epoch [281/400], Train Loss: 0.0552, Val Loss: 0.0618\n",
      "Epoch [282/400], Train Loss: 0.0549, Val Loss: 0.0619\n",
      "Epoch [283/400], Train Loss: 0.0549, Val Loss: 0.0615\n",
      "Epoch [284/400], Train Loss: 0.0550, Val Loss: 0.0615\n",
      "Epoch [285/400], Train Loss: 0.0550, Val Loss: 0.0618\n",
      "Epoch [286/400], Train Loss: 0.0549, Val Loss: 0.0616\n",
      "Epoch [287/400], Train Loss: 0.0554, Val Loss: 0.0625\n",
      "Epoch [288/400], Train Loss: 0.0553, Val Loss: 0.0617\n",
      "Epoch [289/400], Train Loss: 0.0551, Val Loss: 0.0617\n",
      "Epoch [290/400], Train Loss: 0.0551, Val Loss: 0.0619\n",
      "Epoch [291/400], Train Loss: 0.0549, Val Loss: 0.0618\n",
      "Epoch [292/400], Train Loss: 0.0549, Val Loss: 0.0620\n",
      "Epoch [293/400], Train Loss: 0.0549, Val Loss: 0.0618\n",
      "Epoch [294/400], Train Loss: 0.0548, Val Loss: 0.0620\n",
      "Epoch [295/400], Train Loss: 0.0547, Val Loss: 0.0624\n",
      "Epoch [296/400], Train Loss: 0.0549, Val Loss: 0.0626\n",
      "Epoch [297/400], Train Loss: 0.0548, Val Loss: 0.0616\n",
      "Epoch [298/400], Train Loss: 0.0547, Val Loss: 0.0624\n",
      "Epoch [299/400], Train Loss: 0.0549, Val Loss: 0.0619\n",
      "Epoch [300/400], Train Loss: 0.0548, Val Loss: 0.0622\n",
      "Epoch [301/400], Train Loss: 0.0552, Val Loss: 0.0624\n",
      "Epoch [302/400], Train Loss: 0.0551, Val Loss: 0.0618\n",
      "Epoch [303/400], Train Loss: 0.0547, Val Loss: 0.0622\n",
      "Epoch [304/400], Train Loss: 0.0551, Val Loss: 0.0616\n",
      "Epoch [305/400], Train Loss: 0.0547, Val Loss: 0.0623\n",
      "Epoch [306/400], Train Loss: 0.0546, Val Loss: 0.0623\n",
      "Epoch [307/400], Train Loss: 0.0549, Val Loss: 0.0622\n",
      "Epoch [308/400], Train Loss: 0.0548, Val Loss: 0.0621\n",
      "Epoch [309/400], Train Loss: 0.0547, Val Loss: 0.0621\n",
      "Epoch [310/400], Train Loss: 0.0548, Val Loss: 0.0622\n",
      "Epoch [311/400], Train Loss: 0.0545, Val Loss: 0.0626\n",
      "Epoch [312/400], Train Loss: 0.0553, Val Loss: 0.0621\n",
      "Epoch [313/400], Train Loss: 0.0548, Val Loss: 0.0621\n",
      "Epoch [314/400], Train Loss: 0.0546, Val Loss: 0.0615\n",
      "Epoch [315/400], Train Loss: 0.0546, Val Loss: 0.0622\n",
      "Epoch [316/400], Train Loss: 0.0547, Val Loss: 0.0619\n",
      "Epoch [317/400], Train Loss: 0.0546, Val Loss: 0.0620\n",
      "Epoch [318/400], Train Loss: 0.0548, Val Loss: 0.0617\n",
      "Epoch [319/400], Train Loss: 0.0547, Val Loss: 0.0618\n",
      "Epoch [320/400], Train Loss: 0.0546, Val Loss: 0.0626\n",
      "Epoch [321/400], Train Loss: 0.0551, Val Loss: 0.0620\n",
      "Epoch [322/400], Train Loss: 0.0544, Val Loss: 0.0619\n",
      "Epoch [323/400], Train Loss: 0.0545, Val Loss: 0.0620\n",
      "Epoch [324/400], Train Loss: 0.0544, Val Loss: 0.0623\n",
      "Epoch [325/400], Train Loss: 0.0546, Val Loss: 0.0625\n",
      "Epoch [326/400], Train Loss: 0.0547, Val Loss: 0.0621\n",
      "Epoch [327/400], Train Loss: 0.0545, Val Loss: 0.0620\n",
      "Epoch [328/400], Train Loss: 0.0546, Val Loss: 0.0620\n",
      "Epoch [329/400], Train Loss: 0.0546, Val Loss: 0.0621\n",
      "Epoch [330/400], Train Loss: 0.0544, Val Loss: 0.0620\n",
      "Epoch [331/400], Train Loss: 0.0543, Val Loss: 0.0623\n",
      "Epoch [332/400], Train Loss: 0.0545, Val Loss: 0.0618\n",
      "Epoch [333/400], Train Loss: 0.0544, Val Loss: 0.0623\n",
      "Epoch [334/400], Train Loss: 0.0545, Val Loss: 0.0621\n",
      "Epoch [335/400], Train Loss: 0.0544, Val Loss: 0.0623\n",
      "Epoch [336/400], Train Loss: 0.0544, Val Loss: 0.0621\n",
      "Epoch [337/400], Train Loss: 0.0545, Val Loss: 0.0623\n",
      "Epoch [338/400], Train Loss: 0.0546, Val Loss: 0.0625\n",
      "Epoch [339/400], Train Loss: 0.0544, Val Loss: 0.0631\n",
      "Epoch [340/400], Train Loss: 0.0544, Val Loss: 0.0620\n",
      "Epoch [341/400], Train Loss: 0.0540, Val Loss: 0.0620\n",
      "Epoch [342/400], Train Loss: 0.0545, Val Loss: 0.0626\n",
      "Epoch [343/400], Train Loss: 0.0542, Val Loss: 0.0620\n",
      "Epoch [344/400], Train Loss: 0.0543, Val Loss: 0.0634\n",
      "Epoch [345/400], Train Loss: 0.0548, Val Loss: 0.0620\n",
      "Epoch [346/400], Train Loss: 0.0543, Val Loss: 0.0622\n",
      "Epoch [347/400], Train Loss: 0.0542, Val Loss: 0.0621\n",
      "Epoch [348/400], Train Loss: 0.0541, Val Loss: 0.0619\n",
      "Epoch [349/400], Train Loss: 0.0541, Val Loss: 0.0623\n",
      "Epoch [350/400], Train Loss: 0.0542, Val Loss: 0.0622\n",
      "Epoch [351/400], Train Loss: 0.0541, Val Loss: 0.0628\n",
      "Epoch [352/400], Train Loss: 0.0543, Val Loss: 0.0626\n",
      "Epoch [353/400], Train Loss: 0.0547, Val Loss: 0.0627\n",
      "Epoch [354/400], Train Loss: 0.0545, Val Loss: 0.0626\n",
      "Epoch [355/400], Train Loss: 0.0545, Val Loss: 0.0623\n",
      "Epoch [356/400], Train Loss: 0.0546, Val Loss: 0.0629\n",
      "Epoch [357/400], Train Loss: 0.0543, Val Loss: 0.0621\n",
      "Epoch [358/400], Train Loss: 0.0540, Val Loss: 0.0621\n",
      "Epoch [359/400], Train Loss: 0.0542, Val Loss: 0.0623\n",
      "Epoch [360/400], Train Loss: 0.0542, Val Loss: 0.0622\n",
      "Epoch [361/400], Train Loss: 0.0541, Val Loss: 0.0628\n",
      "Epoch [362/400], Train Loss: 0.0543, Val Loss: 0.0621\n",
      "Epoch [363/400], Train Loss: 0.0545, Val Loss: 0.0630\n",
      "Epoch [364/400], Train Loss: 0.0543, Val Loss: 0.0623\n",
      "Epoch [365/400], Train Loss: 0.0544, Val Loss: 0.0625\n",
      "Epoch [366/400], Train Loss: 0.0539, Val Loss: 0.0620\n",
      "Epoch [367/400], Train Loss: 0.0541, Val Loss: 0.0624\n",
      "Epoch [368/400], Train Loss: 0.0541, Val Loss: 0.0627\n",
      "Epoch [369/400], Train Loss: 0.0539, Val Loss: 0.0624\n",
      "Epoch [370/400], Train Loss: 0.0540, Val Loss: 0.0625\n",
      "Epoch [371/400], Train Loss: 0.0542, Val Loss: 0.0630\n",
      "Epoch [372/400], Train Loss: 0.0542, Val Loss: 0.0622\n",
      "Epoch [373/400], Train Loss: 0.0542, Val Loss: 0.0625\n",
      "Epoch [374/400], Train Loss: 0.0540, Val Loss: 0.0627\n",
      "Epoch [375/400], Train Loss: 0.0543, Val Loss: 0.0629\n",
      "Epoch [376/400], Train Loss: 0.0541, Val Loss: 0.0629\n",
      "Epoch [377/400], Train Loss: 0.0539, Val Loss: 0.0628\n",
      "Epoch [378/400], Train Loss: 0.0538, Val Loss: 0.0626\n",
      "Epoch [379/400], Train Loss: 0.0539, Val Loss: 0.0630\n",
      "Epoch [380/400], Train Loss: 0.0539, Val Loss: 0.0628\n",
      "Epoch [381/400], Train Loss: 0.0539, Val Loss: 0.0630\n",
      "Epoch [382/400], Train Loss: 0.0538, Val Loss: 0.0625\n",
      "Epoch [383/400], Train Loss: 0.0537, Val Loss: 0.0627\n",
      "Epoch [384/400], Train Loss: 0.0540, Val Loss: 0.0628\n",
      "Epoch [385/400], Train Loss: 0.0541, Val Loss: 0.0627\n",
      "Epoch [386/400], Train Loss: 0.0539, Val Loss: 0.0630\n",
      "Epoch [387/400], Train Loss: 0.0541, Val Loss: 0.0630\n",
      "Epoch [388/400], Train Loss: 0.0540, Val Loss: 0.0629\n",
      "Epoch [389/400], Train Loss: 0.0538, Val Loss: 0.0628\n",
      "Epoch [390/400], Train Loss: 0.0537, Val Loss: 0.0627\n",
      "Epoch [391/400], Train Loss: 0.0539, Val Loss: 0.0632\n",
      "Epoch [392/400], Train Loss: 0.0538, Val Loss: 0.0630\n",
      "Epoch [393/400], Train Loss: 0.0542, Val Loss: 0.0629\n",
      "Epoch [394/400], Train Loss: 0.0540, Val Loss: 0.0627\n",
      "Epoch [395/400], Train Loss: 0.0540, Val Loss: 0.0624\n",
      "Epoch [396/400], Train Loss: 0.0538, Val Loss: 0.0631\n",
      "Epoch [397/400], Train Loss: 0.0541, Val Loss: 0.0634\n",
      "Epoch [398/400], Train Loss: 0.0540, Val Loss: 0.0631\n",
      "Epoch [399/400], Train Loss: 0.0539, Val Loss: 0.0632\n",
      "Epoch [400/400], Train Loss: 0.0541, Val Loss: 0.0645\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Assume that the Encoder, Decoder, and Model classes are already defined\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 50        # Number of observed variables (same as the number of x_i's in the dataset)\n",
    "output_dim = 5        # Output dimension of the encoder (dimension of Z)\n",
    "embedding_dim = 64    # Embedding dimension for the embeddings e and e_i's\n",
    "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
    "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 400           # Number of epochs\n",
    "batch_size = 32             # Batch size (already set in the DataLoader)\n",
    "print_every = 1           # How often to print loss (in epochs)\n",
    "\n",
    "\n",
    "model_path = \"trained_model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Trained model found. Loading the model.\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (batch,) in enumerate(train_loader):  # Use train_loader for training\n",
    "        batch = batch.to(device)  # Move batch to device\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute predicted x_hat by passing input x through the model\n",
    "        x_hat = model(batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(x_hat, batch)\n",
    "\n",
    "        # Backward pass: Compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for batch_idx, (batch,) in enumerate(val_loader):  # Use val_loader for validation\n",
    "            batch = batch.to(device)  # Move batch to device\n",
    "\n",
    "            # Forward pass for validation\n",
    "            x_hat = model(batch)\n",
    "\n",
    "            # Compute the loss for validation\n",
    "            loss = criterion(x_hat, batch)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Example save the trained model after training\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoBeOpJnYZhC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
