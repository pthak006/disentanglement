{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_HACIxMBlMv",
        "outputId": "a266a5c4-b111-46e8-b71d-77b930d36ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of instances in the dataset: 2000\n",
            "Number of columns in the dataset: 50\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
            "0        2       0         3          1   4        1       4         1   \n",
            "1        2       0         1          2   2        1       4         3   \n",
            "2        3       0         2          1   3        1       4         3   \n",
            "3        2       0         1          1   1        0       4         1   \n",
            "4        2       0         1          1   3        0       4         3   \n",
            "\n",
            "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
            "0          2   2  ...         3        3          3           2    3   \n",
            "1          3   1  ...         2        3          2           2    3   \n",
            "2          3   0  ...         4        4          2           1    4   \n",
            "3          3   1  ...         1        2          2           1    3   \n",
            "4          2   0  ...         3        4          1           3    4   \n",
            "\n",
            "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
            "0         1        4          4           2    4  \n",
            "1         1        3          2           2    3  \n",
            "2         2        4          2           0    4  \n",
            "3         1        3          2           1    2  \n",
            "4         1        3          1           3    4  \n",
            "\n",
            "[5 rows x 50 columns]\n",
            "\n",
            "Data Types and Non-Null Counts:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 50 columns):\n",
            " #   Column      Non-Null Count  Dtype\n",
            "---  ------      --------------  -----\n",
            " 0   blue_q0     2000 non-null   int64\n",
            " 1   red_q1      2000 non-null   int64\n",
            " 2   green_q2    2000 non-null   int64\n",
            " 3   purple_q3   2000 non-null   int64\n",
            " 4   q4          2000 non-null   int64\n",
            " 5   blue_q5     2000 non-null   int64\n",
            " 6   red_q6      2000 non-null   int64\n",
            " 7   green_q7    2000 non-null   int64\n",
            " 8   purple_q8   2000 non-null   int64\n",
            " 9   q9          2000 non-null   int64\n",
            " 10  blue_q10    2000 non-null   int64\n",
            " 11  red_q11     2000 non-null   int64\n",
            " 12  green_q12   2000 non-null   int64\n",
            " 13  purple_q13  2000 non-null   int64\n",
            " 14  q14         2000 non-null   int64\n",
            " 15  blue_q15    2000 non-null   int64\n",
            " 16  red_q16     2000 non-null   int64\n",
            " 17  green_q17   2000 non-null   int64\n",
            " 18  purple_q18  2000 non-null   int64\n",
            " 19  q19         2000 non-null   int64\n",
            " 20  blue_q20    2000 non-null   int64\n",
            " 21  red_q21     2000 non-null   int64\n",
            " 22  green_q22   2000 non-null   int64\n",
            " 23  purple_q23  2000 non-null   int64\n",
            " 24  q24         2000 non-null   int64\n",
            " 25  blue_q25    2000 non-null   int64\n",
            " 26  red_q26     2000 non-null   int64\n",
            " 27  green_q27   2000 non-null   int64\n",
            " 28  purple_q28  2000 non-null   int64\n",
            " 29  q29         2000 non-null   int64\n",
            " 30  blue_q30    2000 non-null   int64\n",
            " 31  red_q31     2000 non-null   int64\n",
            " 32  green_q32   2000 non-null   int64\n",
            " 33  purple_q33  2000 non-null   int64\n",
            " 34  q34         2000 non-null   int64\n",
            " 35  blue_q35    2000 non-null   int64\n",
            " 36  red_q36     2000 non-null   int64\n",
            " 37  green_q37   2000 non-null   int64\n",
            " 38  purple_q38  2000 non-null   int64\n",
            " 39  q39         2000 non-null   int64\n",
            " 40  blue_q40    2000 non-null   int64\n",
            " 41  red_q41     2000 non-null   int64\n",
            " 42  green_q42   2000 non-null   int64\n",
            " 43  purple_q43  2000 non-null   int64\n",
            " 44  q44         2000 non-null   int64\n",
            " 45  blue_q45    2000 non-null   int64\n",
            " 46  red_q46     2000 non-null   int64\n",
            " 47  green_q47   2000 non-null   int64\n",
            " 48  purple_q48  2000 non-null   int64\n",
            " 49  q49         2000 non-null   int64\n",
            "dtypes: int64(50)\n",
            "memory usage: 781.4 KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the GitHub repository\n",
        "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Number of instances in the dataset:\", df.shape[0])\n",
        "print(\"Number of columns in the dataset:\", df.shape[1])\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display additional information\n",
        "print(\"\\nData Types and Non-Null Counts:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify column prefixes for each true factor\n",
        "factor_columns = {\n",
        "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
        "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
        "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
        "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
        "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
        "}\n",
        "\n",
        "# Calculate true factors by summing the respective columns\n",
        "true_factors = pd.DataFrame()\n",
        "for factor_name, columns in factor_columns.items():\n",
        "    true_factors[factor_name] = df[columns].sum(axis=1)\n",
        "\n",
        "# Display the first few rows of the calculated true factors\n",
        "print(true_factors.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81r2mDP3BzZ-",
        "outputId": "8faa8967-c63d-4230-c155-42b29b34e6d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Factor1  Factor2  Factor3  Factor4  Factor5\n",
            "0       20       21       21       22       28\n",
            "1       21       20       21       26       23\n",
            "2       23       20       17       22       25\n",
            "3       17       15       11       22       15\n",
            "4       20       14       24       23       24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df / 4.0\n",
        "# print(df.head())"
      ],
      "metadata": {
        "id": "yaO1N84CB7_U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "data_array = df.to_numpy()\n",
        "\n",
        "# Convert the data to a PyTorch tensor\n",
        "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
        "\n",
        "# Create a PyTorch dataset\n",
        "dataset = TensorDataset(data_tensor)\n",
        "\n",
        "# Create a DataLoader for the dataset\n",
        "batch_size = 32  # You can adjust the batch size as needed\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Display the shape of the tensor to verify\n",
        "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
        "print(f\"Number of batches: {len(dataloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6jA29S2CJQV",
        "outputId": "97c14234-90fb-43e0-944e-38113a3f8b9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data tensor shape: torch.Size([2000, 50])\n",
            "Number of batches: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dims=[]):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Define the layers of the MLP\n",
        "        dims = [input_dim] + hidden_dims + [output_dim]\n",
        "        layers = []\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
        "            if i < len(dims) - 2:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        # Learnable embedding vector e (moved from Decoder to Encoder)\n",
        "        self.e = nn.Parameter(torch.randn(embedding_dim))\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the MLP to get Z\n",
        "        Z = self.mlp(x)  # Shape: (batch_size, output_dim)\n",
        "\n",
        "        # Convert Z to \\hat Z by multiplying each scalar z_i with embedding vector e\n",
        "        batch_size = Z.size(0)\n",
        "        output_dim = Z.size(1)\n",
        "        e_expanded = self.e.unsqueeze(0).unsqueeze(0)      # Shape: (1, 1, embedding_dim)\n",
        "        Z_expanded = Z.unsqueeze(2)                        # Shape: (batch_size, output_dim, 1)\n",
        "        hat_Z = Z_expanded * e_expanded                    # Shape: (batch_size, output_dim, embedding_dim)\n",
        "\n",
        "        return hat_Z"
      ],
      "metadata": {
        "id": "rywEvDazCYkL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dims=[]):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.input_dim = input_dim      # Number of observed variables (n)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Learnable query embeddings (e1, e2, ..., en)\n",
        "        self.query_embeddings = nn.Parameter(torch.randn(input_dim, embedding_dim))\n",
        "\n",
        "        # MultiheadAttention module with 1 head\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        # MLP to predict x_i's from embeddings\n",
        "        dims = [embedding_dim] + hidden_dims + [1]\n",
        "        layers = []\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
        "            if i < len(dims) - 2:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, hat_Z):\n",
        "        \"\"\"\n",
        "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
        "        \"\"\"\n",
        "        batch_size = hat_Z.size(0)\n",
        "\n",
        "        # Prepare query embeddings and expand to batch size\n",
        "        query_embeddings = self.query_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, embedding_dim)\n",
        "\n",
        "        # Apply scaled dot-product attention\n",
        "        attn_output, attn_weights = self.attention(query_embeddings, hat_Z, hat_Z)        # Output shape: (batch_size, input_dim, embedding_dim)\n",
        "\n",
        "        # Add residual connection and apply layer normalization\n",
        "        out = self.layer_norm(attn_output + query_embeddings)                             # Shape: (batch_size, input_dim, embedding_dim)\n",
        "\n",
        "        # Flatten the embeddings and pass through MLP to predict x_i's\n",
        "        out_flat = out.reshape(-1, self.embedding_dim)                                    # Shape: (batch_size * input_dim, embedding_dim)\n",
        "        x_hat_flat = self.mlp(out_flat)                                                   # Shape: (batch_size * input_dim, 1)\n",
        "        x_hat = x_hat_flat.view(batch_size, self.input_dim)                               # Shape: (batch_size, input_dim)\n",
        "\n",
        "        return x_hat"
      ],
      "metadata": {
        "id": "tUXzn0AHHq1T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete model combining the encoder and decoder\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder(input_dim=input_dim, output_dim=output_dim, embedding_dim=embedding_dim, hidden_dims=encoder_hidden_dims)\n",
        "        self.decoder = Decoder(input_dim=input_dim, embedding_dim=embedding_dim, hidden_dims=decoder_hidden_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hat_Z = self.encoder(x)     # Obtain \\hat Z from the encoder\n",
        "        x_hat = self.decoder(hat_Z) # Reconstruct x from \\hat Z using the decoder\n",
        "        return x_hat"
      ],
      "metadata": {
        "id": "xnBqmgVjIat0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assume that the Encoder, Decoder, and Model classes are already defined\n",
        "\n",
        "# Define dimensions\n",
        "input_dim = 50        # Number of observed variables (same as the number of x_i's in the dataset)\n",
        "output_dim = 5        # Output dimension of the encoder (dimension of Z)\n",
        "embedding_dim = 64    # Embedding dimension for the embeddings e and e_i's\n",
        "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
        "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
        "\n",
        "# Instantiate the model\n",
        "model = Model(\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    embedding_dim=embedding_dim,\n",
        "    encoder_hidden_dims=encoder_hidden_dims,\n",
        "    decoder_hidden_dims=decoder_hidden_dims\n",
        ")\n",
        "\n",
        "# Move the model to the appropriate device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 1024           # Number of epochs\n",
        "batch_size = 32           # Batch size (already set in the DataLoader)\n",
        "print_every = 1           # How often to print loss (in epochs)\n",
        "\n",
        "# Load the dataset from the DataLoader (assume it is already defined)\n",
        "# Example DataLoader code\n",
        "# df = pd.read_csv(url)\n",
        "# data_array = df.to_numpy()\n",
        "# data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
        "# dataset = TensorDataset(data_tensor)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (batch,) in enumerate(dataloader):\n",
        "        batch = batch.to(device)  # Move batch to device\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute predicted x_hat by passing input x through the model\n",
        "        x_hat = model(batch)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(x_hat, batch)\n",
        "\n",
        "        # Backward pass: Compute the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for reporting\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    if (epoch + 1) % print_every == 0:\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Example save the trained model after training\n",
        "torch.save(model.state_dict(), \"trained_model.pth\")\n",
        "print(\"Training complete and model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki1licN-PTun",
        "outputId": "fb864687-79d0-4763-ec7f-18a71da1ed50"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1024], Loss: 0.1158\n",
            "Epoch [2/1024], Loss: 0.0828\n",
            "Epoch [3/1024], Loss: 0.0746\n",
            "Epoch [4/1024], Loss: 0.0721\n",
            "Epoch [5/1024], Loss: 0.0702\n",
            "Epoch [6/1024], Loss: 0.0678\n",
            "Epoch [7/1024], Loss: 0.0656\n",
            "Epoch [8/1024], Loss: 0.0646\n",
            "Epoch [9/1024], Loss: 0.0639\n",
            "Epoch [10/1024], Loss: 0.0636\n",
            "Epoch [11/1024], Loss: 0.0633\n",
            "Epoch [12/1024], Loss: 0.0629\n",
            "Epoch [13/1024], Loss: 0.0626\n",
            "Epoch [14/1024], Loss: 0.0623\n",
            "Epoch [15/1024], Loss: 0.0620\n",
            "Epoch [16/1024], Loss: 0.0619\n",
            "Epoch [17/1024], Loss: 0.0617\n",
            "Epoch [18/1024], Loss: 0.0616\n",
            "Epoch [19/1024], Loss: 0.0615\n",
            "Epoch [20/1024], Loss: 0.0616\n",
            "Epoch [21/1024], Loss: 0.0615\n",
            "Epoch [22/1024], Loss: 0.0612\n",
            "Epoch [23/1024], Loss: 0.0611\n",
            "Epoch [24/1024], Loss: 0.0609\n",
            "Epoch [25/1024], Loss: 0.0608\n",
            "Epoch [26/1024], Loss: 0.0605\n",
            "Epoch [27/1024], Loss: 0.0604\n",
            "Epoch [28/1024], Loss: 0.0602\n",
            "Epoch [29/1024], Loss: 0.0601\n",
            "Epoch [30/1024], Loss: 0.0601\n",
            "Epoch [31/1024], Loss: 0.0600\n",
            "Epoch [32/1024], Loss: 0.0606\n",
            "Epoch [33/1024], Loss: 0.0603\n",
            "Epoch [34/1024], Loss: 0.0600\n",
            "Epoch [35/1024], Loss: 0.0597\n",
            "Epoch [36/1024], Loss: 0.0595\n",
            "Epoch [37/1024], Loss: 0.0594\n",
            "Epoch [38/1024], Loss: 0.0594\n",
            "Epoch [39/1024], Loss: 0.0592\n",
            "Epoch [40/1024], Loss: 0.0591\n",
            "Epoch [41/1024], Loss: 0.0590\n",
            "Epoch [42/1024], Loss: 0.0590\n",
            "Epoch [43/1024], Loss: 0.0589\n",
            "Epoch [44/1024], Loss: 0.0588\n",
            "Epoch [45/1024], Loss: 0.0588\n",
            "Epoch [46/1024], Loss: 0.0588\n",
            "Epoch [47/1024], Loss: 0.0589\n",
            "Epoch [48/1024], Loss: 0.0592\n",
            "Epoch [49/1024], Loss: 0.0589\n",
            "Epoch [50/1024], Loss: 0.0591\n",
            "Epoch [51/1024], Loss: 0.0591\n",
            "Epoch [52/1024], Loss: 0.0593\n",
            "Epoch [53/1024], Loss: 0.0586\n",
            "Epoch [54/1024], Loss: 0.0583\n",
            "Epoch [55/1024], Loss: 0.0581\n",
            "Epoch [56/1024], Loss: 0.0581\n",
            "Epoch [57/1024], Loss: 0.0580\n",
            "Epoch [58/1024], Loss: 0.0580\n",
            "Epoch [59/1024], Loss: 0.0579\n",
            "Epoch [60/1024], Loss: 0.0579\n",
            "Epoch [61/1024], Loss: 0.0580\n",
            "Epoch [62/1024], Loss: 0.0584\n",
            "Epoch [63/1024], Loss: 0.0583\n",
            "Epoch [64/1024], Loss: 0.0580\n",
            "Epoch [65/1024], Loss: 0.0578\n",
            "Epoch [66/1024], Loss: 0.0578\n",
            "Epoch [67/1024], Loss: 0.0578\n",
            "Epoch [68/1024], Loss: 0.0580\n",
            "Epoch [69/1024], Loss: 0.0578\n",
            "Epoch [70/1024], Loss: 0.0577\n",
            "Epoch [71/1024], Loss: 0.0580\n",
            "Epoch [72/1024], Loss: 0.0578\n",
            "Epoch [73/1024], Loss: 0.0576\n",
            "Epoch [74/1024], Loss: 0.0575\n",
            "Epoch [75/1024], Loss: 0.0573\n",
            "Epoch [76/1024], Loss: 0.0572\n",
            "Epoch [77/1024], Loss: 0.0572\n",
            "Epoch [78/1024], Loss: 0.0572\n",
            "Epoch [79/1024], Loss: 0.0572\n",
            "Epoch [80/1024], Loss: 0.0572\n",
            "Epoch [81/1024], Loss: 0.0572\n",
            "Epoch [82/1024], Loss: 0.0572\n",
            "Epoch [83/1024], Loss: 0.0575\n",
            "Epoch [84/1024], Loss: 0.0583\n",
            "Epoch [85/1024], Loss: 0.0573\n",
            "Epoch [86/1024], Loss: 0.0571\n",
            "Epoch [87/1024], Loss: 0.0571\n",
            "Epoch [88/1024], Loss: 0.0570\n",
            "Epoch [89/1024], Loss: 0.0569\n",
            "Epoch [90/1024], Loss: 0.0568\n",
            "Epoch [91/1024], Loss: 0.0568\n",
            "Epoch [92/1024], Loss: 0.0568\n",
            "Epoch [93/1024], Loss: 0.0567\n",
            "Epoch [94/1024], Loss: 0.0567\n",
            "Epoch [95/1024], Loss: 0.0567\n",
            "Epoch [96/1024], Loss: 0.0567\n",
            "Epoch [97/1024], Loss: 0.0569\n",
            "Epoch [98/1024], Loss: 0.0572\n",
            "Epoch [99/1024], Loss: 0.0570\n",
            "Epoch [100/1024], Loss: 0.0572\n",
            "Epoch [101/1024], Loss: 0.0575\n",
            "Epoch [102/1024], Loss: 0.0573\n",
            "Epoch [103/1024], Loss: 0.0570\n",
            "Epoch [104/1024], Loss: 0.0567\n",
            "Epoch [105/1024], Loss: 0.0566\n",
            "Epoch [106/1024], Loss: 0.0566\n",
            "Epoch [107/1024], Loss: 0.0565\n",
            "Epoch [108/1024], Loss: 0.0564\n",
            "Epoch [109/1024], Loss: 0.0564\n",
            "Epoch [110/1024], Loss: 0.0563\n",
            "Epoch [111/1024], Loss: 0.0563\n",
            "Epoch [112/1024], Loss: 0.0563\n",
            "Epoch [113/1024], Loss: 0.0563\n",
            "Epoch [114/1024], Loss: 0.0562\n",
            "Epoch [115/1024], Loss: 0.0563\n",
            "Epoch [116/1024], Loss: 0.0563\n",
            "Epoch [117/1024], Loss: 0.0566\n",
            "Epoch [118/1024], Loss: 0.0566\n",
            "Epoch [119/1024], Loss: 0.0563\n",
            "Epoch [120/1024], Loss: 0.0562\n",
            "Epoch [121/1024], Loss: 0.0561\n",
            "Epoch [122/1024], Loss: 0.0562\n",
            "Epoch [123/1024], Loss: 0.0561\n",
            "Epoch [124/1024], Loss: 0.0562\n",
            "Epoch [125/1024], Loss: 0.0565\n",
            "Epoch [126/1024], Loss: 0.0565\n",
            "Epoch [127/1024], Loss: 0.0564\n",
            "Epoch [128/1024], Loss: 0.0562\n",
            "Epoch [129/1024], Loss: 0.0563\n",
            "Epoch [130/1024], Loss: 0.0565\n",
            "Epoch [131/1024], Loss: 0.0569\n",
            "Epoch [132/1024], Loss: 0.0566\n",
            "Epoch [133/1024], Loss: 0.0563\n",
            "Epoch [134/1024], Loss: 0.0563\n",
            "Epoch [135/1024], Loss: 0.0563\n",
            "Epoch [136/1024], Loss: 0.0566\n",
            "Epoch [137/1024], Loss: 0.0579\n",
            "Epoch [138/1024], Loss: 0.0573\n",
            "Epoch [139/1024], Loss: 0.0567\n",
            "Epoch [140/1024], Loss: 0.0564\n",
            "Epoch [141/1024], Loss: 0.0562\n",
            "Epoch [142/1024], Loss: 0.0562\n",
            "Epoch [143/1024], Loss: 0.0561\n",
            "Epoch [144/1024], Loss: 0.0561\n",
            "Epoch [145/1024], Loss: 0.0561\n",
            "Epoch [146/1024], Loss: 0.0561\n",
            "Epoch [147/1024], Loss: 0.0565\n",
            "Epoch [148/1024], Loss: 0.0571\n",
            "Epoch [149/1024], Loss: 0.0564\n",
            "Epoch [150/1024], Loss: 0.0561\n",
            "Epoch [151/1024], Loss: 0.0560\n",
            "Epoch [152/1024], Loss: 0.0559\n",
            "Epoch [153/1024], Loss: 0.0558\n",
            "Epoch [154/1024], Loss: 0.0557\n",
            "Epoch [155/1024], Loss: 0.0557\n",
            "Epoch [156/1024], Loss: 0.0557\n",
            "Epoch [157/1024], Loss: 0.0557\n",
            "Epoch [158/1024], Loss: 0.0556\n",
            "Epoch [159/1024], Loss: 0.0556\n",
            "Epoch [160/1024], Loss: 0.0556\n",
            "Epoch [161/1024], Loss: 0.0557\n",
            "Epoch [162/1024], Loss: 0.0556\n",
            "Epoch [163/1024], Loss: 0.0555\n",
            "Epoch [164/1024], Loss: 0.0556\n",
            "Epoch [165/1024], Loss: 0.0556\n",
            "Epoch [166/1024], Loss: 0.0557\n",
            "Epoch [167/1024], Loss: 0.0558\n",
            "Epoch [168/1024], Loss: 0.0558\n",
            "Epoch [169/1024], Loss: 0.0563\n",
            "Epoch [170/1024], Loss: 0.0565\n",
            "Epoch [171/1024], Loss: 0.0562\n",
            "Epoch [172/1024], Loss: 0.0560\n",
            "Epoch [173/1024], Loss: 0.0557\n",
            "Epoch [174/1024], Loss: 0.0557\n",
            "Epoch [175/1024], Loss: 0.0557\n",
            "Epoch [176/1024], Loss: 0.0556\n",
            "Epoch [177/1024], Loss: 0.0558\n",
            "Epoch [178/1024], Loss: 0.0558\n",
            "Epoch [179/1024], Loss: 0.0559\n",
            "Epoch [180/1024], Loss: 0.0561\n",
            "Epoch [181/1024], Loss: 0.0565\n",
            "Epoch [182/1024], Loss: 0.0564\n",
            "Epoch [183/1024], Loss: 0.0564\n",
            "Epoch [184/1024], Loss: 0.0562\n",
            "Epoch [185/1024], Loss: 0.0558\n",
            "Epoch [186/1024], Loss: 0.0557\n",
            "Epoch [187/1024], Loss: 0.0557\n",
            "Epoch [188/1024], Loss: 0.0557\n",
            "Epoch [189/1024], Loss: 0.0562\n",
            "Epoch [190/1024], Loss: 0.0568\n",
            "Epoch [191/1024], Loss: 0.0560\n",
            "Epoch [192/1024], Loss: 0.0557\n",
            "Epoch [193/1024], Loss: 0.0557\n",
            "Epoch [194/1024], Loss: 0.0556\n",
            "Epoch [195/1024], Loss: 0.0555\n",
            "Epoch [196/1024], Loss: 0.0554\n",
            "Epoch [197/1024], Loss: 0.0554\n",
            "Epoch [198/1024], Loss: 0.0553\n",
            "Epoch [199/1024], Loss: 0.0553\n",
            "Epoch [200/1024], Loss: 0.0553\n",
            "Epoch [201/1024], Loss: 0.0553\n",
            "Epoch [202/1024], Loss: 0.0553\n",
            "Epoch [203/1024], Loss: 0.0552\n",
            "Epoch [204/1024], Loss: 0.0553\n",
            "Epoch [205/1024], Loss: 0.0554\n",
            "Epoch [206/1024], Loss: 0.0553\n",
            "Epoch [207/1024], Loss: 0.0553\n",
            "Epoch [208/1024], Loss: 0.0553\n",
            "Epoch [209/1024], Loss: 0.0553\n",
            "Epoch [210/1024], Loss: 0.0554\n",
            "Epoch [211/1024], Loss: 0.0557\n",
            "Epoch [212/1024], Loss: 0.0559\n",
            "Epoch [213/1024], Loss: 0.0561\n",
            "Epoch [214/1024], Loss: 0.0570\n",
            "Epoch [215/1024], Loss: 0.0561\n",
            "Epoch [216/1024], Loss: 0.0558\n",
            "Epoch [217/1024], Loss: 0.0557\n",
            "Epoch [218/1024], Loss: 0.0555\n",
            "Epoch [219/1024], Loss: 0.0554\n",
            "Epoch [220/1024], Loss: 0.0553\n",
            "Epoch [221/1024], Loss: 0.0553\n",
            "Epoch [222/1024], Loss: 0.0553\n",
            "Epoch [223/1024], Loss: 0.0553\n",
            "Epoch [224/1024], Loss: 0.0553\n",
            "Epoch [225/1024], Loss: 0.0553\n",
            "Epoch [226/1024], Loss: 0.0556\n",
            "Epoch [227/1024], Loss: 0.0564\n",
            "Epoch [228/1024], Loss: 0.0564\n",
            "Epoch [229/1024], Loss: 0.0556\n",
            "Epoch [230/1024], Loss: 0.0554\n",
            "Epoch [231/1024], Loss: 0.0553\n",
            "Epoch [232/1024], Loss: 0.0552\n",
            "Epoch [233/1024], Loss: 0.0552\n",
            "Epoch [234/1024], Loss: 0.0552\n",
            "Epoch [235/1024], Loss: 0.0550\n",
            "Epoch [236/1024], Loss: 0.0550\n",
            "Epoch [237/1024], Loss: 0.0549\n",
            "Epoch [238/1024], Loss: 0.0549\n",
            "Epoch [239/1024], Loss: 0.0548\n",
            "Epoch [240/1024], Loss: 0.0548\n",
            "Epoch [241/1024], Loss: 0.0547\n",
            "Epoch [242/1024], Loss: 0.0548\n",
            "Epoch [243/1024], Loss: 0.0548\n",
            "Epoch [244/1024], Loss: 0.0548\n",
            "Epoch [245/1024], Loss: 0.0548\n",
            "Epoch [246/1024], Loss: 0.0549\n",
            "Epoch [247/1024], Loss: 0.0548\n",
            "Epoch [248/1024], Loss: 0.0549\n",
            "Epoch [249/1024], Loss: 0.0549\n",
            "Epoch [250/1024], Loss: 0.0549\n",
            "Epoch [251/1024], Loss: 0.0550\n",
            "Epoch [252/1024], Loss: 0.0553\n",
            "Epoch [253/1024], Loss: 0.0553\n",
            "Epoch [254/1024], Loss: 0.0550\n",
            "Epoch [255/1024], Loss: 0.0549\n",
            "Epoch [256/1024], Loss: 0.0548\n",
            "Epoch [257/1024], Loss: 0.0549\n",
            "Epoch [258/1024], Loss: 0.0549\n",
            "Epoch [259/1024], Loss: 0.0549\n",
            "Epoch [260/1024], Loss: 0.0550\n",
            "Epoch [261/1024], Loss: 0.0552\n",
            "Epoch [262/1024], Loss: 0.0553\n",
            "Epoch [263/1024], Loss: 0.0553\n",
            "Epoch [264/1024], Loss: 0.0550\n",
            "Epoch [265/1024], Loss: 0.0548\n",
            "Epoch [266/1024], Loss: 0.0549\n",
            "Epoch [267/1024], Loss: 0.0548\n",
            "Epoch [268/1024], Loss: 0.0548\n",
            "Epoch [269/1024], Loss: 0.0547\n",
            "Epoch [270/1024], Loss: 0.0547\n",
            "Epoch [271/1024], Loss: 0.0547\n",
            "Epoch [272/1024], Loss: 0.0547\n",
            "Epoch [273/1024], Loss: 0.0547\n",
            "Epoch [274/1024], Loss: 0.0547\n",
            "Epoch [275/1024], Loss: 0.0549\n",
            "Epoch [276/1024], Loss: 0.0549\n",
            "Epoch [277/1024], Loss: 0.0549\n",
            "Epoch [278/1024], Loss: 0.0551\n",
            "Epoch [279/1024], Loss: 0.0552\n",
            "Epoch [280/1024], Loss: 0.0550\n",
            "Epoch [281/1024], Loss: 0.0551\n",
            "Epoch [282/1024], Loss: 0.0552\n",
            "Epoch [283/1024], Loss: 0.0553\n",
            "Epoch [284/1024], Loss: 0.0562\n",
            "Epoch [285/1024], Loss: 0.0560\n",
            "Epoch [286/1024], Loss: 0.0556\n",
            "Epoch [287/1024], Loss: 0.0553\n",
            "Epoch [288/1024], Loss: 0.0555\n",
            "Epoch [289/1024], Loss: 0.0555\n",
            "Epoch [290/1024], Loss: 0.0550\n",
            "Epoch [291/1024], Loss: 0.0547\n",
            "Epoch [292/1024], Loss: 0.0546\n",
            "Epoch [293/1024], Loss: 0.0545\n",
            "Epoch [294/1024], Loss: 0.0545\n",
            "Epoch [295/1024], Loss: 0.0545\n",
            "Epoch [296/1024], Loss: 0.0545\n",
            "Epoch [297/1024], Loss: 0.0545\n",
            "Epoch [298/1024], Loss: 0.0546\n",
            "Epoch [299/1024], Loss: 0.0546\n",
            "Epoch [300/1024], Loss: 0.0545\n",
            "Epoch [301/1024], Loss: 0.0547\n",
            "Epoch [302/1024], Loss: 0.0546\n",
            "Epoch [303/1024], Loss: 0.0548\n",
            "Epoch [304/1024], Loss: 0.0547\n",
            "Epoch [305/1024], Loss: 0.0547\n",
            "Epoch [306/1024], Loss: 0.0548\n",
            "Epoch [307/1024], Loss: 0.0547\n",
            "Epoch [308/1024], Loss: 0.0551\n",
            "Epoch [309/1024], Loss: 0.0550\n",
            "Epoch [310/1024], Loss: 0.0552\n",
            "Epoch [311/1024], Loss: 0.0552\n",
            "Epoch [312/1024], Loss: 0.0550\n",
            "Epoch [313/1024], Loss: 0.0546\n",
            "Epoch [314/1024], Loss: 0.0547\n",
            "Epoch [315/1024], Loss: 0.0546\n",
            "Epoch [316/1024], Loss: 0.0546\n",
            "Epoch [317/1024], Loss: 0.0547\n",
            "Epoch [318/1024], Loss: 0.0546\n",
            "Epoch [319/1024], Loss: 0.0546\n",
            "Epoch [320/1024], Loss: 0.0547\n",
            "Epoch [321/1024], Loss: 0.0548\n",
            "Epoch [322/1024], Loss: 0.0548\n",
            "Epoch [323/1024], Loss: 0.0549\n",
            "Epoch [324/1024], Loss: 0.0552\n",
            "Epoch [325/1024], Loss: 0.0553\n",
            "Epoch [326/1024], Loss: 0.0557\n",
            "Epoch [327/1024], Loss: 0.0564\n",
            "Epoch [328/1024], Loss: 0.0561\n",
            "Epoch [329/1024], Loss: 0.0556\n",
            "Epoch [330/1024], Loss: 0.0556\n",
            "Epoch [331/1024], Loss: 0.0557\n",
            "Epoch [332/1024], Loss: 0.0552\n",
            "Epoch [333/1024], Loss: 0.0550\n",
            "Epoch [334/1024], Loss: 0.0548\n",
            "Epoch [335/1024], Loss: 0.0547\n",
            "Epoch [336/1024], Loss: 0.0548\n",
            "Epoch [337/1024], Loss: 0.0548\n",
            "Epoch [338/1024], Loss: 0.0547\n",
            "Epoch [339/1024], Loss: 0.0546\n",
            "Epoch [340/1024], Loss: 0.0547\n",
            "Epoch [341/1024], Loss: 0.0545\n",
            "Epoch [342/1024], Loss: 0.0545\n",
            "Epoch [343/1024], Loss: 0.0547\n",
            "Epoch [344/1024], Loss: 0.0546\n",
            "Epoch [345/1024], Loss: 0.0546\n",
            "Epoch [346/1024], Loss: 0.0544\n",
            "Epoch [347/1024], Loss: 0.0544\n",
            "Epoch [348/1024], Loss: 0.0544\n",
            "Epoch [349/1024], Loss: 0.0544\n",
            "Epoch [350/1024], Loss: 0.0545\n",
            "Epoch [351/1024], Loss: 0.0544\n",
            "Epoch [352/1024], Loss: 0.0544\n",
            "Epoch [353/1024], Loss: 0.0545\n",
            "Epoch [354/1024], Loss: 0.0545\n",
            "Epoch [355/1024], Loss: 0.0548\n",
            "Epoch [356/1024], Loss: 0.0548\n",
            "Epoch [357/1024], Loss: 0.0548\n",
            "Epoch [358/1024], Loss: 0.0547\n",
            "Epoch [359/1024], Loss: 0.0548\n",
            "Epoch [360/1024], Loss: 0.0549\n",
            "Epoch [361/1024], Loss: 0.0546\n",
            "Epoch [362/1024], Loss: 0.0544\n",
            "Epoch [363/1024], Loss: 0.0544\n",
            "Epoch [364/1024], Loss: 0.0545\n",
            "Epoch [365/1024], Loss: 0.0546\n",
            "Epoch [366/1024], Loss: 0.0544\n",
            "Epoch [367/1024], Loss: 0.0546\n",
            "Epoch [368/1024], Loss: 0.0544\n",
            "Epoch [369/1024], Loss: 0.0543\n",
            "Epoch [370/1024], Loss: 0.0544\n",
            "Epoch [371/1024], Loss: 0.0543\n",
            "Epoch [372/1024], Loss: 0.0546\n",
            "Epoch [373/1024], Loss: 0.0545\n",
            "Epoch [374/1024], Loss: 0.0549\n",
            "Epoch [375/1024], Loss: 0.0549\n",
            "Epoch [376/1024], Loss: 0.0551\n",
            "Epoch [377/1024], Loss: 0.0552\n",
            "Epoch [378/1024], Loss: 0.0548\n",
            "Epoch [379/1024], Loss: 0.0546\n",
            "Epoch [380/1024], Loss: 0.0549\n",
            "Epoch [381/1024], Loss: 0.0551\n",
            "Epoch [382/1024], Loss: 0.0552\n",
            "Epoch [383/1024], Loss: 0.0555\n",
            "Epoch [384/1024], Loss: 0.0558\n",
            "Epoch [385/1024], Loss: 0.0557\n",
            "Epoch [386/1024], Loss: 0.0556\n",
            "Epoch [387/1024], Loss: 0.0554\n",
            "Epoch [388/1024], Loss: 0.0556\n",
            "Epoch [389/1024], Loss: 0.0561\n",
            "Epoch [390/1024], Loss: 0.0559\n",
            "Epoch [391/1024], Loss: 0.0554\n",
            "Epoch [392/1024], Loss: 0.0550\n",
            "Epoch [393/1024], Loss: 0.0547\n",
            "Epoch [394/1024], Loss: 0.0545\n",
            "Epoch [395/1024], Loss: 0.0543\n",
            "Epoch [396/1024], Loss: 0.0542\n",
            "Epoch [397/1024], Loss: 0.0542\n",
            "Epoch [398/1024], Loss: 0.0543\n",
            "Epoch [399/1024], Loss: 0.0542\n",
            "Epoch [400/1024], Loss: 0.0542\n",
            "Epoch [401/1024], Loss: 0.0541\n",
            "Epoch [402/1024], Loss: 0.0541\n",
            "Epoch [403/1024], Loss: 0.0540\n",
            "Epoch [404/1024], Loss: 0.0542\n",
            "Epoch [405/1024], Loss: 0.0543\n",
            "Epoch [406/1024], Loss: 0.0542\n",
            "Epoch [407/1024], Loss: 0.0543\n",
            "Epoch [408/1024], Loss: 0.0546\n",
            "Epoch [409/1024], Loss: 0.0544\n",
            "Epoch [410/1024], Loss: 0.0542\n",
            "Epoch [411/1024], Loss: 0.0541\n",
            "Epoch [412/1024], Loss: 0.0540\n",
            "Epoch [413/1024], Loss: 0.0540\n",
            "Epoch [414/1024], Loss: 0.0541\n",
            "Epoch [415/1024], Loss: 0.0542\n",
            "Epoch [416/1024], Loss: 0.0542\n",
            "Epoch [417/1024], Loss: 0.0546\n",
            "Epoch [418/1024], Loss: 0.0545\n",
            "Epoch [419/1024], Loss: 0.0545\n",
            "Epoch [420/1024], Loss: 0.0545\n",
            "Epoch [421/1024], Loss: 0.0548\n",
            "Epoch [422/1024], Loss: 0.0547\n",
            "Epoch [423/1024], Loss: 0.0547\n",
            "Epoch [424/1024], Loss: 0.0548\n",
            "Epoch [425/1024], Loss: 0.0547\n",
            "Epoch [426/1024], Loss: 0.0544\n",
            "Epoch [427/1024], Loss: 0.0545\n",
            "Epoch [428/1024], Loss: 0.0548\n",
            "Epoch [429/1024], Loss: 0.0549\n",
            "Epoch [430/1024], Loss: 0.0544\n",
            "Epoch [431/1024], Loss: 0.0545\n",
            "Epoch [432/1024], Loss: 0.0546\n",
            "Epoch [433/1024], Loss: 0.0548\n",
            "Epoch [434/1024], Loss: 0.0547\n",
            "Epoch [435/1024], Loss: 0.0546\n",
            "Epoch [436/1024], Loss: 0.0545\n",
            "Epoch [437/1024], Loss: 0.0547\n",
            "Epoch [438/1024], Loss: 0.0551\n",
            "Epoch [439/1024], Loss: 0.0551\n",
            "Epoch [440/1024], Loss: 0.0553\n",
            "Epoch [441/1024], Loss: 0.0551\n",
            "Epoch [442/1024], Loss: 0.0552\n",
            "Epoch [443/1024], Loss: 0.0551\n",
            "Epoch [444/1024], Loss: 0.0546\n",
            "Epoch [445/1024], Loss: 0.0544\n",
            "Epoch [446/1024], Loss: 0.0543\n",
            "Epoch [447/1024], Loss: 0.0543\n",
            "Epoch [448/1024], Loss: 0.0542\n",
            "Epoch [449/1024], Loss: 0.0541\n",
            "Epoch [450/1024], Loss: 0.0541\n",
            "Epoch [451/1024], Loss: 0.0541\n",
            "Epoch [452/1024], Loss: 0.0540\n",
            "Epoch [453/1024], Loss: 0.0541\n",
            "Epoch [454/1024], Loss: 0.0541\n",
            "Epoch [455/1024], Loss: 0.0545\n",
            "Epoch [456/1024], Loss: 0.0546\n",
            "Epoch [457/1024], Loss: 0.0543\n",
            "Epoch [458/1024], Loss: 0.0541\n",
            "Epoch [459/1024], Loss: 0.0540\n",
            "Epoch [460/1024], Loss: 0.0540\n",
            "Epoch [461/1024], Loss: 0.0541\n",
            "Epoch [462/1024], Loss: 0.0543\n",
            "Epoch [463/1024], Loss: 0.0547\n",
            "Epoch [464/1024], Loss: 0.0547\n",
            "Epoch [465/1024], Loss: 0.0550\n",
            "Epoch [466/1024], Loss: 0.0545\n",
            "Epoch [467/1024], Loss: 0.0542\n",
            "Epoch [468/1024], Loss: 0.0541\n",
            "Epoch [469/1024], Loss: 0.0543\n",
            "Epoch [470/1024], Loss: 0.0542\n",
            "Epoch [471/1024], Loss: 0.0541\n",
            "Epoch [472/1024], Loss: 0.0541\n",
            "Epoch [473/1024], Loss: 0.0541\n",
            "Epoch [474/1024], Loss: 0.0542\n",
            "Epoch [475/1024], Loss: 0.0542\n",
            "Epoch [476/1024], Loss: 0.0545\n",
            "Epoch [477/1024], Loss: 0.0546\n",
            "Epoch [478/1024], Loss: 0.0549\n",
            "Epoch [479/1024], Loss: 0.0546\n",
            "Epoch [480/1024], Loss: 0.0544\n",
            "Epoch [481/1024], Loss: 0.0543\n",
            "Epoch [482/1024], Loss: 0.0542\n",
            "Epoch [483/1024], Loss: 0.0542\n",
            "Epoch [484/1024], Loss: 0.0540\n",
            "Epoch [485/1024], Loss: 0.0540\n",
            "Epoch [486/1024], Loss: 0.0540\n",
            "Epoch [487/1024], Loss: 0.0540\n",
            "Epoch [488/1024], Loss: 0.0543\n",
            "Epoch [489/1024], Loss: 0.0544\n",
            "Epoch [490/1024], Loss: 0.0543\n",
            "Epoch [491/1024], Loss: 0.0547\n",
            "Epoch [492/1024], Loss: 0.0553\n",
            "Epoch [493/1024], Loss: 0.0553\n",
            "Epoch [494/1024], Loss: 0.0557\n",
            "Epoch [495/1024], Loss: 0.0552\n",
            "Epoch [496/1024], Loss: 0.0554\n",
            "Epoch [497/1024], Loss: 0.0552\n",
            "Epoch [498/1024], Loss: 0.0550\n",
            "Epoch [499/1024], Loss: 0.0553\n",
            "Epoch [500/1024], Loss: 0.0550\n",
            "Epoch [501/1024], Loss: 0.0550\n",
            "Epoch [502/1024], Loss: 0.0555\n",
            "Epoch [503/1024], Loss: 0.0559\n",
            "Epoch [504/1024], Loss: 0.0559\n",
            "Epoch [505/1024], Loss: 0.0551\n",
            "Epoch [506/1024], Loss: 0.0549\n",
            "Epoch [507/1024], Loss: 0.0545\n",
            "Epoch [508/1024], Loss: 0.0543\n",
            "Epoch [509/1024], Loss: 0.0545\n",
            "Epoch [510/1024], Loss: 0.0545\n",
            "Epoch [511/1024], Loss: 0.0545\n",
            "Epoch [512/1024], Loss: 0.0545\n",
            "Epoch [513/1024], Loss: 0.0544\n",
            "Epoch [514/1024], Loss: 0.0544\n",
            "Epoch [515/1024], Loss: 0.0544\n",
            "Epoch [516/1024], Loss: 0.0544\n",
            "Epoch [517/1024], Loss: 0.0542\n",
            "Epoch [518/1024], Loss: 0.0542\n",
            "Epoch [519/1024], Loss: 0.0541\n",
            "Epoch [520/1024], Loss: 0.0540\n",
            "Epoch [521/1024], Loss: 0.0538\n",
            "Epoch [522/1024], Loss: 0.0539\n",
            "Epoch [523/1024], Loss: 0.0538\n",
            "Epoch [524/1024], Loss: 0.0538\n",
            "Epoch [525/1024], Loss: 0.0537\n",
            "Epoch [526/1024], Loss: 0.0537\n",
            "Epoch [527/1024], Loss: 0.0537\n",
            "Epoch [528/1024], Loss: 0.0538\n",
            "Epoch [529/1024], Loss: 0.0538\n",
            "Epoch [530/1024], Loss: 0.0540\n",
            "Epoch [531/1024], Loss: 0.0539\n",
            "Epoch [532/1024], Loss: 0.0540\n",
            "Epoch [533/1024], Loss: 0.0542\n",
            "Epoch [534/1024], Loss: 0.0548\n",
            "Epoch [535/1024], Loss: 0.0547\n",
            "Epoch [536/1024], Loss: 0.0541\n",
            "Epoch [537/1024], Loss: 0.0540\n",
            "Epoch [538/1024], Loss: 0.0540\n",
            "Epoch [539/1024], Loss: 0.0542\n",
            "Epoch [540/1024], Loss: 0.0548\n",
            "Epoch [541/1024], Loss: 0.0557\n",
            "Epoch [542/1024], Loss: 0.0553\n",
            "Epoch [543/1024], Loss: 0.0549\n",
            "Epoch [544/1024], Loss: 0.0544\n",
            "Epoch [545/1024], Loss: 0.0546\n",
            "Epoch [546/1024], Loss: 0.0543\n",
            "Epoch [547/1024], Loss: 0.0541\n",
            "Epoch [548/1024], Loss: 0.0541\n",
            "Epoch [549/1024], Loss: 0.0542\n",
            "Epoch [550/1024], Loss: 0.0541\n",
            "Epoch [551/1024], Loss: 0.0539\n",
            "Epoch [552/1024], Loss: 0.0538\n",
            "Epoch [553/1024], Loss: 0.0538\n",
            "Epoch [554/1024], Loss: 0.0536\n",
            "Epoch [555/1024], Loss: 0.0536\n",
            "Epoch [556/1024], Loss: 0.0536\n",
            "Epoch [557/1024], Loss: 0.0536\n",
            "Epoch [558/1024], Loss: 0.0536\n",
            "Epoch [559/1024], Loss: 0.0536\n",
            "Epoch [560/1024], Loss: 0.0536\n",
            "Epoch [561/1024], Loss: 0.0540\n",
            "Epoch [562/1024], Loss: 0.0537\n",
            "Epoch [563/1024], Loss: 0.0537\n",
            "Epoch [564/1024], Loss: 0.0535\n",
            "Epoch [565/1024], Loss: 0.0537\n",
            "Epoch [566/1024], Loss: 0.0539\n",
            "Epoch [567/1024], Loss: 0.0539\n",
            "Epoch [568/1024], Loss: 0.0537\n",
            "Epoch [569/1024], Loss: 0.0538\n",
            "Epoch [570/1024], Loss: 0.0540\n",
            "Epoch [571/1024], Loss: 0.0542\n",
            "Epoch [572/1024], Loss: 0.0542\n",
            "Epoch [573/1024], Loss: 0.0540\n",
            "Epoch [574/1024], Loss: 0.0538\n",
            "Epoch [575/1024], Loss: 0.0537\n",
            "Epoch [576/1024], Loss: 0.0538\n",
            "Epoch [577/1024], Loss: 0.0540\n",
            "Epoch [578/1024], Loss: 0.0540\n",
            "Epoch [579/1024], Loss: 0.0538\n",
            "Epoch [580/1024], Loss: 0.0541\n",
            "Epoch [581/1024], Loss: 0.0541\n",
            "Epoch [582/1024], Loss: 0.0542\n",
            "Epoch [583/1024], Loss: 0.0543\n",
            "Epoch [584/1024], Loss: 0.0551\n",
            "Epoch [585/1024], Loss: 0.0550\n",
            "Epoch [586/1024], Loss: 0.0546\n",
            "Epoch [587/1024], Loss: 0.0540\n",
            "Epoch [588/1024], Loss: 0.0539\n",
            "Epoch [589/1024], Loss: 0.0539\n",
            "Epoch [590/1024], Loss: 0.0539\n",
            "Epoch [591/1024], Loss: 0.0539\n",
            "Epoch [592/1024], Loss: 0.0540\n",
            "Epoch [593/1024], Loss: 0.0541\n",
            "Epoch [594/1024], Loss: 0.0540\n",
            "Epoch [595/1024], Loss: 0.0541\n",
            "Epoch [596/1024], Loss: 0.0543\n",
            "Epoch [597/1024], Loss: 0.0545\n",
            "Epoch [598/1024], Loss: 0.0553\n",
            "Epoch [599/1024], Loss: 0.0559\n",
            "Epoch [600/1024], Loss: 0.0552\n",
            "Epoch [601/1024], Loss: 0.0547\n",
            "Epoch [602/1024], Loss: 0.0543\n",
            "Epoch [603/1024], Loss: 0.0540\n",
            "Epoch [604/1024], Loss: 0.0540\n",
            "Epoch [605/1024], Loss: 0.0539\n",
            "Epoch [606/1024], Loss: 0.0540\n",
            "Epoch [607/1024], Loss: 0.0540\n",
            "Epoch [608/1024], Loss: 0.0544\n",
            "Epoch [609/1024], Loss: 0.0542\n",
            "Epoch [610/1024], Loss: 0.0538\n",
            "Epoch [611/1024], Loss: 0.0539\n",
            "Epoch [612/1024], Loss: 0.0538\n",
            "Epoch [613/1024], Loss: 0.0544\n",
            "Epoch [614/1024], Loss: 0.0541\n",
            "Epoch [615/1024], Loss: 0.0545\n",
            "Epoch [616/1024], Loss: 0.0544\n",
            "Epoch [617/1024], Loss: 0.0545\n",
            "Epoch [618/1024], Loss: 0.0540\n",
            "Epoch [619/1024], Loss: 0.0540\n",
            "Epoch [620/1024], Loss: 0.0538\n",
            "Epoch [621/1024], Loss: 0.0536\n",
            "Epoch [622/1024], Loss: 0.0537\n",
            "Epoch [623/1024], Loss: 0.0538\n",
            "Epoch [624/1024], Loss: 0.0539\n",
            "Epoch [625/1024], Loss: 0.0541\n",
            "Epoch [626/1024], Loss: 0.0544\n",
            "Epoch [627/1024], Loss: 0.0541\n",
            "Epoch [628/1024], Loss: 0.0541\n",
            "Epoch [629/1024], Loss: 0.0538\n",
            "Epoch [630/1024], Loss: 0.0539\n",
            "Epoch [631/1024], Loss: 0.0540\n",
            "Epoch [632/1024], Loss: 0.0541\n",
            "Epoch [633/1024], Loss: 0.0538\n",
            "Epoch [634/1024], Loss: 0.0537\n",
            "Epoch [635/1024], Loss: 0.0538\n",
            "Epoch [636/1024], Loss: 0.0538\n",
            "Epoch [637/1024], Loss: 0.0537\n",
            "Epoch [638/1024], Loss: 0.0537\n",
            "Epoch [639/1024], Loss: 0.0538\n",
            "Epoch [640/1024], Loss: 0.0538\n",
            "Epoch [641/1024], Loss: 0.0539\n",
            "Epoch [642/1024], Loss: 0.0539\n",
            "Epoch [643/1024], Loss: 0.0538\n",
            "Epoch [644/1024], Loss: 0.0539\n",
            "Epoch [645/1024], Loss: 0.0540\n",
            "Epoch [646/1024], Loss: 0.0544\n",
            "Epoch [647/1024], Loss: 0.0551\n",
            "Epoch [648/1024], Loss: 0.0564\n",
            "Epoch [649/1024], Loss: 0.0559\n",
            "Epoch [650/1024], Loss: 0.0551\n",
            "Epoch [651/1024], Loss: 0.0543\n",
            "Epoch [652/1024], Loss: 0.0539\n",
            "Epoch [653/1024], Loss: 0.0540\n",
            "Epoch [654/1024], Loss: 0.0540\n",
            "Epoch [655/1024], Loss: 0.0542\n",
            "Epoch [656/1024], Loss: 0.0542\n",
            "Epoch [657/1024], Loss: 0.0538\n",
            "Epoch [658/1024], Loss: 0.0545\n",
            "Epoch [659/1024], Loss: 0.0542\n",
            "Epoch [660/1024], Loss: 0.0540\n",
            "Epoch [661/1024], Loss: 0.0541\n",
            "Epoch [662/1024], Loss: 0.0541\n",
            "Epoch [663/1024], Loss: 0.0539\n",
            "Epoch [664/1024], Loss: 0.0539\n",
            "Epoch [665/1024], Loss: 0.0539\n",
            "Epoch [666/1024], Loss: 0.0540\n",
            "Epoch [667/1024], Loss: 0.0540\n",
            "Epoch [668/1024], Loss: 0.0538\n",
            "Epoch [669/1024], Loss: 0.0535\n",
            "Epoch [670/1024], Loss: 0.0534\n",
            "Epoch [671/1024], Loss: 0.0534\n",
            "Epoch [672/1024], Loss: 0.0533\n",
            "Epoch [673/1024], Loss: 0.0535\n",
            "Epoch [674/1024], Loss: 0.0532\n",
            "Epoch [675/1024], Loss: 0.0533\n",
            "Epoch [676/1024], Loss: 0.0533\n",
            "Epoch [677/1024], Loss: 0.0533\n",
            "Epoch [678/1024], Loss: 0.0533\n",
            "Epoch [679/1024], Loss: 0.0538\n",
            "Epoch [680/1024], Loss: 0.0534\n",
            "Epoch [681/1024], Loss: 0.0536\n",
            "Epoch [682/1024], Loss: 0.0540\n",
            "Epoch [683/1024], Loss: 0.0540\n",
            "Epoch [684/1024], Loss: 0.0540\n",
            "Epoch [685/1024], Loss: 0.0540\n",
            "Epoch [686/1024], Loss: 0.0543\n",
            "Epoch [687/1024], Loss: 0.0542\n",
            "Epoch [688/1024], Loss: 0.0544\n",
            "Epoch [689/1024], Loss: 0.0549\n",
            "Epoch [690/1024], Loss: 0.0554\n",
            "Epoch [691/1024], Loss: 0.0557\n",
            "Epoch [692/1024], Loss: 0.0551\n",
            "Epoch [693/1024], Loss: 0.0543\n",
            "Epoch [694/1024], Loss: 0.0540\n",
            "Epoch [695/1024], Loss: 0.0537\n",
            "Epoch [696/1024], Loss: 0.0536\n",
            "Epoch [697/1024], Loss: 0.0535\n",
            "Epoch [698/1024], Loss: 0.0537\n",
            "Epoch [699/1024], Loss: 0.0540\n",
            "Epoch [700/1024], Loss: 0.0541\n",
            "Epoch [701/1024], Loss: 0.0548\n",
            "Epoch [702/1024], Loss: 0.0546\n",
            "Epoch [703/1024], Loss: 0.0542\n",
            "Epoch [704/1024], Loss: 0.0538\n",
            "Epoch [705/1024], Loss: 0.0536\n",
            "Epoch [706/1024], Loss: 0.0537\n",
            "Epoch [707/1024], Loss: 0.0538\n",
            "Epoch [708/1024], Loss: 0.0538\n",
            "Epoch [709/1024], Loss: 0.0539\n",
            "Epoch [710/1024], Loss: 0.0538\n",
            "Epoch [711/1024], Loss: 0.0539\n",
            "Epoch [712/1024], Loss: 0.0539\n",
            "Epoch [713/1024], Loss: 0.0544\n",
            "Epoch [714/1024], Loss: 0.0549\n",
            "Epoch [715/1024], Loss: 0.0551\n",
            "Epoch [716/1024], Loss: 0.0548\n",
            "Epoch [717/1024], Loss: 0.0542\n",
            "Epoch [718/1024], Loss: 0.0544\n",
            "Epoch [719/1024], Loss: 0.0540\n",
            "Epoch [720/1024], Loss: 0.0542\n",
            "Epoch [721/1024], Loss: 0.0544\n",
            "Epoch [722/1024], Loss: 0.0543\n",
            "Epoch [723/1024], Loss: 0.0543\n",
            "Epoch [724/1024], Loss: 0.0547\n",
            "Epoch [725/1024], Loss: 0.0551\n",
            "Epoch [726/1024], Loss: 0.0551\n",
            "Epoch [727/1024], Loss: 0.0555\n",
            "Epoch [728/1024], Loss: 0.0552\n",
            "Epoch [729/1024], Loss: 0.0544\n",
            "Epoch [730/1024], Loss: 0.0541\n",
            "Epoch [731/1024], Loss: 0.0538\n",
            "Epoch [732/1024], Loss: 0.0537\n",
            "Epoch [733/1024], Loss: 0.0537\n",
            "Epoch [734/1024], Loss: 0.0534\n",
            "Epoch [735/1024], Loss: 0.0533\n",
            "Epoch [736/1024], Loss: 0.0533\n",
            "Epoch [737/1024], Loss: 0.0531\n",
            "Epoch [738/1024], Loss: 0.0531\n",
            "Epoch [739/1024], Loss: 0.0532\n",
            "Epoch [740/1024], Loss: 0.0533\n",
            "Epoch [741/1024], Loss: 0.0533\n",
            "Epoch [742/1024], Loss: 0.0535\n",
            "Epoch [743/1024], Loss: 0.0535\n",
            "Epoch [744/1024], Loss: 0.0533\n",
            "Epoch [745/1024], Loss: 0.0531\n",
            "Epoch [746/1024], Loss: 0.0532\n",
            "Epoch [747/1024], Loss: 0.0532\n",
            "Epoch [748/1024], Loss: 0.0533\n",
            "Epoch [749/1024], Loss: 0.0533\n",
            "Epoch [750/1024], Loss: 0.0530\n",
            "Epoch [751/1024], Loss: 0.0532\n",
            "Epoch [752/1024], Loss: 0.0536\n",
            "Epoch [753/1024], Loss: 0.0537\n",
            "Epoch [754/1024], Loss: 0.0539\n",
            "Epoch [755/1024], Loss: 0.0544\n",
            "Epoch [756/1024], Loss: 0.0541\n",
            "Epoch [757/1024], Loss: 0.0538\n",
            "Epoch [758/1024], Loss: 0.0535\n",
            "Epoch [759/1024], Loss: 0.0536\n",
            "Epoch [760/1024], Loss: 0.0535\n",
            "Epoch [761/1024], Loss: 0.0539\n",
            "Epoch [762/1024], Loss: 0.0534\n",
            "Epoch [763/1024], Loss: 0.0535\n",
            "Epoch [764/1024], Loss: 0.0542\n",
            "Epoch [765/1024], Loss: 0.0550\n",
            "Epoch [766/1024], Loss: 0.0544\n",
            "Epoch [767/1024], Loss: 0.0537\n",
            "Epoch [768/1024], Loss: 0.0534\n",
            "Epoch [769/1024], Loss: 0.0533\n",
            "Epoch [770/1024], Loss: 0.0534\n",
            "Epoch [771/1024], Loss: 0.0534\n",
            "Epoch [772/1024], Loss: 0.0534\n",
            "Epoch [773/1024], Loss: 0.0534\n",
            "Epoch [774/1024], Loss: 0.0535\n",
            "Epoch [775/1024], Loss: 0.0535\n",
            "Epoch [776/1024], Loss: 0.0534\n",
            "Epoch [777/1024], Loss: 0.0533\n",
            "Epoch [778/1024], Loss: 0.0535\n",
            "Epoch [779/1024], Loss: 0.0537\n",
            "Epoch [780/1024], Loss: 0.0539\n",
            "Epoch [781/1024], Loss: 0.0538\n",
            "Epoch [782/1024], Loss: 0.0535\n",
            "Epoch [783/1024], Loss: 0.0532\n",
            "Epoch [784/1024], Loss: 0.0531\n",
            "Epoch [785/1024], Loss: 0.0530\n",
            "Epoch [786/1024], Loss: 0.0531\n",
            "Epoch [787/1024], Loss: 0.0530\n",
            "Epoch [788/1024], Loss: 0.0530\n",
            "Epoch [789/1024], Loss: 0.0532\n",
            "Epoch [790/1024], Loss: 0.0532\n",
            "Epoch [791/1024], Loss: 0.0531\n",
            "Epoch [792/1024], Loss: 0.0529\n",
            "Epoch [793/1024], Loss: 0.0534\n",
            "Epoch [794/1024], Loss: 0.0533\n",
            "Epoch [795/1024], Loss: 0.0533\n",
            "Epoch [796/1024], Loss: 0.0536\n",
            "Epoch [797/1024], Loss: 0.0541\n",
            "Epoch [798/1024], Loss: 0.0541\n",
            "Epoch [799/1024], Loss: 0.0540\n",
            "Epoch [800/1024], Loss: 0.0541\n",
            "Epoch [801/1024], Loss: 0.0540\n",
            "Epoch [802/1024], Loss: 0.0535\n",
            "Epoch [803/1024], Loss: 0.0534\n",
            "Epoch [804/1024], Loss: 0.0537\n",
            "Epoch [805/1024], Loss: 0.0540\n",
            "Epoch [806/1024], Loss: 0.0542\n",
            "Epoch [807/1024], Loss: 0.0550\n",
            "Epoch [808/1024], Loss: 0.0549\n",
            "Epoch [809/1024], Loss: 0.0545\n",
            "Epoch [810/1024], Loss: 0.0537\n",
            "Epoch [811/1024], Loss: 0.0533\n",
            "Epoch [812/1024], Loss: 0.0533\n",
            "Epoch [813/1024], Loss: 0.0531\n",
            "Epoch [814/1024], Loss: 0.0532\n",
            "Epoch [815/1024], Loss: 0.0531\n",
            "Epoch [816/1024], Loss: 0.0531\n",
            "Epoch [817/1024], Loss: 0.0530\n",
            "Epoch [818/1024], Loss: 0.0530\n",
            "Epoch [819/1024], Loss: 0.0531\n",
            "Epoch [820/1024], Loss: 0.0532\n",
            "Epoch [821/1024], Loss: 0.0533\n",
            "Epoch [822/1024], Loss: 0.0532\n",
            "Epoch [823/1024], Loss: 0.0532\n",
            "Epoch [824/1024], Loss: 0.0534\n",
            "Epoch [825/1024], Loss: 0.0535\n",
            "Epoch [826/1024], Loss: 0.0538\n",
            "Epoch [827/1024], Loss: 0.0540\n",
            "Epoch [828/1024], Loss: 0.0535\n",
            "Epoch [829/1024], Loss: 0.0533\n",
            "Epoch [830/1024], Loss: 0.0532\n",
            "Epoch [831/1024], Loss: 0.0530\n",
            "Epoch [832/1024], Loss: 0.0531\n",
            "Epoch [833/1024], Loss: 0.0534\n",
            "Epoch [834/1024], Loss: 0.0535\n",
            "Epoch [835/1024], Loss: 0.0537\n",
            "Epoch [836/1024], Loss: 0.0540\n",
            "Epoch [837/1024], Loss: 0.0541\n",
            "Epoch [838/1024], Loss: 0.0546\n",
            "Epoch [839/1024], Loss: 0.0548\n",
            "Epoch [840/1024], Loss: 0.0547\n",
            "Epoch [841/1024], Loss: 0.0543\n",
            "Epoch [842/1024], Loss: 0.0549\n",
            "Epoch [843/1024], Loss: 0.0551\n",
            "Epoch [844/1024], Loss: 0.0549\n",
            "Epoch [845/1024], Loss: 0.0543\n",
            "Epoch [846/1024], Loss: 0.0539\n",
            "Epoch [847/1024], Loss: 0.0534\n",
            "Epoch [848/1024], Loss: 0.0530\n",
            "Epoch [849/1024], Loss: 0.0528\n",
            "Epoch [850/1024], Loss: 0.0528\n",
            "Epoch [851/1024], Loss: 0.0529\n",
            "Epoch [852/1024], Loss: 0.0529\n",
            "Epoch [853/1024], Loss: 0.0527\n",
            "Epoch [854/1024], Loss: 0.0527\n",
            "Epoch [855/1024], Loss: 0.0530\n",
            "Epoch [856/1024], Loss: 0.0531\n",
            "Epoch [857/1024], Loss: 0.0531\n",
            "Epoch [858/1024], Loss: 0.0530\n",
            "Epoch [859/1024], Loss: 0.0528\n",
            "Epoch [860/1024], Loss: 0.0527\n",
            "Epoch [861/1024], Loss: 0.0530\n",
            "Epoch [862/1024], Loss: 0.0530\n",
            "Epoch [863/1024], Loss: 0.0531\n",
            "Epoch [864/1024], Loss: 0.0532\n",
            "Epoch [865/1024], Loss: 0.0538\n",
            "Epoch [866/1024], Loss: 0.0541\n",
            "Epoch [867/1024], Loss: 0.0539\n",
            "Epoch [868/1024], Loss: 0.0538\n",
            "Epoch [869/1024], Loss: 0.0537\n",
            "Epoch [870/1024], Loss: 0.0541\n",
            "Epoch [871/1024], Loss: 0.0546\n",
            "Epoch [872/1024], Loss: 0.0544\n",
            "Epoch [873/1024], Loss: 0.0541\n",
            "Epoch [874/1024], Loss: 0.0539\n",
            "Epoch [875/1024], Loss: 0.0539\n",
            "Epoch [876/1024], Loss: 0.0540\n",
            "Epoch [877/1024], Loss: 0.0543\n",
            "Epoch [878/1024], Loss: 0.0549\n",
            "Epoch [879/1024], Loss: 0.0552\n",
            "Epoch [880/1024], Loss: 0.0560\n",
            "Epoch [881/1024], Loss: 0.0556\n",
            "Epoch [882/1024], Loss: 0.0554\n",
            "Epoch [883/1024], Loss: 0.0543\n",
            "Epoch [884/1024], Loss: 0.0537\n",
            "Epoch [885/1024], Loss: 0.0535\n",
            "Epoch [886/1024], Loss: 0.0532\n",
            "Epoch [887/1024], Loss: 0.0530\n",
            "Epoch [888/1024], Loss: 0.0528\n",
            "Epoch [889/1024], Loss: 0.0527\n",
            "Epoch [890/1024], Loss: 0.0527\n",
            "Epoch [891/1024], Loss: 0.0528\n",
            "Epoch [892/1024], Loss: 0.0527\n",
            "Epoch [893/1024], Loss: 0.0527\n",
            "Epoch [894/1024], Loss: 0.0525\n",
            "Epoch [895/1024], Loss: 0.0525\n",
            "Epoch [896/1024], Loss: 0.0526\n",
            "Epoch [897/1024], Loss: 0.0526\n",
            "Epoch [898/1024], Loss: 0.0524\n",
            "Epoch [899/1024], Loss: 0.0525\n",
            "Epoch [900/1024], Loss: 0.0525\n",
            "Epoch [901/1024], Loss: 0.0526\n",
            "Epoch [902/1024], Loss: 0.0526\n",
            "Epoch [903/1024], Loss: 0.0530\n",
            "Epoch [904/1024], Loss: 0.0527\n",
            "Epoch [905/1024], Loss: 0.0529\n",
            "Epoch [906/1024], Loss: 0.0534\n",
            "Epoch [907/1024], Loss: 0.0533\n",
            "Epoch [908/1024], Loss: 0.0528\n",
            "Epoch [909/1024], Loss: 0.0525\n",
            "Epoch [910/1024], Loss: 0.0527\n",
            "Epoch [911/1024], Loss: 0.0529\n",
            "Epoch [912/1024], Loss: 0.0527\n",
            "Epoch [913/1024], Loss: 0.0530\n",
            "Epoch [914/1024], Loss: 0.0529\n",
            "Epoch [915/1024], Loss: 0.0531\n",
            "Epoch [916/1024], Loss: 0.0533\n",
            "Epoch [917/1024], Loss: 0.0534\n",
            "Epoch [918/1024], Loss: 0.0538\n",
            "Epoch [919/1024], Loss: 0.0546\n",
            "Epoch [920/1024], Loss: 0.0545\n",
            "Epoch [921/1024], Loss: 0.0538\n",
            "Epoch [922/1024], Loss: 0.0531\n",
            "Epoch [923/1024], Loss: 0.0527\n",
            "Epoch [924/1024], Loss: 0.0526\n",
            "Epoch [925/1024], Loss: 0.0525\n",
            "Epoch [926/1024], Loss: 0.0526\n",
            "Epoch [927/1024], Loss: 0.0524\n",
            "Epoch [928/1024], Loss: 0.0523\n",
            "Epoch [929/1024], Loss: 0.0524\n",
            "Epoch [930/1024], Loss: 0.0523\n",
            "Epoch [931/1024], Loss: 0.0523\n",
            "Epoch [932/1024], Loss: 0.0523\n",
            "Epoch [933/1024], Loss: 0.0523\n",
            "Epoch [934/1024], Loss: 0.0526\n",
            "Epoch [935/1024], Loss: 0.0529\n",
            "Epoch [936/1024], Loss: 0.0529\n",
            "Epoch [937/1024], Loss: 0.0530\n",
            "Epoch [938/1024], Loss: 0.0532\n",
            "Epoch [939/1024], Loss: 0.0531\n",
            "Epoch [940/1024], Loss: 0.0535\n",
            "Epoch [941/1024], Loss: 0.0546\n",
            "Epoch [942/1024], Loss: 0.0543\n",
            "Epoch [943/1024], Loss: 0.0535\n",
            "Epoch [944/1024], Loss: 0.0533\n",
            "Epoch [945/1024], Loss: 0.0533\n",
            "Epoch [946/1024], Loss: 0.0533\n",
            "Epoch [947/1024], Loss: 0.0536\n",
            "Epoch [948/1024], Loss: 0.0540\n",
            "Epoch [949/1024], Loss: 0.0540\n",
            "Epoch [950/1024], Loss: 0.0543\n",
            "Epoch [951/1024], Loss: 0.0540\n",
            "Epoch [952/1024], Loss: 0.0539\n",
            "Epoch [953/1024], Loss: 0.0534\n",
            "Epoch [954/1024], Loss: 0.0536\n",
            "Epoch [955/1024], Loss: 0.0533\n",
            "Epoch [956/1024], Loss: 0.0532\n",
            "Epoch [957/1024], Loss: 0.0529\n",
            "Epoch [958/1024], Loss: 0.0528\n",
            "Epoch [959/1024], Loss: 0.0525\n",
            "Epoch [960/1024], Loss: 0.0525\n",
            "Epoch [961/1024], Loss: 0.0522\n",
            "Epoch [962/1024], Loss: 0.0523\n",
            "Epoch [963/1024], Loss: 0.0521\n",
            "Epoch [964/1024], Loss: 0.0521\n",
            "Epoch [965/1024], Loss: 0.0522\n",
            "Epoch [966/1024], Loss: 0.0521\n",
            "Epoch [967/1024], Loss: 0.0524\n",
            "Epoch [968/1024], Loss: 0.0521\n",
            "Epoch [969/1024], Loss: 0.0522\n",
            "Epoch [970/1024], Loss: 0.0523\n",
            "Epoch [971/1024], Loss: 0.0521\n",
            "Epoch [972/1024], Loss: 0.0520\n",
            "Epoch [973/1024], Loss: 0.0522\n",
            "Epoch [974/1024], Loss: 0.0522\n",
            "Epoch [975/1024], Loss: 0.0523\n",
            "Epoch [976/1024], Loss: 0.0523\n",
            "Epoch [977/1024], Loss: 0.0525\n",
            "Epoch [978/1024], Loss: 0.0525\n",
            "Epoch [979/1024], Loss: 0.0525\n",
            "Epoch [980/1024], Loss: 0.0531\n",
            "Epoch [981/1024], Loss: 0.0527\n",
            "Epoch [982/1024], Loss: 0.0530\n",
            "Epoch [983/1024], Loss: 0.0526\n",
            "Epoch [984/1024], Loss: 0.0528\n",
            "Epoch [985/1024], Loss: 0.0529\n",
            "Epoch [986/1024], Loss: 0.0532\n",
            "Epoch [987/1024], Loss: 0.0539\n",
            "Epoch [988/1024], Loss: 0.0546\n",
            "Epoch [989/1024], Loss: 0.0545\n",
            "Epoch [990/1024], Loss: 0.0532\n",
            "Epoch [991/1024], Loss: 0.0528\n",
            "Epoch [992/1024], Loss: 0.0525\n",
            "Epoch [993/1024], Loss: 0.0525\n",
            "Epoch [994/1024], Loss: 0.0523\n",
            "Epoch [995/1024], Loss: 0.0527\n",
            "Epoch [996/1024], Loss: 0.0523\n",
            "Epoch [997/1024], Loss: 0.0521\n",
            "Epoch [998/1024], Loss: 0.0522\n",
            "Epoch [999/1024], Loss: 0.0523\n",
            "Epoch [1000/1024], Loss: 0.0522\n",
            "Epoch [1001/1024], Loss: 0.0524\n",
            "Epoch [1002/1024], Loss: 0.0523\n",
            "Epoch [1003/1024], Loss: 0.0522\n",
            "Epoch [1004/1024], Loss: 0.0522\n",
            "Epoch [1005/1024], Loss: 0.0523\n",
            "Epoch [1006/1024], Loss: 0.0522\n",
            "Epoch [1007/1024], Loss: 0.0524\n",
            "Epoch [1008/1024], Loss: 0.0524\n",
            "Epoch [1009/1024], Loss: 0.0533\n",
            "Epoch [1010/1024], Loss: 0.0527\n",
            "Epoch [1011/1024], Loss: 0.0527\n",
            "Epoch [1012/1024], Loss: 0.0531\n",
            "Epoch [1013/1024], Loss: 0.0532\n",
            "Epoch [1014/1024], Loss: 0.0538\n",
            "Epoch [1015/1024], Loss: 0.0538\n",
            "Epoch [1016/1024], Loss: 0.0545\n",
            "Epoch [1017/1024], Loss: 0.0548\n",
            "Epoch [1018/1024], Loss: 0.0553\n",
            "Epoch [1019/1024], Loss: 0.0549\n",
            "Epoch [1020/1024], Loss: 0.0542\n",
            "Epoch [1021/1024], Loss: 0.0539\n",
            "Epoch [1022/1024], Loss: 0.0534\n",
            "Epoch [1023/1024], Loss: 0.0531\n",
            "Epoch [1024/1024], Loss: 0.0533\n",
            "Training complete and model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hoBeOpJnYZhC"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}