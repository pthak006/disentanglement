{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_HACIxMBlMv",
    "outputId": "a266a5c4-b111-46e8-b71d-77b930d36ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 2000\n",
      "Number of columns in the dataset: 50\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
      "0        2       0         3          1   4        1       4         1   \n",
      "1        2       0         1          2   2        1       4         3   \n",
      "2        3       0         2          1   3        1       4         3   \n",
      "3        2       0         1          1   1        0       4         1   \n",
      "4        2       0         1          1   3        0       4         3   \n",
      "\n",
      "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
      "0          2   2  ...         3        3          3           2    3   \n",
      "1          3   1  ...         2        3          2           2    3   \n",
      "2          3   0  ...         4        4          2           1    4   \n",
      "3          3   1  ...         1        2          2           1    3   \n",
      "4          2   0  ...         3        4          1           3    4   \n",
      "\n",
      "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
      "0         1        4          4           2    4  \n",
      "1         1        3          2           2    3  \n",
      "2         2        4          2           0    4  \n",
      "3         1        3          2           1    2  \n",
      "4         1        3          1           3    4  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Data Types and Non-Null Counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 50 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   blue_q0     2000 non-null   int64\n",
      " 1   red_q1      2000 non-null   int64\n",
      " 2   green_q2    2000 non-null   int64\n",
      " 3   purple_q3   2000 non-null   int64\n",
      " 4   q4          2000 non-null   int64\n",
      " 5   blue_q5     2000 non-null   int64\n",
      " 6   red_q6      2000 non-null   int64\n",
      " 7   green_q7    2000 non-null   int64\n",
      " 8   purple_q8   2000 non-null   int64\n",
      " 9   q9          2000 non-null   int64\n",
      " 10  blue_q10    2000 non-null   int64\n",
      " 11  red_q11     2000 non-null   int64\n",
      " 12  green_q12   2000 non-null   int64\n",
      " 13  purple_q13  2000 non-null   int64\n",
      " 14  q14         2000 non-null   int64\n",
      " 15  blue_q15    2000 non-null   int64\n",
      " 16  red_q16     2000 non-null   int64\n",
      " 17  green_q17   2000 non-null   int64\n",
      " 18  purple_q18  2000 non-null   int64\n",
      " 19  q19         2000 non-null   int64\n",
      " 20  blue_q20    2000 non-null   int64\n",
      " 21  red_q21     2000 non-null   int64\n",
      " 22  green_q22   2000 non-null   int64\n",
      " 23  purple_q23  2000 non-null   int64\n",
      " 24  q24         2000 non-null   int64\n",
      " 25  blue_q25    2000 non-null   int64\n",
      " 26  red_q26     2000 non-null   int64\n",
      " 27  green_q27   2000 non-null   int64\n",
      " 28  purple_q28  2000 non-null   int64\n",
      " 29  q29         2000 non-null   int64\n",
      " 30  blue_q30    2000 non-null   int64\n",
      " 31  red_q31     2000 non-null   int64\n",
      " 32  green_q32   2000 non-null   int64\n",
      " 33  purple_q33  2000 non-null   int64\n",
      " 34  q34         2000 non-null   int64\n",
      " 35  blue_q35    2000 non-null   int64\n",
      " 36  red_q36     2000 non-null   int64\n",
      " 37  green_q37   2000 non-null   int64\n",
      " 38  purple_q38  2000 non-null   int64\n",
      " 39  q39         2000 non-null   int64\n",
      " 40  blue_q40    2000 non-null   int64\n",
      " 41  red_q41     2000 non-null   int64\n",
      " 42  green_q42   2000 non-null   int64\n",
      " 43  purple_q43  2000 non-null   int64\n",
      " 44  q44         2000 non-null   int64\n",
      " 45  blue_q45    2000 non-null   int64\n",
      " 46  red_q46     2000 non-null   int64\n",
      " 47  green_q47   2000 non-null   int64\n",
      " 48  purple_q48  2000 non-null   int64\n",
      " 49  q49         2000 non-null   int64\n",
      "dtypes: int64(50)\n",
      "memory usage: 781.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of instances in the dataset:\", df.shape[0])\n",
    "print(\"Number of columns in the dataset:\", df.shape[1])\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display additional information\n",
    "print(\"\\nData Types and Non-Null Counts:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yaO1N84CB7_U"
   },
   "outputs": [],
   "source": [
    "df = df / 4.0\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1600, 50)\n",
      "Test set shape: (400, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target (Y) if necessary.\n",
    "# In the case of autoencoder-like models, we do not have target Y, so we'll treat the whole dataset as X.\n",
    "X = df.values  # Convert the DataFrame into a NumPy array for model input\n",
    "\n",
    "# Split the dataset into training (80%) and testing sets (20%)\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shapes to verify\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jA29S2CJQV",
    "outputId": "97c14234-90fb-43e0-944e-38113a3f8b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([2000, 50])\n",
      "Number of training batches: 50\n",
      "Number of validation batches: 13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "data_array = df.to_numpy()\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for the training and validation sets\n",
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display the shape of the tensor to verify\n",
    "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rywEvDazCYkL"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define the layers of the MLP\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # Learnable embedding vectors e_i for each z_i\n",
    "        self.e = nn.Parameter(torch.randn(output_dim, embedding_dim))\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the MLP to get Z\n",
    "        Z = self.mlp(x)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Convert Z to \\hat Z by multiplying each scalar z_i with its own embedding vector e_i\n",
    "        batch_size = Z.size(0)\n",
    "        Z_expanded = Z.unsqueeze(2)                         # Shape: (batch_size, output_dim, 1)\n",
    "        e_expanded = self.e.unsqueeze(0)                    # Shape: (1, output_dim, embedding_dim)\n",
    "        hat_Z = Z_expanded * e_expanded                     # Shape: (batch_size, output_dim, embedding_dim)\n",
    "\n",
    "        return hat_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tUXzn0AHHq1T"
   },
   "outputs": [],
   "source": [
    "# Decoder class with individual MLPs for each observed variable\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dims=[]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_dim = input_dim      # Number of observed variables (n)\n",
    "        self.output_dim = output_dim    # Number of latent factors (k)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Learnable matrix M of shape (input_dim, output_dim)\n",
    "        self.M = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "\n",
    "        # Value weight matrix W_v to transform \\hat{Z} to V\n",
    "        self.W_v = nn.Linear(embedding_dim, embedding_dim, bias=False)  # Optional bias\n",
    "\n",
    "        # Define individual MLPs for each observed variable\n",
    "        dims = [embedding_dim] + hidden_dims + [1]\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            nn.Sequential(*[\n",
    "                nn.Linear(dims[i], dims[i + 1]) if i == len(dims) - 2 else nn.Sequential(\n",
    "                    nn.Linear(dims[i], dims[i + 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                for i in range(len(dims) - 1)\n",
    "            ])\n",
    "            for _ in range(input_dim)\n",
    "        ])\n",
    "\n",
    "    def forward(self, hat_Z):\n",
    "        \"\"\"\n",
    "        hat_Z: Tensor of shape (batch_size, output_dim, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = hat_Z.size(0)\n",
    "\n",
    "        # Compute value matrix V by applying W_v to \\hat{Z}\n",
    "        V = self.W_v(hat_Z)  # Shape: (batch_size, output_dim, embedding_dim)\n",
    "\n",
    "        # Compute attention weights\n",
    "        # Take absolute value of M and apply softmax over output_dim (latent factors) axis\n",
    "        M_abs = torch.abs(self.M)  # Shape: (input_dim, output_dim)\n",
    "        attention_weights = F.softmax(M_abs, dim=1)  # Shape: (input_dim, output_dim)\n",
    "\n",
    "        # Expand attention weights to batch dimension\n",
    "        attention_weights = attention_weights.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, input_dim, output_dim)\n",
    "\n",
    "        # Compute context vectors for each observed variable\n",
    "        # Multiply attention weights with V\n",
    "        # Need to transpose V to (batch_size, embedding_dim, output_dim)\n",
    "        V_transposed = V.permute(0, 2, 1)  # Shape: (batch_size, embedding_dim, output_dim)\n",
    "        context = torch.bmm(attention_weights, V_transposed.transpose(1, 2))  # Shape: (batch_size, input_dim, embedding_dim)\n",
    "\n",
    "        # Pass each context vector through its corresponding MLP\n",
    "        x_hat = []\n",
    "        for i in range(self.input_dim):\n",
    "            x_i = context[:, i, :]  # Shape: (batch_size, embedding_dim)\n",
    "            x_i_hat = self.mlp_layers[i](x_i)  # Shape: (batch_size, 1)\n",
    "            x_hat.append(x_i_hat)\n",
    "        x_hat = torch.cat(x_hat, dim=1)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        return x_hat, attention_weights  # Return attention weights for potential analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xnBqmgVjIat0"
   },
   "outputs": [],
   "source": [
    "# Complete model combining the encoder and decoder\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, encoder_hidden_dims=[], decoder_hidden_dims=[]):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=encoder_hidden_dims\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=decoder_hidden_dims\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hat_Z = self.encoder(x)     # Obtain \\hat{Z} from the encoder\n",
    "        x_hat, attention_weights = self.decoder(hat_Z)  # Reconstruct x from \\hat{Z} using the decoder\n",
    "        return x_hat, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ki1licN-PTun",
    "outputId": "fb864687-79d0-4763-ec7f-18a71da1ed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Lambda Entropy: 0.000000, Train Total Loss: 0.1656, Train Recon Loss: 0.1656, Train Entropy Loss: 0.0000, Val Total Loss: 0.0843, Val Recon Loss: 0.0843, Val Entropy Loss: 0.0000\n",
      "Epoch [2/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0856, Train Recon Loss: 0.0856, Train Entropy Loss: 0.0000, Val Total Loss: 0.0806, Val Recon Loss: 0.0806, Val Entropy Loss: 0.0000\n",
      "Epoch [3/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0796, Train Recon Loss: 0.0796, Train Entropy Loss: 0.0000, Val Total Loss: 0.0712, Val Recon Loss: 0.0712, Val Entropy Loss: 0.0000\n",
      "Epoch [4/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0689, Train Recon Loss: 0.0689, Train Entropy Loss: 0.0000, Val Total Loss: 0.0628, Val Recon Loss: 0.0628, Val Entropy Loss: 0.0000\n",
      "Epoch [5/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0624, Train Recon Loss: 0.0624, Train Entropy Loss: 0.0000, Val Total Loss: 0.0586, Val Recon Loss: 0.0586, Val Entropy Loss: 0.0000\n",
      "Epoch [6/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0583, Train Recon Loss: 0.0583, Train Entropy Loss: 0.0000, Val Total Loss: 0.0550, Val Recon Loss: 0.0550, Val Entropy Loss: 0.0000\n",
      "Epoch [7/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0553, Train Recon Loss: 0.0553, Train Entropy Loss: 0.0000, Val Total Loss: 0.0533, Val Recon Loss: 0.0533, Val Entropy Loss: 0.0000\n",
      "Epoch [8/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0530, Train Recon Loss: 0.0530, Train Entropy Loss: 0.0000, Val Total Loss: 0.0509, Val Recon Loss: 0.0509, Val Entropy Loss: 0.0000\n",
      "Epoch [9/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0506, Train Recon Loss: 0.0506, Train Entropy Loss: 0.0000, Val Total Loss: 0.0491, Val Recon Loss: 0.0491, Val Entropy Loss: 0.0000\n",
      "Epoch [10/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0494, Train Recon Loss: 0.0494, Train Entropy Loss: 0.0000, Val Total Loss: 0.0485, Val Recon Loss: 0.0485, Val Entropy Loss: 0.0000\n",
      "Epoch [11/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0488, Train Recon Loss: 0.0488, Train Entropy Loss: 0.0000, Val Total Loss: 0.0481, Val Recon Loss: 0.0481, Val Entropy Loss: 0.0000\n",
      "Epoch [12/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0484, Train Recon Loss: 0.0484, Train Entropy Loss: 0.0000, Val Total Loss: 0.0481, Val Recon Loss: 0.0481, Val Entropy Loss: 0.0000\n",
      "Epoch [13/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0481, Train Recon Loss: 0.0481, Train Entropy Loss: 0.0000, Val Total Loss: 0.0478, Val Recon Loss: 0.0478, Val Entropy Loss: 0.0000\n",
      "Epoch [14/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0478, Train Recon Loss: 0.0478, Train Entropy Loss: 0.0000, Val Total Loss: 0.0479, Val Recon Loss: 0.0479, Val Entropy Loss: 0.0000\n",
      "Epoch [15/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0476, Train Recon Loss: 0.0476, Train Entropy Loss: 0.0000, Val Total Loss: 0.0477, Val Recon Loss: 0.0477, Val Entropy Loss: 0.0000\n",
      "Epoch [16/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0475, Train Recon Loss: 0.0475, Train Entropy Loss: 0.0000, Val Total Loss: 0.0474, Val Recon Loss: 0.0474, Val Entropy Loss: 0.0000\n",
      "Epoch [17/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0473, Train Recon Loss: 0.0473, Train Entropy Loss: 0.0000, Val Total Loss: 0.0476, Val Recon Loss: 0.0476, Val Entropy Loss: 0.0000\n",
      "Epoch [18/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0471, Train Recon Loss: 0.0471, Train Entropy Loss: 0.0000, Val Total Loss: 0.0475, Val Recon Loss: 0.0475, Val Entropy Loss: 0.0000\n",
      "Epoch [19/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0469, Train Recon Loss: 0.0469, Train Entropy Loss: 0.0000, Val Total Loss: 0.0474, Val Recon Loss: 0.0474, Val Entropy Loss: 0.0000\n",
      "Epoch [20/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0467, Train Recon Loss: 0.0467, Train Entropy Loss: 0.0000, Val Total Loss: 0.0472, Val Recon Loss: 0.0472, Val Entropy Loss: 0.0000\n",
      "Epoch [21/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0466, Train Recon Loss: 0.0466, Train Entropy Loss: 0.0000, Val Total Loss: 0.0473, Val Recon Loss: 0.0473, Val Entropy Loss: 0.0000\n",
      "Epoch [22/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0465, Train Recon Loss: 0.0465, Train Entropy Loss: 0.0000, Val Total Loss: 0.0472, Val Recon Loss: 0.0472, Val Entropy Loss: 0.0000\n",
      "Epoch [23/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0462, Train Recon Loss: 0.0462, Train Entropy Loss: 0.0000, Val Total Loss: 0.0475, Val Recon Loss: 0.0475, Val Entropy Loss: 0.0000\n",
      "Epoch [24/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0462, Train Recon Loss: 0.0462, Train Entropy Loss: 0.0000, Val Total Loss: 0.0471, Val Recon Loss: 0.0471, Val Entropy Loss: 0.0000\n",
      "Epoch [25/25], Lambda Entropy: 0.000000, Train Total Loss: 0.0462, Train Recon Loss: 0.0462, Train Entropy Loss: 0.0000, Val Total Loss: 0.0472, Val Recon Loss: 0.0472, Val Entropy Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume that the Encoder, Decoder, and Model classes are already defined\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 50        # Number of observed variables\n",
    "output_dim = 5        # Output dimension of the encoder (dimension of Z)\n",
    "embedding_dim = 64    # Embedding dimension for the embeddings e and e_i's\n",
    "encoder_hidden_dims = [128, 64]  # Hidden dimensions for the encoder\n",
    "decoder_hidden_dims = [64, 32]   # Hidden dimensions for the decoder\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 25          # Number of epochs\n",
    "batch_size = 32          # Batch size (already set in the DataLoader)\n",
    "print_every = 1          # How often to print loss (in epochs)\n",
    "\n",
    "# Define the maximum value for the entropy regularization coefficient\n",
    "max_lambda_entropy = 1e-3  # Adjust this value as needed\n",
    "\n",
    "# Flag to enable or disable entropy regularizer\n",
    "use_entropy_regularizer = False  # Set to True to enable, False to disable\n",
    "\n",
    "# Scheduler function for lambda_entropy\n",
    "def get_lambda_entropy(epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=True):\n",
    "    if not use_entropy_regularizer:\n",
    "        return 0.0\n",
    "    if schedule_type == 'constant':\n",
    "        # Always return max_lambda_entropy\n",
    "        return max_lambda_entropy\n",
    "    elif schedule_type == 'linear':\n",
    "        # Linear increase from 0 to max_lambda_entropy\n",
    "        return max_lambda_entropy * (epoch / num_epochs)\n",
    "    elif schedule_type == 'exponential':\n",
    "        # Exponential increase from 0 to max_lambda_entropy\n",
    "        k = 5  # Adjust this value to control the speed of increase\n",
    "        return max_lambda_entropy * (1 - math.exp(-k * epoch / num_epochs))\n",
    "    elif schedule_type == 'logarithmic':\n",
    "        # Logarithmic increase from 0 to max_lambda_entropy\n",
    "        if epoch == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return max_lambda_entropy * math.log(epoch + 1) / math.log(num_epochs + 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")\n",
    "\n",
    "# Initialize a list to store the average attention matrices per epoch\n",
    "attention_matrices = []\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute lambda_entropy for the current epoch\n",
    "    lambda_entropy = get_lambda_entropy(\n",
    "        epoch, num_epochs, max_lambda_entropy, schedule_type='exponential', use_entropy_regularizer=use_entropy_regularizer)\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0          # Accumulates total loss (reconstruction + regularizer)\n",
    "    running_recon_loss = 0.0    # Accumulates reconstruction loss\n",
    "    running_entropy_loss = 0.0  # Accumulates entropy regularizer loss\n",
    "    epoch_attn_weights = []     # List to store attention weights for all batches in the epoch\n",
    "\n",
    "    for batch_idx, (batch,) in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute predicted x_hat and attention weights\n",
    "        x_hat, attn_weights = model(batch)\n",
    "\n",
    "        # Squeeze attn_weights to get shape: (batch_size, input_dim, output_dim)\n",
    "        attn_weights = attn_weights.squeeze(1)\n",
    "\n",
    "        # Collect attention weights for the epoch\n",
    "        epoch_attn_weights.append(attn_weights.detach().cpu())\n",
    "\n",
    "        # Compute the reconstruction loss\n",
    "        recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "        # Initialize entropy_regularizer to zero\n",
    "        entropy_regularizer = 0.0\n",
    "\n",
    "        # Compute the entropy regularizer if enabled\n",
    "        if use_entropy_regularizer:\n",
    "            # Add a small epsilon to prevent log(0)\n",
    "            epsilon = 1e-8\n",
    "\n",
    "            # Compute entropy for each query (input_dim)\n",
    "            entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "            # Sum entropies over queries and average over batch\n",
    "            entropy_regularizer = torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
    "\n",
    "        # Total loss\n",
    "        loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training losses\n",
    "        running_loss += loss.item()\n",
    "        running_recon_loss += recon_loss.item()\n",
    "        running_entropy_loss += entropy_regularizer # Accumulate entropy regularizer loss (before scaling)\n",
    "\n",
    "    # Compute average losses for training\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_train_recon_loss = running_recon_loss / len(train_loader)\n",
    "    avg_train_entropy_loss = running_entropy_loss / len(train_loader)\n",
    "\n",
    "    # Compute the average attention matrix for the epoch\n",
    "    epoch_attn_weights_tensor = torch.cat(epoch_attn_weights, dim=0)  # Shape: (num_samples_in_epoch, input_dim, output_dim)\n",
    "    avg_attn_weights_epoch = epoch_attn_weights_tensor.mean(dim=0)    # Shape: (input_dim, output_dim)\n",
    "    avg_attn_weights_epoch_np = avg_attn_weights_epoch.numpy()\n",
    "\n",
    "    # Transpose to have shape (output_dim, input_dim) so that queries are on x-axis and keys on y-axis\n",
    "    attention_matrices.append(avg_attn_weights_epoch_np.T)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0          # Accumulates total loss (reconstruction + regularizer)\n",
    "    val_recon_loss = 0.0    # Accumulates reconstruction loss\n",
    "    val_entropy_loss = 0.0  # Accumulates entropy regularizer loss\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch,) in enumerate(val_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            x_hat, attn_weights = model(batch)\n",
    "\n",
    "            # Compute the reconstruction loss\n",
    "            recon_loss = criterion(x_hat, batch)\n",
    "\n",
    "            # Initialize entropy_regularizer to zero\n",
    "            entropy_regularizer = 0.0\n",
    "\n",
    "            # Compute the entropy regularizer if enabled\n",
    "            if use_entropy_regularizer:\n",
    "                attn_weights = attn_weights.squeeze(1)  # Shape: (batch_size, input_dim, output_dim)\n",
    "\n",
    "                # Add a small epsilon to prevent log(0)\n",
    "                epsilon = 1e-8\n",
    "\n",
    "                # Compute entropy for each query (input_dim)\n",
    "                entropy = -torch.sum(attn_weights * torch.log(attn_weights + epsilon), dim=2)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "                # Sum entropies over queries and average over batch\n",
    "                entropy_regularizer = torch.mean(torch.sum(entropy, dim=1))  # Scalar\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss + lambda_entropy * entropy_regularizer\n",
    "\n",
    "            # Accumulate validation losses\n",
    "            val_loss += loss.item()\n",
    "            val_recon_loss += recon_loss.item()\n",
    "            val_entropy_loss += entropy_regularizer  # Accumulate entropy regularizer loss (before scaling)\n",
    "\n",
    "    # Compute average losses for validation\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_recon_loss = val_recon_loss / len(val_loader)\n",
    "    avg_val_entropy_loss = val_entropy_loss / len(val_loader)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Lambda Entropy: {lambda_entropy:.6f}, '\n",
    "              f'Train Total Loss: {avg_train_loss:.4f}, Train Recon Loss: {avg_train_recon_loss:.4f}, Train Entropy Loss: {avg_train_entropy_loss:.4f}, '\n",
    "              f'Val Total Loss: {avg_val_loss:.4f}, Val Recon Loss: {avg_val_recon_loss:.4f}, Val Entropy Loss: {avg_val_entropy_loss:.4f}')\n",
    "\n",
    "# # Save the trained model after training\n",
    "# torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "# print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Step 1: Extract the attention matrix M from the trained model\n",
    "# Assuming 'model' is your trained model instance\n",
    "M = model.decoder.M.data.cpu().numpy()  # Shape: (input_dim, output_dim)\n",
    "\n",
    "# Step 2: Compute the attention weights\n",
    "# Take the absolute value of M\n",
    "M_abs = np.abs(M)  # Shape: (input_dim, output_dim)\n",
    "\n",
    "# Convert to tensor for softmax computation\n",
    "M_abs_tensor = torch.tensor(M_abs, dtype=torch.float32)\n",
    "\n",
    "# Apply softmax over the output_dim axis (dim=1)\n",
    "attention_weights_tensor = F.softmax(M_abs_tensor, dim=1)  # Shape: (input_dim, output_dim)\n",
    "\n",
    "# Convert back to NumPy array\n",
    "attention_weights = attention_weights_tensor.numpy()\n",
    "\n",
    "# Step 3: Assign each observed variable to the latent factor with the highest attention weight\n",
    "predicted_labels = np.argmax(attention_weights, axis=1)  # Shape: (input_dim,)\n",
    "\n",
    "# Step 4: Create true labels for the observed variables\n",
    "# Assuming 'df' is your DataFrame containing the observed variables with their names\n",
    "\n",
    "# Identify column prefixes for each true factor\n",
    "factor_columns = {\n",
    "    'Factor0': [col for col in df.columns if col.startswith('blue')],\n",
    "    'Factor1': [col for col in df.columns if col.startswith('green')],\n",
    "    'Factor2': [col for col in df.columns if col.startswith('purple')],\n",
    "    'Factor3': [col for col in df.columns if col.startswith('red')],\n",
    "    'Factor4': [col for col in df.columns if col.startswith('q')]\n",
    "}\n",
    "\n",
    "# Map factor names to column indices\n",
    "factor_indices = {}\n",
    "for factor_name, columns in factor_columns.items():\n",
    "    indices = [df.columns.get_loc(col) for col in columns]\n",
    "    factor_indices[factor_name] = indices\n",
    "\n",
    "# Create true labels for variables\n",
    "n_features = df.shape[1]\n",
    "true_labels = np.full(n_features, -1)  # Initialize with -1\n",
    "\n",
    "factor_names = ['Factor0', 'Factor1', 'Factor2', 'Factor3', 'Factor4']\n",
    "factor_name_to_index = {name: idx for idx, name in enumerate(factor_names)}\n",
    "\n",
    "for factor_name, indices in factor_indices.items():\n",
    "    factor_idx = factor_name_to_index[factor_name]\n",
    "    true_labels[indices] = factor_idx\n",
    "\n",
    "# Ensure all variables have been assigned\n",
    "assert np.all(true_labels >= 0), \"Some variables have not been assigned a true label\"\n",
    "\n",
    "# Step 5: Compute the Adjusted Rand Index\n",
    "ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "print(f'Adjusted Rand Index (ARI): {ari:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract the attention matrix M from the trained model\n",
    "# Assuming 'model' is your trained model instance\n",
    "M = model.decoder.M.data.cpu().numpy()  # Shape: (input_dim, output_dim)\n",
    "\n",
    "# Step 2: Compute the attention weights\n",
    "# Take the absolute value of M\n",
    "M_abs = np.abs(M)  # Shape: (input_dim, output_dim)\n",
    "\n",
    "# Apply softmax over the output_dim axis (dim=1)\n",
    "M_abs_tensor = torch.tensor(M_abs, dtype=torch.float32)\n",
    "attention_weights_tensor = F.softmax(M_abs_tensor, dim=1)\n",
    "attention_weights = attention_weights_tensor.numpy()\n",
    "\n",
    "# Step 3: Transpose the attention weights to have latent factors along y-axis and observed variables along x-axis\n",
    "attention_matrix = attention_weights.T  # Shape: (output_dim, input_dim)\n",
    "\n",
    "# Step 4: Prepare labels for observed variables and latent factors\n",
    "# Assuming 'df' is your DataFrame containing the observed variables with their names\n",
    "observed_variable_names = df.columns.tolist()  # List of observed variable names\n",
    "latent_factor_names = [f'Latent {i}' for i in range(attention_matrix.shape[0])]  # Latent factor names\n",
    "\n",
    "# Step 5: Create a DataFrame for visualization\n",
    "attention_df = pd.DataFrame(\n",
    "    attention_matrix,\n",
    "    index=latent_factor_names,\n",
    "    columns=observed_variable_names\n",
    ")\n",
    "\n",
    "# Step 6: Plot the attention matrix heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    attention_df,\n",
    "    cmap='viridis',\n",
    "    linewidths=0.5,\n",
    "    annot=False,\n",
    "    cbar=True,\n",
    "    vmin=0,\n",
    "    vmax=1\n",
    ")\n",
    "plt.title('Attention Matrix Heatmap', fontsize=16)\n",
    "plt.xlabel('Observed Variables', fontsize=14)\n",
    "plt.ylabel('Latent Factors', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
