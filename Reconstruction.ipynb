{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QoBlbi807OgL"
      },
      "outputs": [],
      "source": [
        "# Necessary imports\n",
        "import numpy as np\n",
        "\n",
        "def histogram_discretize(target, num_bins=20):\n",
        "    \"\"\"\n",
        "    Discretization based on histograms.\n",
        "    Converts continuous representations into discrete bins.\n",
        "\n",
        "    Args:\n",
        "    - target (np.ndarray): The continuous representation array to discretize.\n",
        "    - num_bins (int): The number of bins to use for discretization.\n",
        "\n",
        "    Returns:\n",
        "    - discretized (np.ndarray): Discretized version of the input array.\n",
        "    \"\"\"\n",
        "    discretized = np.zeros_like(target)\n",
        "    # Iterate over each dimension of the input array and apply histogram-based discretization\n",
        "    for i in range(target.shape[0]):\n",
        "        # Use np.digitize to assign bin numbers\n",
        "        discretized[i, :] = np.digitize(target[i, :], np.histogram(target[i, :], num_bins)[1][:-1])\n",
        "\n",
        "    return discretized\n",
        "\n",
        "# # Test the function with a sample input\n",
        "# test_array = np.random.rand(5, 100)  # Random array for testing purposes\n",
        "# discretized_array = histogram_discretize(test_array, num_bins=10)\n",
        "\n",
        "# discretized_array.shape, discretized_array  # Display the shape and content of the discretized array\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "def discrete_mutual_info(mus, ys):\n",
        "    \"\"\"\n",
        "    Compute discrete mutual information.\n",
        "    \"\"\"\n",
        "    num_codes = mus.shape[0]\n",
        "    num_factors = ys.shape[0]\n",
        "    m = np.zeros([num_codes, num_factors])\n",
        "    for i in range(num_codes):\n",
        "        for j in range(num_factors):\n",
        "            m[i, j] = mutual_info_score(ys[j, :], mus[i, :])\n",
        "    return m"
      ],
      "metadata": {
        "id": "veXXufch7k5_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "def discrete_entropy(ys):\n",
        "    \"\"\"\n",
        "    Compute discrete mutual information.\n",
        "    \"\"\"\n",
        "    num_factors = ys.shape[0]\n",
        "    h = np.zeros(num_factors)\n",
        "    for j in range(num_factors):\n",
        "        h[j] = mutual_info_score(ys[j, :], ys[j, :])\n",
        "    return h\n"
      ],
      "metadata": {
        "id": "3UVnQfpg7riB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mig(mus_train, ys_train, num_bins=20):\n",
        "    \"\"\" Computes the Mutual Information Gap (MIG) score.\n",
        "    Args:\n",
        "    - mus_train: Latent representations, numpy array of shape (num_latents, num_samples).\n",
        "    - ys_train: True factors, numpy array of shape (num_factors, num_samples).\n",
        "    - num_bins: Number of bins for discretization (default is 20).\n",
        "    Returns:\n",
        "    - mig_score: The computed MIG score.\n",
        "    \"\"\"\n",
        "    # Discretize the latent representations\n",
        "    discretized_mus = histogram_discretize(mus_train, num_bins=num_bins)\n",
        "\n",
        "    # Compute mutual information matrix\n",
        "    mutual_info_matrix = discrete_mutual_info(discretized_mus, ys_train)\n",
        "\n",
        "    # Compute entropy for each factor\n",
        "    entropy_values = discrete_entropy(ys_train)\n",
        "\n",
        "    # Compute the MIG score\n",
        "    mig_scores = np.zeros(mutual_info_matrix.shape[1])\n",
        "    for k in range(mutual_info_matrix.shape[1]):\n",
        "        mi_k = mutual_info_matrix[:, k]\n",
        "        top_mi = np.max(mi_k)\n",
        "        top_mi_idx = np.argmax(mi_k)\n",
        "        second_mi = np.max(mi_k[np.arange(len(mi_k)) != top_mi_idx])\n",
        "        mig_scores[k] = (top_mi - second_mi) / entropy_values[k]\n",
        "\n",
        "    mig_score = np.mean(mig_scores)\n",
        "    return mig_score"
      ],
      "metadata": {
        "id": "FPfSQcUD7taL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the GitHub repository\n",
        "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Number of instances in the dataset:\", df.shape[0])\n",
        "print(\"Number of columns in the dataset:\", df.shape[1])\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display additional information\n",
        "print(\"\\nData Types and Non-Null Counts:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nSummary Statistics of the Dataset:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENdDwwNj7vOx",
        "outputId": "3cd70549-5466-4308-f7f3-6ce6e6bbb3f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of instances in the dataset: 2000\n",
            "Number of columns in the dataset: 50\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "   blue_q0  red_q1  green_q2  purple_q3  q4  blue_q5  red_q6  green_q7  \\\n",
            "0        2       0         3          1   4        1       4         1   \n",
            "1        2       0         1          2   2        1       4         3   \n",
            "2        3       0         2          1   3        1       4         3   \n",
            "3        2       0         1          1   1        0       4         1   \n",
            "4        2       0         1          1   3        0       4         3   \n",
            "\n",
            "   purple_q8  q9  ...  blue_q40  red_q41  green_q42  purple_q43  q44  \\\n",
            "0          2   2  ...         3        3          3           2    3   \n",
            "1          3   1  ...         2        3          2           2    3   \n",
            "2          3   0  ...         4        4          2           1    4   \n",
            "3          3   1  ...         1        2          2           1    3   \n",
            "4          2   0  ...         3        4          1           3    4   \n",
            "\n",
            "   blue_q45  red_q46  green_q47  purple_q48  q49  \n",
            "0         1        4          4           2    4  \n",
            "1         1        3          2           2    3  \n",
            "2         2        4          2           0    4  \n",
            "3         1        3          2           1    2  \n",
            "4         1        3          1           3    4  \n",
            "\n",
            "[5 rows x 50 columns]\n",
            "\n",
            "Data Types and Non-Null Counts:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 50 columns):\n",
            " #   Column      Non-Null Count  Dtype\n",
            "---  ------      --------------  -----\n",
            " 0   blue_q0     2000 non-null   int64\n",
            " 1   red_q1      2000 non-null   int64\n",
            " 2   green_q2    2000 non-null   int64\n",
            " 3   purple_q3   2000 non-null   int64\n",
            " 4   q4          2000 non-null   int64\n",
            " 5   blue_q5     2000 non-null   int64\n",
            " 6   red_q6      2000 non-null   int64\n",
            " 7   green_q7    2000 non-null   int64\n",
            " 8   purple_q8   2000 non-null   int64\n",
            " 9   q9          2000 non-null   int64\n",
            " 10  blue_q10    2000 non-null   int64\n",
            " 11  red_q11     2000 non-null   int64\n",
            " 12  green_q12   2000 non-null   int64\n",
            " 13  purple_q13  2000 non-null   int64\n",
            " 14  q14         2000 non-null   int64\n",
            " 15  blue_q15    2000 non-null   int64\n",
            " 16  red_q16     2000 non-null   int64\n",
            " 17  green_q17   2000 non-null   int64\n",
            " 18  purple_q18  2000 non-null   int64\n",
            " 19  q19         2000 non-null   int64\n",
            " 20  blue_q20    2000 non-null   int64\n",
            " 21  red_q21     2000 non-null   int64\n",
            " 22  green_q22   2000 non-null   int64\n",
            " 23  purple_q23  2000 non-null   int64\n",
            " 24  q24         2000 non-null   int64\n",
            " 25  blue_q25    2000 non-null   int64\n",
            " 26  red_q26     2000 non-null   int64\n",
            " 27  green_q27   2000 non-null   int64\n",
            " 28  purple_q28  2000 non-null   int64\n",
            " 29  q29         2000 non-null   int64\n",
            " 30  blue_q30    2000 non-null   int64\n",
            " 31  red_q31     2000 non-null   int64\n",
            " 32  green_q32   2000 non-null   int64\n",
            " 33  purple_q33  2000 non-null   int64\n",
            " 34  q34         2000 non-null   int64\n",
            " 35  blue_q35    2000 non-null   int64\n",
            " 36  red_q36     2000 non-null   int64\n",
            " 37  green_q37   2000 non-null   int64\n",
            " 38  purple_q38  2000 non-null   int64\n",
            " 39  q39         2000 non-null   int64\n",
            " 40  blue_q40    2000 non-null   int64\n",
            " 41  red_q41     2000 non-null   int64\n",
            " 42  green_q42   2000 non-null   int64\n",
            " 43  purple_q43  2000 non-null   int64\n",
            " 44  q44         2000 non-null   int64\n",
            " 45  blue_q45    2000 non-null   int64\n",
            " 46  red_q46     2000 non-null   int64\n",
            " 47  green_q47   2000 non-null   int64\n",
            " 48  purple_q48  2000 non-null   int64\n",
            " 49  q49         2000 non-null   int64\n",
            "dtypes: int64(50)\n",
            "memory usage: 781.4 KB\n",
            "None\n",
            "\n",
            "Summary Statistics of the Dataset:\n",
            "           blue_q0       red_q1     green_q2    purple_q3           q4  \\\n",
            "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
            "mean      1.633500     1.155000     2.352000     2.126500     2.869500   \n",
            "std       1.216107     1.278199     1.116118     1.314672     1.130977   \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "25%       1.000000     0.000000     2.000000     1.000000     2.000000   \n",
            "50%       2.000000     1.000000     2.000000     2.000000     3.000000   \n",
            "75%       3.000000     2.000000     3.000000     3.000000     4.000000   \n",
            "max       4.000000     4.000000     4.000000     4.000000     4.000000   \n",
            "\n",
            "           blue_q5       red_q6    green_q7   purple_q8           q9  ...  \\\n",
            "count  2000.000000  2000.000000  2000.00000  2000.00000  2000.000000  ...   \n",
            "mean      1.689000     2.938000     2.03000     2.27550     0.973500  ...   \n",
            "std       1.333855     1.061942     1.35352     1.20055     1.061773  ...   \n",
            "min       0.000000     0.000000     0.00000     0.00000     0.000000  ...   \n",
            "25%       1.000000     2.000000     1.00000     1.00000     0.000000  ...   \n",
            "50%       2.000000     3.000000     2.00000     2.00000     1.000000  ...   \n",
            "75%       3.000000     4.000000     3.00000     3.00000     2.000000  ...   \n",
            "max       4.000000     4.000000     4.00000     4.00000     4.000000  ...   \n",
            "\n",
            "          blue_q40      red_q41    green_q42   purple_q43         q44  \\\n",
            "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.00000   \n",
            "mean      2.121000     2.869500     2.167000     2.017000     3.19350   \n",
            "std       1.358043     1.108641     1.274332     1.297518     0.99426   \n",
            "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.00000   \n",
            "25%       1.000000     2.000000     1.000000     1.000000     3.00000   \n",
            "50%       2.000000     3.000000     2.000000     2.000000     3.00000   \n",
            "75%       3.000000     4.000000     3.000000     3.000000     4.00000   \n",
            "max       4.000000     4.000000     4.000000     4.000000     4.00000   \n",
            "\n",
            "          blue_q45      red_q46    green_q47   purple_q48          q49  \n",
            "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
            "mean      2.390500     2.674500     2.685000     1.774500     3.065500  \n",
            "std       1.331501     1.069169     1.032622     1.296723     0.985746  \n",
            "min      -1.000000     0.000000    -1.000000     0.000000    -1.000000  \n",
            "25%       1.000000     2.000000     2.000000     1.000000     2.000000  \n",
            "50%       3.000000     3.000000     3.000000     2.000000     3.000000  \n",
            "75%       4.000000     4.000000     3.000000     3.000000     4.000000  \n",
            "max       4.000000     4.000000     4.000000     4.000000     4.000000  \n",
            "\n",
            "[8 rows x 50 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify column prefixes for each true factor\n",
        "factor_columns = {\n",
        "    'Factor1': [col for col in df.columns if col.startswith('blue')],\n",
        "    'Factor2': [col for col in df.columns if col.startswith('green')],\n",
        "    'Factor3': [col for col in df.columns if col.startswith('purple')],\n",
        "    'Factor4': [col for col in df.columns if col.startswith('red')],\n",
        "    'Factor5': [col for col in df.columns if col.startswith('q')]\n",
        "}\n",
        "\n",
        "# Calculate true factors by summing the respective columns\n",
        "true_factors = pd.DataFrame()\n",
        "for factor_name, columns in factor_columns.items():\n",
        "    true_factors[factor_name] = df[columns].sum(axis=1)\n",
        "\n",
        "# Display the first few rows of the calculated true factors\n",
        "print(true_factors.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiBY-Tes7xwX",
        "outputId": "8934cafc-ab6c-4daa-c44e-2dddf034fd75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Factor1  Factor2  Factor3  Factor4  Factor5\n",
            "0       20       21       21       22       28\n",
            "1       21       20       21       26       23\n",
            "2       23       20       17       22       25\n",
            "3       17       15       11       22       15\n",
            "4       20       14       24       23       24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define the Autoencoder architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=50, latent_dim=5):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder: Compress the input to the latent space\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),  # First hidden layer\n",
        "            nn.ReLU(),  # Activation function\n",
        "            nn.Linear(32, 16),  # Second hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, latent_dim)  # Latent layer\n",
        "        )\n",
        "\n",
        "        # Decoder: Reconstruct the input from the latent space\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 16),  # First hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),  # Second hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, input_dim),  # Output layer\n",
        "            nn.Sigmoid()  # Output activation to bring the reconstructed values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode input\n",
        "        latent = self.encoder(x)\n",
        "        # Decode input from latent representation\n",
        "        reconstructed = self.decoder(latent)\n",
        "        return reconstructed\n",
        "\n",
        "# Create an instance of the Autoencoder model\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "# Display the architecture\n",
        "print(autoencoder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRi_JN7W7z9r",
        "outputId": "1ec38b36-8872-4f4a-86ba-dd221f9bb3bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=16, out_features=5, bias=True)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=16, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=50, bias=True)\n",
            "    (5): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df / 4.0\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSeiyZ4G72KO",
        "outputId": "d66d252a-29f0-4235-849c-2220fea06d60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   blue_q0  red_q1  green_q2  purple_q3    q4  blue_q5  red_q6  green_q7  \\\n",
            "0     0.50     0.0      0.75       0.25  1.00     0.25     1.0      0.25   \n",
            "1     0.50     0.0      0.25       0.50  0.50     0.25     1.0      0.75   \n",
            "2     0.75     0.0      0.50       0.25  0.75     0.25     1.0      0.75   \n",
            "3     0.50     0.0      0.25       0.25  0.25     0.00     1.0      0.25   \n",
            "4     0.50     0.0      0.25       0.25  0.75     0.00     1.0      0.75   \n",
            "\n",
            "   purple_q8    q9  ...  blue_q40  red_q41  green_q42  purple_q43   q44  \\\n",
            "0       0.50  0.50  ...      0.75     0.75       0.75        0.50  0.75   \n",
            "1       0.75  0.25  ...      0.50     0.75       0.50        0.50  0.75   \n",
            "2       0.75  0.00  ...      1.00     1.00       0.50        0.25  1.00   \n",
            "3       0.75  0.25  ...      0.25     0.50       0.50        0.25  0.75   \n",
            "4       0.50  0.00  ...      0.75     1.00       0.25        0.75  1.00   \n",
            "\n",
            "   blue_q45  red_q46  green_q47  purple_q48   q49  \n",
            "0      0.25     1.00       1.00        0.50  1.00  \n",
            "1      0.25     0.75       0.50        0.50  0.75  \n",
            "2      0.50     1.00       0.50        0.00  1.00  \n",
            "3      0.25     0.75       0.50        0.25  0.50  \n",
            "4      0.25     0.75       0.25        0.75  1.00  \n",
            "\n",
            "[5 rows x 50 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "data_array = df.to_numpy()\n",
        "\n",
        "# Convert the data to a PyTorch tensor\n",
        "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
        "\n",
        "# Create a PyTorch dataset\n",
        "dataset = TensorDataset(data_tensor)\n",
        "\n",
        "# Create a DataLoader for the dataset\n",
        "batch_size = 64  # You can adjust the batch size as needed\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Display the shape of the tensor to verify\n",
        "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
        "print(f\"Number of batches: {len(dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La0qllnf74Wd",
        "outputId": "1f6475d5-5106-4bc2-98cc-437f747c97b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data tensor shape: torch.Size([2000, 50])\n",
            "Number of batches: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer for training\n",
        "loss_function = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# Training loop for the autoencoder\n",
        "num_epochs = 400  # You can adjust the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0  # To accumulate the loss for the current epoch\n",
        "\n",
        "    for data in dataloader:\n",
        "        inputs = data[0]  # Get the input data\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Encode and decode the input data\n",
        "        outputs = autoencoder(inputs)\n",
        "\n",
        "        # Compute the reconstruction loss\n",
        "        loss = loss_function(outputs, inputs)\n",
        "\n",
        "        # Backward pass: Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss for the current epoch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Print the loss for each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCO59MpQ7-sk",
        "outputId": "d8efffd0-8532-4d12-9043-f8b5a5f21a65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/400], Loss: 0.1125\n",
            "Epoch [2/400], Loss: 0.0945\n",
            "Epoch [3/400], Loss: 0.0873\n",
            "Epoch [4/400], Loss: 0.0836\n",
            "Epoch [5/400], Loss: 0.0809\n",
            "Epoch [6/400], Loss: 0.0794\n",
            "Epoch [7/400], Loss: 0.0756\n",
            "Epoch [8/400], Loss: 0.0682\n",
            "Epoch [9/400], Loss: 0.0638\n",
            "Epoch [10/400], Loss: 0.0609\n",
            "Epoch [11/400], Loss: 0.0595\n",
            "Epoch [12/400], Loss: 0.0594\n",
            "Epoch [13/400], Loss: 0.0588\n",
            "Epoch [14/400], Loss: 0.0586\n",
            "Epoch [15/400], Loss: 0.0582\n",
            "Epoch [16/400], Loss: 0.0579\n",
            "Epoch [17/400], Loss: 0.0578\n",
            "Epoch [18/400], Loss: 0.0573\n",
            "Epoch [19/400], Loss: 0.0572\n",
            "Epoch [20/400], Loss: 0.0566\n",
            "Epoch [21/400], Loss: 0.0564\n",
            "Epoch [22/400], Loss: 0.0559\n",
            "Epoch [23/400], Loss: 0.0552\n",
            "Epoch [24/400], Loss: 0.0539\n",
            "Epoch [25/400], Loss: 0.0529\n",
            "Epoch [26/400], Loss: 0.0527\n",
            "Epoch [27/400], Loss: 0.0523\n",
            "Epoch [28/400], Loss: 0.0519\n",
            "Epoch [29/400], Loss: 0.0516\n",
            "Epoch [30/400], Loss: 0.0515\n",
            "Epoch [31/400], Loss: 0.0513\n",
            "Epoch [32/400], Loss: 0.0514\n",
            "Epoch [33/400], Loss: 0.0510\n",
            "Epoch [34/400], Loss: 0.0514\n",
            "Epoch [35/400], Loss: 0.0510\n",
            "Epoch [36/400], Loss: 0.0509\n",
            "Epoch [37/400], Loss: 0.0509\n",
            "Epoch [38/400], Loss: 0.0504\n",
            "Epoch [39/400], Loss: 0.0506\n",
            "Epoch [40/400], Loss: 0.0506\n",
            "Epoch [41/400], Loss: 0.0501\n",
            "Epoch [42/400], Loss: 0.0502\n",
            "Epoch [43/400], Loss: 0.0497\n",
            "Epoch [44/400], Loss: 0.0489\n",
            "Epoch [45/400], Loss: 0.0479\n",
            "Epoch [46/400], Loss: 0.0475\n",
            "Epoch [47/400], Loss: 0.0472\n",
            "Epoch [48/400], Loss: 0.0468\n",
            "Epoch [49/400], Loss: 0.0471\n",
            "Epoch [50/400], Loss: 0.0470\n",
            "Epoch [51/400], Loss: 0.0468\n",
            "Epoch [52/400], Loss: 0.0466\n",
            "Epoch [53/400], Loss: 0.0469\n",
            "Epoch [54/400], Loss: 0.0469\n",
            "Epoch [55/400], Loss: 0.0467\n",
            "Epoch [56/400], Loss: 0.0466\n",
            "Epoch [57/400], Loss: 0.0464\n",
            "Epoch [58/400], Loss: 0.0466\n",
            "Epoch [59/400], Loss: 0.0463\n",
            "Epoch [60/400], Loss: 0.0467\n",
            "Epoch [61/400], Loss: 0.0464\n",
            "Epoch [62/400], Loss: 0.0469\n",
            "Epoch [63/400], Loss: 0.0464\n",
            "Epoch [64/400], Loss: 0.0465\n",
            "Epoch [65/400], Loss: 0.0465\n",
            "Epoch [66/400], Loss: 0.0465\n",
            "Epoch [67/400], Loss: 0.0465\n",
            "Epoch [68/400], Loss: 0.0464\n",
            "Epoch [69/400], Loss: 0.0462\n",
            "Epoch [70/400], Loss: 0.0463\n",
            "Epoch [71/400], Loss: 0.0467\n",
            "Epoch [72/400], Loss: 0.0462\n",
            "Epoch [73/400], Loss: 0.0463\n",
            "Epoch [74/400], Loss: 0.0463\n",
            "Epoch [75/400], Loss: 0.0461\n",
            "Epoch [76/400], Loss: 0.0466\n",
            "Epoch [77/400], Loss: 0.0462\n",
            "Epoch [78/400], Loss: 0.0463\n",
            "Epoch [79/400], Loss: 0.0462\n",
            "Epoch [80/400], Loss: 0.0463\n",
            "Epoch [81/400], Loss: 0.0465\n",
            "Epoch [82/400], Loss: 0.0461\n",
            "Epoch [83/400], Loss: 0.0464\n",
            "Epoch [84/400], Loss: 0.0462\n",
            "Epoch [85/400], Loss: 0.0462\n",
            "Epoch [86/400], Loss: 0.0460\n",
            "Epoch [87/400], Loss: 0.0462\n",
            "Epoch [88/400], Loss: 0.0462\n",
            "Epoch [89/400], Loss: 0.0460\n",
            "Epoch [90/400], Loss: 0.0462\n",
            "Epoch [91/400], Loss: 0.0459\n",
            "Epoch [92/400], Loss: 0.0462\n",
            "Epoch [93/400], Loss: 0.0462\n",
            "Epoch [94/400], Loss: 0.0461\n",
            "Epoch [95/400], Loss: 0.0462\n",
            "Epoch [96/400], Loss: 0.0462\n",
            "Epoch [97/400], Loss: 0.0461\n",
            "Epoch [98/400], Loss: 0.0459\n",
            "Epoch [99/400], Loss: 0.0461\n",
            "Epoch [100/400], Loss: 0.0461\n",
            "Epoch [101/400], Loss: 0.0461\n",
            "Epoch [102/400], Loss: 0.0461\n",
            "Epoch [103/400], Loss: 0.0461\n",
            "Epoch [104/400], Loss: 0.0460\n",
            "Epoch [105/400], Loss: 0.0460\n",
            "Epoch [106/400], Loss: 0.0460\n",
            "Epoch [107/400], Loss: 0.0463\n",
            "Epoch [108/400], Loss: 0.0463\n",
            "Epoch [109/400], Loss: 0.0461\n",
            "Epoch [110/400], Loss: 0.0463\n",
            "Epoch [111/400], Loss: 0.0459\n",
            "Epoch [112/400], Loss: 0.0461\n",
            "Epoch [113/400], Loss: 0.0458\n",
            "Epoch [114/400], Loss: 0.0459\n",
            "Epoch [115/400], Loss: 0.0459\n",
            "Epoch [116/400], Loss: 0.0459\n",
            "Epoch [117/400], Loss: 0.0460\n",
            "Epoch [118/400], Loss: 0.0462\n",
            "Epoch [119/400], Loss: 0.0461\n",
            "Epoch [120/400], Loss: 0.0462\n",
            "Epoch [121/400], Loss: 0.0459\n",
            "Epoch [122/400], Loss: 0.0460\n",
            "Epoch [123/400], Loss: 0.0459\n",
            "Epoch [124/400], Loss: 0.0462\n",
            "Epoch [125/400], Loss: 0.0459\n",
            "Epoch [126/400], Loss: 0.0458\n",
            "Epoch [127/400], Loss: 0.0459\n",
            "Epoch [128/400], Loss: 0.0460\n",
            "Epoch [129/400], Loss: 0.0458\n",
            "Epoch [130/400], Loss: 0.0459\n",
            "Epoch [131/400], Loss: 0.0460\n",
            "Epoch [132/400], Loss: 0.0459\n",
            "Epoch [133/400], Loss: 0.0460\n",
            "Epoch [134/400], Loss: 0.0458\n",
            "Epoch [135/400], Loss: 0.0460\n",
            "Epoch [136/400], Loss: 0.0460\n",
            "Epoch [137/400], Loss: 0.0458\n",
            "Epoch [138/400], Loss: 0.0458\n",
            "Epoch [139/400], Loss: 0.0457\n",
            "Epoch [140/400], Loss: 0.0458\n",
            "Epoch [141/400], Loss: 0.0461\n",
            "Epoch [142/400], Loss: 0.0458\n",
            "Epoch [143/400], Loss: 0.0458\n",
            "Epoch [144/400], Loss: 0.0459\n",
            "Epoch [145/400], Loss: 0.0458\n",
            "Epoch [146/400], Loss: 0.0458\n",
            "Epoch [147/400], Loss: 0.0458\n",
            "Epoch [148/400], Loss: 0.0459\n",
            "Epoch [149/400], Loss: 0.0459\n",
            "Epoch [150/400], Loss: 0.0459\n",
            "Epoch [151/400], Loss: 0.0459\n",
            "Epoch [152/400], Loss: 0.0457\n",
            "Epoch [153/400], Loss: 0.0457\n",
            "Epoch [154/400], Loss: 0.0458\n",
            "Epoch [155/400], Loss: 0.0458\n",
            "Epoch [156/400], Loss: 0.0457\n",
            "Epoch [157/400], Loss: 0.0458\n",
            "Epoch [158/400], Loss: 0.0457\n",
            "Epoch [159/400], Loss: 0.0456\n",
            "Epoch [160/400], Loss: 0.0458\n",
            "Epoch [161/400], Loss: 0.0460\n",
            "Epoch [162/400], Loss: 0.0461\n",
            "Epoch [163/400], Loss: 0.0457\n",
            "Epoch [164/400], Loss: 0.0459\n",
            "Epoch [165/400], Loss: 0.0458\n",
            "Epoch [166/400], Loss: 0.0458\n",
            "Epoch [167/400], Loss: 0.0458\n",
            "Epoch [168/400], Loss: 0.0459\n",
            "Epoch [169/400], Loss: 0.0458\n",
            "Epoch [170/400], Loss: 0.0458\n",
            "Epoch [171/400], Loss: 0.0457\n",
            "Epoch [172/400], Loss: 0.0459\n",
            "Epoch [173/400], Loss: 0.0459\n",
            "Epoch [174/400], Loss: 0.0459\n",
            "Epoch [175/400], Loss: 0.0457\n",
            "Epoch [176/400], Loss: 0.0456\n",
            "Epoch [177/400], Loss: 0.0455\n",
            "Epoch [178/400], Loss: 0.0458\n",
            "Epoch [179/400], Loss: 0.0458\n",
            "Epoch [180/400], Loss: 0.0462\n",
            "Epoch [181/400], Loss: 0.0456\n",
            "Epoch [182/400], Loss: 0.0456\n",
            "Epoch [183/400], Loss: 0.0457\n",
            "Epoch [184/400], Loss: 0.0457\n",
            "Epoch [185/400], Loss: 0.0458\n",
            "Epoch [186/400], Loss: 0.0457\n",
            "Epoch [187/400], Loss: 0.0459\n",
            "Epoch [188/400], Loss: 0.0457\n",
            "Epoch [189/400], Loss: 0.0459\n",
            "Epoch [190/400], Loss: 0.0456\n",
            "Epoch [191/400], Loss: 0.0457\n",
            "Epoch [192/400], Loss: 0.0455\n",
            "Epoch [193/400], Loss: 0.0455\n",
            "Epoch [194/400], Loss: 0.0457\n",
            "Epoch [195/400], Loss: 0.0459\n",
            "Epoch [196/400], Loss: 0.0457\n",
            "Epoch [197/400], Loss: 0.0456\n",
            "Epoch [198/400], Loss: 0.0455\n",
            "Epoch [199/400], Loss: 0.0456\n",
            "Epoch [200/400], Loss: 0.0458\n",
            "Epoch [201/400], Loss: 0.0454\n",
            "Epoch [202/400], Loss: 0.0457\n",
            "Epoch [203/400], Loss: 0.0456\n",
            "Epoch [204/400], Loss: 0.0456\n",
            "Epoch [205/400], Loss: 0.0456\n",
            "Epoch [206/400], Loss: 0.0456\n",
            "Epoch [207/400], Loss: 0.0457\n",
            "Epoch [208/400], Loss: 0.0455\n",
            "Epoch [209/400], Loss: 0.0456\n",
            "Epoch [210/400], Loss: 0.0457\n",
            "Epoch [211/400], Loss: 0.0459\n",
            "Epoch [212/400], Loss: 0.0455\n",
            "Epoch [213/400], Loss: 0.0457\n",
            "Epoch [214/400], Loss: 0.0459\n",
            "Epoch [215/400], Loss: 0.0455\n",
            "Epoch [216/400], Loss: 0.0457\n",
            "Epoch [217/400], Loss: 0.0454\n",
            "Epoch [218/400], Loss: 0.0457\n",
            "Epoch [219/400], Loss: 0.0454\n",
            "Epoch [220/400], Loss: 0.0459\n",
            "Epoch [221/400], Loss: 0.0455\n",
            "Epoch [222/400], Loss: 0.0457\n",
            "Epoch [223/400], Loss: 0.0454\n",
            "Epoch [224/400], Loss: 0.0454\n",
            "Epoch [225/400], Loss: 0.0458\n",
            "Epoch [226/400], Loss: 0.0456\n",
            "Epoch [227/400], Loss: 0.0456\n",
            "Epoch [228/400], Loss: 0.0455\n",
            "Epoch [229/400], Loss: 0.0458\n",
            "Epoch [230/400], Loss: 0.0455\n",
            "Epoch [231/400], Loss: 0.0455\n",
            "Epoch [232/400], Loss: 0.0455\n",
            "Epoch [233/400], Loss: 0.0457\n",
            "Epoch [234/400], Loss: 0.0454\n",
            "Epoch [235/400], Loss: 0.0456\n",
            "Epoch [236/400], Loss: 0.0456\n",
            "Epoch [237/400], Loss: 0.0456\n",
            "Epoch [238/400], Loss: 0.0456\n",
            "Epoch [239/400], Loss: 0.0455\n",
            "Epoch [240/400], Loss: 0.0453\n",
            "Epoch [241/400], Loss: 0.0456\n",
            "Epoch [242/400], Loss: 0.0457\n",
            "Epoch [243/400], Loss: 0.0456\n",
            "Epoch [244/400], Loss: 0.0456\n",
            "Epoch [245/400], Loss: 0.0457\n",
            "Epoch [246/400], Loss: 0.0457\n",
            "Epoch [247/400], Loss: 0.0458\n",
            "Epoch [248/400], Loss: 0.0455\n",
            "Epoch [249/400], Loss: 0.0453\n",
            "Epoch [250/400], Loss: 0.0453\n",
            "Epoch [251/400], Loss: 0.0455\n",
            "Epoch [252/400], Loss: 0.0453\n",
            "Epoch [253/400], Loss: 0.0454\n",
            "Epoch [254/400], Loss: 0.0457\n",
            "Epoch [255/400], Loss: 0.0453\n",
            "Epoch [256/400], Loss: 0.0456\n",
            "Epoch [257/400], Loss: 0.0457\n",
            "Epoch [258/400], Loss: 0.0452\n",
            "Epoch [259/400], Loss: 0.0458\n",
            "Epoch [260/400], Loss: 0.0453\n",
            "Epoch [261/400], Loss: 0.0453\n",
            "Epoch [262/400], Loss: 0.0453\n",
            "Epoch [263/400], Loss: 0.0456\n",
            "Epoch [264/400], Loss: 0.0454\n",
            "Epoch [265/400], Loss: 0.0455\n",
            "Epoch [266/400], Loss: 0.0454\n",
            "Epoch [267/400], Loss: 0.0452\n",
            "Epoch [268/400], Loss: 0.0454\n",
            "Epoch [269/400], Loss: 0.0454\n",
            "Epoch [270/400], Loss: 0.0454\n",
            "Epoch [271/400], Loss: 0.0456\n",
            "Epoch [272/400], Loss: 0.0453\n",
            "Epoch [273/400], Loss: 0.0455\n",
            "Epoch [274/400], Loss: 0.0453\n",
            "Epoch [275/400], Loss: 0.0456\n",
            "Epoch [276/400], Loss: 0.0453\n",
            "Epoch [277/400], Loss: 0.0452\n",
            "Epoch [278/400], Loss: 0.0453\n",
            "Epoch [279/400], Loss: 0.0455\n",
            "Epoch [280/400], Loss: 0.0453\n",
            "Epoch [281/400], Loss: 0.0453\n",
            "Epoch [282/400], Loss: 0.0456\n",
            "Epoch [283/400], Loss: 0.0452\n",
            "Epoch [284/400], Loss: 0.0455\n",
            "Epoch [285/400], Loss: 0.0453\n",
            "Epoch [286/400], Loss: 0.0454\n",
            "Epoch [287/400], Loss: 0.0455\n",
            "Epoch [288/400], Loss: 0.0452\n",
            "Epoch [289/400], Loss: 0.0453\n",
            "Epoch [290/400], Loss: 0.0457\n",
            "Epoch [291/400], Loss: 0.0453\n",
            "Epoch [292/400], Loss: 0.0455\n",
            "Epoch [293/400], Loss: 0.0452\n",
            "Epoch [294/400], Loss: 0.0452\n",
            "Epoch [295/400], Loss: 0.0452\n",
            "Epoch [296/400], Loss: 0.0455\n",
            "Epoch [297/400], Loss: 0.0455\n",
            "Epoch [298/400], Loss: 0.0453\n",
            "Epoch [299/400], Loss: 0.0456\n",
            "Epoch [300/400], Loss: 0.0452\n",
            "Epoch [301/400], Loss: 0.0455\n",
            "Epoch [302/400], Loss: 0.0454\n",
            "Epoch [303/400], Loss: 0.0453\n",
            "Epoch [304/400], Loss: 0.0454\n",
            "Epoch [305/400], Loss: 0.0452\n",
            "Epoch [306/400], Loss: 0.0455\n",
            "Epoch [307/400], Loss: 0.0453\n",
            "Epoch [308/400], Loss: 0.0450\n",
            "Epoch [309/400], Loss: 0.0451\n",
            "Epoch [310/400], Loss: 0.0456\n",
            "Epoch [311/400], Loss: 0.0453\n",
            "Epoch [312/400], Loss: 0.0453\n",
            "Epoch [313/400], Loss: 0.0452\n",
            "Epoch [314/400], Loss: 0.0456\n",
            "Epoch [315/400], Loss: 0.0453\n",
            "Epoch [316/400], Loss: 0.0453\n",
            "Epoch [317/400], Loss: 0.0451\n",
            "Epoch [318/400], Loss: 0.0453\n",
            "Epoch [319/400], Loss: 0.0455\n",
            "Epoch [320/400], Loss: 0.0456\n",
            "Epoch [321/400], Loss: 0.0451\n",
            "Epoch [322/400], Loss: 0.0454\n",
            "Epoch [323/400], Loss: 0.0454\n",
            "Epoch [324/400], Loss: 0.0452\n",
            "Epoch [325/400], Loss: 0.0452\n",
            "Epoch [326/400], Loss: 0.0453\n",
            "Epoch [327/400], Loss: 0.0452\n",
            "Epoch [328/400], Loss: 0.0453\n",
            "Epoch [329/400], Loss: 0.0451\n",
            "Epoch [330/400], Loss: 0.0452\n",
            "Epoch [331/400], Loss: 0.0456\n",
            "Epoch [332/400], Loss: 0.0452\n",
            "Epoch [333/400], Loss: 0.0453\n",
            "Epoch [334/400], Loss: 0.0457\n",
            "Epoch [335/400], Loss: 0.0454\n",
            "Epoch [336/400], Loss: 0.0453\n",
            "Epoch [337/400], Loss: 0.0454\n",
            "Epoch [338/400], Loss: 0.0451\n",
            "Epoch [339/400], Loss: 0.0453\n",
            "Epoch [340/400], Loss: 0.0453\n",
            "Epoch [341/400], Loss: 0.0452\n",
            "Epoch [342/400], Loss: 0.0450\n",
            "Epoch [343/400], Loss: 0.0451\n",
            "Epoch [344/400], Loss: 0.0453\n",
            "Epoch [345/400], Loss: 0.0453\n",
            "Epoch [346/400], Loss: 0.0453\n",
            "Epoch [347/400], Loss: 0.0453\n",
            "Epoch [348/400], Loss: 0.0453\n",
            "Epoch [349/400], Loss: 0.0453\n",
            "Epoch [350/400], Loss: 0.0451\n",
            "Epoch [351/400], Loss: 0.0452\n",
            "Epoch [352/400], Loss: 0.0450\n",
            "Epoch [353/400], Loss: 0.0451\n",
            "Epoch [354/400], Loss: 0.0450\n",
            "Epoch [355/400], Loss: 0.0452\n",
            "Epoch [356/400], Loss: 0.0454\n",
            "Epoch [357/400], Loss: 0.0452\n",
            "Epoch [358/400], Loss: 0.0455\n",
            "Epoch [359/400], Loss: 0.0452\n",
            "Epoch [360/400], Loss: 0.0452\n",
            "Epoch [361/400], Loss: 0.0452\n",
            "Epoch [362/400], Loss: 0.0454\n",
            "Epoch [363/400], Loss: 0.0452\n",
            "Epoch [364/400], Loss: 0.0451\n",
            "Epoch [365/400], Loss: 0.0451\n",
            "Epoch [366/400], Loss: 0.0451\n",
            "Epoch [367/400], Loss: 0.0451\n",
            "Epoch [368/400], Loss: 0.0450\n",
            "Epoch [369/400], Loss: 0.0451\n",
            "Epoch [370/400], Loss: 0.0454\n",
            "Epoch [371/400], Loss: 0.0452\n",
            "Epoch [372/400], Loss: 0.0452\n",
            "Epoch [373/400], Loss: 0.0452\n",
            "Epoch [374/400], Loss: 0.0450\n",
            "Epoch [375/400], Loss: 0.0451\n",
            "Epoch [376/400], Loss: 0.0450\n",
            "Epoch [377/400], Loss: 0.0451\n",
            "Epoch [378/400], Loss: 0.0452\n",
            "Epoch [379/400], Loss: 0.0452\n",
            "Epoch [380/400], Loss: 0.0451\n",
            "Epoch [381/400], Loss: 0.0450\n",
            "Epoch [382/400], Loss: 0.0450\n",
            "Epoch [383/400], Loss: 0.0452\n",
            "Epoch [384/400], Loss: 0.0450\n",
            "Epoch [385/400], Loss: 0.0449\n",
            "Epoch [386/400], Loss: 0.0451\n",
            "Epoch [387/400], Loss: 0.0450\n",
            "Epoch [388/400], Loss: 0.0451\n",
            "Epoch [389/400], Loss: 0.0450\n",
            "Epoch [390/400], Loss: 0.0451\n",
            "Epoch [391/400], Loss: 0.0450\n",
            "Epoch [392/400], Loss: 0.0451\n",
            "Epoch [393/400], Loss: 0.0452\n",
            "Epoch [394/400], Loss: 0.0454\n",
            "Epoch [395/400], Loss: 0.0451\n",
            "Epoch [396/400], Loss: 0.0450\n",
            "Epoch [397/400], Loss: 0.0451\n",
            "Epoch [398/400], Loss: 0.0449\n",
            "Epoch [399/400], Loss: 0.0451\n",
            "Epoch [400/400], Loss: 0.0451\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get latent representations from input data\n",
        "def get_latent_representation(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    latent_representations = []\n",
        "\n",
        "    # Disable gradient calculation for inference\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs = data[0]  # Get the input data\n",
        "\n",
        "            # Encode the input data to get the latent representation\n",
        "            latent = model.encoder(inputs)\n",
        "\n",
        "            # Collect the latent representations\n",
        "            latent_representations.append(latent)\n",
        "\n",
        "    # Concatenate all latent representations into a single tensor\n",
        "    latent_representations = torch.cat(latent_representations, dim=0)\n",
        "\n",
        "    return latent_representations\n",
        "\n",
        "# Get the latent representations using the trained autoencoder\n",
        "latent_representations = get_latent_representation(autoencoder, dataloader)\n",
        "\n",
        "# Convert to NumPy array and transpose for MIG calculation\n",
        "latent_representations_np = latent_representations.numpy().T  # Shape will be (latent_dim, num_samples)\n",
        "\n",
        "print(\"Latent representations shape:\", latent_representations_np.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnqiwnFo8BRf",
        "outputId": "6c690e89-1c24-4a72-e079-ce31fb03e1fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latent representations shape: (5, 2000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert true factors to numpy array and transpose to match shape (num_factors, num_samples)\n",
        "true_factors_np = true_factors.to_numpy().T  # Shape will be (num_factors, num_samples)\n",
        "\n",
        "# Compute MIG score between latent representations and true factors\n",
        "mig_score = compute_mig(latent_representations_np, true_factors_np)\n",
        "\n",
        "# Print the MIG score\n",
        "print(f\"MIG Score between latent representations and true factors: {mig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHBJrZW_8DG8",
        "outputId": "df886b21-2128-460d-915d-360ad5a09cc9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MIG Score between latent representations and true factors: 0.0014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5U3Uouo3ri0A"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}